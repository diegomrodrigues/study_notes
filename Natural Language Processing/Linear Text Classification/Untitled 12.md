# Estima√ß√£o de Par√¢metros em Classifica√ß√£o de Texto Linear

<imagem: Um gr√°fico mostrando curvas de distribui√ß√£o categ√≥rica e multinomial, com setas apontando para os par√¢metros Œº e œÜ sendo estimados a partir de um conjunto de dados de texto>

## Introdu√ß√£o

A estima√ß√£o de par√¢metros √© um componente crucial na classifica√ß√£o de texto linear, especialmente quando lidamos com distribui√ß√µes categ√≥ricas e multinomiais [1]. Este resumo se concentra na estima√ß√£o dos par√¢metros Œº e œÜ dessas distribui√ß√µes usando a estimativa de frequ√™ncia relativa, que √© equivalente √† estimativa de m√°xima verossimilhan√ßa neste contexto [2]. Compreender esse processo √© fundamental para implementar classificadores eficazes em tarefas de processamento de linguagem natural.

## Conceitos Fundamentais

| Conceito                                 | Explica√ß√£o                                                   |
| ---------------------------------------- | ------------------------------------------------------------ |
| **Distribui√ß√£o Categ√≥rica**              | Uma distribui√ß√£o de probabilidade que descreve o resultado de um experimento aleat√≥rio com K categorias mutuamente exclusivas. Em classifica√ß√£o de texto, √© frequentemente usada para modelar a distribui√ß√£o de r√≥tulos [3]. |
| **Distribui√ß√£o Multinomial**             | Uma generaliza√ß√£o da distribui√ß√£o binomial para K > 2 categorias. No contexto de classifica√ß√£o de texto, modela a distribui√ß√£o de contagens de palavras em um documento [4]. |
| **Estimativa de M√°xima Verossimilhan√ßa** | M√©todo de estima√ß√£o de par√¢metros que maximiza a probabilidade de observar os dados do conjunto de treinamento. Para distribui√ß√µes categ√≥ricas e multinomiais, isso equivale √† estimativa de frequ√™ncia relativa [5]. |

> ‚ö†Ô∏è **Nota Importante**: A estimativa de m√°xima verossimilhan√ßa pode levar a problemas de overfitting, especialmente em conjuntos de dados pequenos ou esparsos. T√©cnicas de regulariza√ß√£o, como smoothing, s√£o frequentemente necess√°rias [6].

### Estima√ß√£o de Par√¢metros para Distribui√ß√£o Categ√≥rica

<imagem: Um diagrama mostrando a contagem de ocorr√™ncias de cada categoria em um conjunto de dados, com setas apontando para as estimativas dos par√¢metros correspondentes>

A distribui√ß√£o categ√≥rica √© fundamental para modelar a probabilidade de r√≥tulos em classifica√ß√£o de texto. Seja Y o conjunto de r√≥tulos poss√≠veis, a estima√ß√£o do par√¢metro Œº para esta distribui√ß√£o √© dada por [7]:

$$
\mu_y = \frac{\text{count}(y)}{\sum_{y' \in Y} \text{count}(y')} = \frac{\sum_{i=1}^N \delta(y^{(i)} = y)}{N}
$$

Onde:
- $\mu_y$ √© a probabilidade estimada para o r√≥tulo y
- count(y) √© o n√∫mero de ocorr√™ncias do r√≥tulo y no conjunto de treinamento
- $\delta(y^{(i)} = y)$ √© a fun√ß√£o delta de Kronecker, que retorna 1 se $y^{(i)} = y$ e 0 caso contr√°rio
- N √© o n√∫mero total de inst√¢ncias no conjunto de treinamento

Esta estimativa √© intuitiva: a probabilidade de cada r√≥tulo √© simplesmente a propor√ß√£o de vezes que ele aparece no conjunto de treinamento [8].

#### Perguntas Te√≥ricas

1. Prove que a estimativa de m√°xima verossimilhan√ßa para Œº na distribui√ß√£o categ√≥rica √© equivalente √† estimativa de frequ√™ncia relativa apresentada acima.
2. Como a estimativa de Œº seria afetada se tiv√©ssemos um conjunto de dados extremamente desbalanceado? Discuta as implica√ß√µes te√≥ricas e pr√°ticas.
3. Derive a express√£o para o erro padr√£o da estimativa de Œº_y e explique como isso poderia ser usado para construir intervalos de confian√ßa para os par√¢metros estimados.

### Estima√ß√£o de Par√¢metros para Distribui√ß√£o Multinomial

<imagem: Um gr√°fico de barras mostrando as contagens de palavras em documentos de diferentes classes, com linhas pontilhadas indicando as estimativas de œÜ para cada classe>

Na classifica√ß√£o de texto usando o modelo Naive Bayes, a distribui√ß√£o multinomial √© usada para modelar a probabilidade de ocorr√™ncia de palavras dado um r√≥tulo espec√≠fico. A estima√ß√£o do par√¢metro œÜ para esta distribui√ß√£o √© dada por [9]:

$$
\phi_{y,j} = \frac{\text{count}(y, j)}{\sum_{j'=1}^V \text{count}(y, j')} = \frac{\sum_{i:y^{(i)}=y} x_j^{(i)}}{\sum_{j'=1}^V \sum_{i:y^{(i)}=y} x_{j'}^{(i)}}
$$

Onde:
- $\phi_{y,j}$ √© a probabilidade estimada da palavra j dado o r√≥tulo y
- count(y, j) √© o n√∫mero de ocorr√™ncias da palavra j em documentos com r√≥tulo y
- $x_j^{(i)}$ √© a contagem da palavra j no documento i
- V √© o tamanho do vocabul√°rio

Esta estimativa representa a propor√ß√£o de vezes que a palavra j aparece em documentos com o r√≥tulo y, em rela√ß√£o ao total de palavras em documentos com esse r√≥tulo [10].

> ‚ùó **Ponto de Aten√ß√£o**: A estimativa de m√°xima verossimilhan√ßa para œÜ pode resultar em probabilidades zero para palavras que n√£o aparecem em nenhum documento de uma determinada classe no conjunto de treinamento. Isso pode levar a problemas na classifica√ß√£o, pois uma √∫nica palavra ausente poderia zerar a probabilidade de uma classe inteira [11].

Para mitigar esse problema, √© comum usar t√©cnicas de smoothing, como o smoothing de Laplace [12]:

$$
\phi_{y,j} = \frac{\alpha + \text{count}(y, j)}{V\alpha + \sum_{j'=1}^V \text{count}(y, j')}
$$

Onde Œ± √© um hiperpar√¢metro que controla o grau de smoothing. Esta t√©cnica adiciona um pseudocount Œ± a todas as contagens, evitando probabilidades zero [13].

#### Perguntas Te√≥ricas

1. Demonstre matematicamente por que a estimativa de m√°xima verossimilhan√ßa para œÜ na distribui√ß√£o multinomial √© equivalente √† estimativa de frequ√™ncia relativa apresentada.
2. Derive a express√£o para o gradiente da log-verossimilhan√ßa com respeito a œÜ_y,j e mostre que o ponto onde este gradiente √© zero corresponde √† estimativa de frequ√™ncia relativa.
3. Analise teoricamente o impacto do smoothing de Laplace na vari√¢ncia das estimativas de œÜ. Como isso afeta o trade-off entre vi√©s e vari√¢ncia no modelo?

### Justificativa Te√≥rica para Estima√ß√£o de M√°xima Verossimilhan√ßa

A estima√ß√£o de m√°xima verossimilhan√ßa (MLE) para os par√¢metros Œº e œÜ pode ser justificada atrav√©s da maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa [14]. Para a distribui√ß√£o multinomial, a log-verossimilhan√ßa √© dada por:

$$
\mathcal{L}(\phi) = \sum_{i=1}^N \log p_{\text{mult}}(x^{(i)}; \phi_{y(i)}) = \sum_{i=1}^N \log B(x^{(i)}) + \sum_{j=1}^V x_j^{(i)} \log \phi_{y(i),j}
$$

Onde B(x^(i)) √© o coeficiente multinomial, que √© constante em rela√ß√£o a œÜ [15].

Para maximizar esta fun√ß√£o sujeita √† restri√ß√£o $\sum_{j=1}^V \phi_{y,j} = 1$ para todo y, podemos usar multiplicadores de Lagrange. O Lagrangiano √© dado por:

$$
\ell(\phi_y) = \sum_{i:y^{(i)}=y} \sum_{j=1}^V x_j^{(i)} \log \phi_{y,j} - \lambda(\sum_{j=1}^V \phi_{y,j} - 1)
$$

Diferenciando em rela√ß√£o a œÜ_y,j e igualando a zero, obtemos:

$$
\frac{\partial \ell(\phi_y)}{\partial \phi_{y,j}} = \sum_{i:y^{(i)}=y} x_j^{(i)} / \phi_{y,j} - \lambda = 0
$$

Resolvendo esta equa√ß√£o, chegamos √† estimativa de frequ√™ncia relativa para œÜ_y,j [16].

> ‚úîÔ∏è **Destaque**: A estimativa de m√°xima verossimilhan√ßa para distribui√ß√µes categ√≥ricas e multinomiais tem a propriedade desej√°vel de ser n√£o-viesada e assintoticamente eficiente, ou seja, atinge o limite inferior de Cram√©r-Rao √† medida que o tamanho da amostra aumenta [17].

#### Perguntas Te√≥ricas

1. Derive a matriz de informa√ß√£o de Fisher para a distribui√ß√£o multinomial e use-a para calcular o limite inferior de Cram√©r-Rao para as estimativas de œÜ.
2. Prove que a estimativa de m√°xima verossimilhan√ßa para œÜ √© consistente, ou seja, converge em probabilidade para o verdadeiro valor do par√¢metro √† medida que o tamanho da amostra aumenta.
3. Analise teoricamente como a adi√ß√£o de regulariza√ß√£o L2 na log-verossimilhan√ßa afetaria as estimativas de œÜ. Derive as novas equa√ß√µes para as estimativas regularizadas.

## Implementa√ß√£o e Considera√ß√µes Pr√°ticas

Na pr√°tica, a implementa√ß√£o da estima√ß√£o de par√¢metros para classifica√ß√£o de texto linear pode ser realizada de forma eficiente usando bibliotecas de processamento de linguagem natural. Aqui est√° um exemplo avan√ßado usando PyTorch para implementar um classificador Naive Bayes multinomial com smoothing de Laplace [18]:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultinomialNaiveBayes(nn.Module):
    def __init__(self, num_classes, vocab_size, alpha=1.0):
        super().__init__()
        self.num_classes = num_classes
        self.vocab_size = vocab_size
        self.alpha = alpha
        
        # Inicializa log(œÜ) e log(Œº)
        self.log_phi = nn.Parameter(torch.randn(num_classes, vocab_size))
        self.log_mu = nn.Parameter(torch.randn(num_classes))
    
    def forward(self, x):
        # x: (batch_size, vocab_size)
        return (x @ self.log_phi.T) + self.log_mu
    
    def fit(self, X, y):
        # X: (num_samples, vocab_size), y: (num_samples,)
        counts = torch.zeros(self.num_classes, self.vocab_size)
        for c in range(self.num_classes):
            counts[c] = X[y == c].sum(dim=0)
        
        # Aplica smoothing de Laplace e calcula log(œÜ)
        smoothed_counts = counts + self.alpha
        log_phi = torch.log(smoothed_counts) - torch.log(smoothed_counts.sum(dim=1, keepdim=True))
        
        # Calcula log(Œº)
        class_counts = torch.bincount(y, minlength=self.num_classes)
        log_mu = torch.log(class_counts + self.alpha) - torch.log(len(y) + self.alpha * self.num_classes)
        
        # Atualiza os par√¢metros
        self.log_phi.data = log_phi
        self.log_mu.data = log_mu

# Uso
model = MultinomialNaiveBayes(num_classes=3, vocab_size=10000)
X_train = torch.randint(0, 5, (1000, 10000))  # Dados de exemplo
y_train = torch.randint(0, 3, (1000,))  # R√≥tulos de exemplo
model.fit(X_train, y_train)

# Previs√£o
X_test = torch.randint(0, 5, (100, 10000))
with torch.no_grad():
    log_probs = model(X_test)
    predictions = log_probs.argmax(dim=1)
```

Este c√≥digo implementa um classificador Naive Bayes multinomial usando PyTorch, permitindo a estima√ß√£o eficiente dos par√¢metros Œº e œÜ com smoothing de Laplace [19]. A classe `MultinomialNaiveBayes` herda de `nn.Module`, permitindo que seja facilmente integrada em pipelines de deep learning mais complexos, se necess√°rio [20].

> üí° **Dica**: Trabalhar no espa√ßo logar√≠tmico (log(œÜ) e log(Œº)) melhora a estabilidade num√©rica e evita underflow em c√°lculos com probabilidades muito pequenas [21].

## Conclus√£o

A estima√ß√£o de par√¢metros para distribui√ß√µes categ√≥ricas e multinomiais √© fundamental na classifica√ß√£o de texto linear, especialmente no contexto do classificador Naive Bayes [22]. A estimativa de frequ√™ncia relativa, equivalente √† estimativa de m√°xima verossimilhan√ßa neste caso, fornece uma base te√≥rica s√≥lida para a estima√ß√£o dos par√¢metros Œº e œÜ [23]. 

No entanto, √© crucial considerar t√©cnicas de regulariza√ß√£o, como o smoothing de Laplace, para lidar com problemas de esparsidade e evitar probabilidades zero [24]. A implementa√ß√£o pr√°tica desses conceitos, como demonstrado no exemplo em PyTorch, permite a cria√ß√£o de classificadores eficientes e robustos para tarefas de processamento de linguagem natural [25].

√Ä medida que avan√ßamos para modelos mais complexos, como redes neurais profundas, os princ√≠pios fundamentais de estima√ß√£o de par√¢metros continuam relevantes, formando a base para compreender e melhorar algoritmos mais avan√ßados de aprendizado de m√°quina em processamento de texto [26].

## Perguntas Te√≥ricas Avan√ßadas

1. Derive a express√£o para a informa√ß√£o m√∫tua entre as features e os r√≥tulos em um classificador Naive Bayes multinomial. Como essa medida poderia ser usada para selecionar features de forma teoricamente fundamentada?

2. Considere um cen√°rio onde os documentos t√™m comprimentos muito variados. Proponha e analise teoricamente uma modifica√ß√£o na estimativa de œÜ que leve em conta essa varia√ß√£o, discutindo suas propriedades estat√≠sticas.

3. Compare teoricamente a vari√¢ncia das estimativas de œÜ obtidas por m√°xima verossimilhan√ßa com as obtidas por infer√™ncia bayesiana usando uma prior Dirichlet. Em que condi√ß√µes a abordagem bayesiana seria prefer√≠vel?

4. Prove que, para um conjunto de dados fixo, √† medida que o par√¢metro de smoothing Œ± aumenta, as estimativas de œÜ para todas as classes convergem para uma distribui√ß√£o uniforme. Discuta as implica√ß√µes desse resultado para a escolha de Œ±.

5. Desenvolva uma prova formal de que, para um classificador Naive Bayes multinomial com smoothing de Laplace, a decis√£o de classifica√ß√£o √© equivalente a encontrar o hiperplano separador em um espa√ßo de caracter√≠sticas transformado. Relacione este resultado com o conceito de kernel trick em SVMs.

## Refer√™ncias

[1] "To predict a label from a bag-of-words, we can assign a score to each word in the vocabulary, measuring the compatibility with the label." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[2] "Maximum likelihood estimation chooses œÜ to maximize the log-likelihood L." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[3] "The categorical distribution involves a product over words, with each term in the product equal to the probability œÜj, exponentiated by the count x