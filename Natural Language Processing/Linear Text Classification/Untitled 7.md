# Classifica√ß√£o de Texto com Multinomial Na√Øve Bayes

<imagem: Um diagrama mostrando um documento de texto sendo transformado em um vetor de contagem de palavras (bag-of-words), que ent√£o alimenta um modelo Na√Øve Bayes com distribui√ß√µes multinomiais para cada classe, resultando em uma previs√£o de classe.>

## Introdu√ß√£o

O **Multinomial Na√Øve Bayes** √© um algoritmo fundamental na classifica√ß√£o de texto, particularmente eficaz quando utilizamos a representa√ß√£o bag-of-words. Este m√©todo combina o princ√≠pio de Na√Øve Bayes com a distribui√ß√£o multinomial para modelar a ocorr√™ncia de palavras em documentos, assumindo a independ√™ncia condicional das palavras dado o r√≥tulo do documento [1]. Esta abordagem √© amplamente utilizada devido √† sua simplicidade, efici√™ncia computacional e surpreendente efic√°cia em muitas tarefas de classifica√ß√£o de texto, como filtragem de spam, categoriza√ß√£o de not√≠cias e an√°lise de sentimentos.

## Conceitos Fundamentais

| Conceito                      | Explica√ß√£o                                                   |
| ----------------------------- | ------------------------------------------------------------ |
| **Bag-of-Words**              | Representa√ß√£o de um documento como um vetor de contagem de palavras, ignorando a ordem e a estrutura gramatical [2]. |
| **Distribui√ß√£o Multinomial**  | Modelo probabil√≠stico que descreve a ocorr√™ncia de eventos discretos em um n√∫mero fixo de tentativas, adequado para modelar a frequ√™ncia de palavras em documentos [3]. |
| **Independ√™ncia Condicional** | Suposi√ß√£o de que as ocorr√™ncias de palavras s√£o independentes entre si, dado o r√≥tulo do documento [4]. |

> ‚ö†Ô∏è **Nota Importante**: A suposi√ß√£o de independ√™ncia condicional, embora simplificadora, √© crucial para a tratabilidade computacional do modelo Na√Øve Bayes, permitindo uma fatoriza√ß√£o eficiente da probabilidade conjunta [5].

### Formula√ß√£o Matem√°tica do Multinomial Na√Øve Bayes

O Multinomial Na√Øve Bayes modela a probabilidade de um documento pertencer a uma classe espec√≠fica usando a regra de Bayes e a distribui√ß√£o multinomial. A probabilidade de um documento $x$ pertencer √† classe $y$ √© dada por [6]:

$$
p(y|x) = \frac{p(x|y)p(y)}{p(x)}
$$

Onde:
- $p(y|x)$ √© a probabilidade posterior da classe $y$ dado o documento $x$
- $p(x|y)$ √© a verossimilhan√ßa do documento $x$ dado a classe $y$
- $p(y)$ √© a probabilidade a priori da classe $y$
- $p(x)$ √© a probabilidade marginal do documento $x$

A verossimilhan√ßa $p(x|y)$ √© modelada usando a distribui√ß√£o multinomial [7]:

$$
p_{\text{mult}}(x; \phi_y) = B(x) \prod_{j=1}^V \phi_{y,j}^{x_j}
$$

Onde:
- $\phi_y$ √© o vetor de par√¢metros da distribui√ß√£o multinomial para a classe $y$
- $x_j$ √© a contagem da palavra $j$ no documento
- $V$ √© o tamanho do vocabul√°rio
- $B(x)$ √© o coeficiente multinomial (que pode ser ignorado na classifica√ß√£o)

### Estima√ß√£o de Par√¢metros

Os par√¢metros $\phi_{y,j}$ s√£o estimados usando a estimativa de m√°xima verossimilhan√ßa (MLE) [8]:

$$
\phi_{y,j} = \frac{\text{count}(y,j)}{\sum_{j'=1}^V \text{count}(y,j')} = \frac{\sum_{i:y^{(i)}=y} x_j^{(i)}}{\sum_{j'=1}^V \sum_{i:y^{(i)}=y} x_{j'}^{(i)}}
$$

Onde $\text{count}(y,j)$ √© a contagem total da palavra $j$ em documentos da classe $y$.

> ‚úîÔ∏è **Destaque**: A estimativa MLE pode levar a problemas com palavras n√£o vistas no conjunto de treinamento. Para mitigar isso, √© comum usar suaviza√ß√£o de Laplace (add-one smoothing) [9].

### Suaviza√ß√£o de Laplace

A suaviza√ß√£o de Laplace adiciona um pseudocontagem $\alpha$ a cada contagem de palavra [10]:

$$
\phi_{y,j} = \frac{\alpha + \text{count}(y,j)}{V\alpha + \sum_{j'=1}^V \text{count}(y,j')}
$$

Esta t√©cnica evita probabilidades zero e melhora a generaliza√ß√£o do modelo.

#### Perguntas Te√≥ricas

1. Derive a estimativa de m√°xima verossimilhan√ßa para o par√¢metro $\mu$ no modelo Na√Øve Bayes, considerando a distribui√ß√£o multinomial.

2. Demonstre matematicamente por que a suaviza√ß√£o de Laplace √© necess√°ria e como ela afeta a estimativa dos par√¢metros $\phi_{y,j}$.

3. Analise teoricamente o impacto da suposi√ß√£o de independ√™ncia condicional na performance do Multinomial Na√Øve Bayes em compara√ß√£o com modelos que n√£o fazem essa suposi√ß√£o.

## Implementa√ß√£o e Algoritmo

A implementa√ß√£o do Multinomial Na√Øve Bayes envolve duas fases principais: treinamento e predi√ß√£o.

### Fase de Treinamento

1. Calcular as probabilidades a priori $p(y)$ para cada classe.
2. Estimar os par√¢metros $\phi_{y,j}$ para cada classe e palavra no vocabul√°rio.

### Fase de Predi√ß√£o

Para um novo documento $x$, calcular:

$$
\hat{y} = \arg\max_y p(y) \prod_{j=1}^V \phi_{y,j}^{x_j}
$$

Na pr√°tica, √© comum trabalhar com logaritmos para evitar underflow num√©rico [11]:

$$
\hat{y} = \arg\max_y \left(\log p(y) + \sum_{j=1}^V x_j \log \phi_{y,j}\right)
$$

```python
import numpy as np
from scipy.sparse import csr_matrix

class MultinomialNB:
    def __init__(self, alpha=1.0):
        self.alpha = alpha
    
    def fit(self, X: csr_matrix, y):
        n_samples, n_features = X.shape
        self.classes = np.unique(y)
        n_classes = len(self.classes)
        
        self.class_log_prior_ = np.log(np.bincount(y) / n_samples)
        
        # Calcular contagens de palavras por classe
        self.feature_log_prob_ = np.zeros((n_classes, n_features))
        for i, c in enumerate(self.classes):
            X_c = X[y == c]
            N_c = X_c.sum(axis=0).A1 + self.alpha
            self.feature_log_prob_[i, :] = np.log(N_c / N_c.sum())
    
    def predict(self, X: csr_matrix):
        return self.classes[np.argmax(self._joint_log_likelihood(X), axis=1)]
    
    def _joint_log_likelihood(self, X: csr_matrix):
        return (safe_sparse_dot(X, self.feature_log_prob_.T) +
                self.class_log_prior_)

def safe_sparse_dot(a, b):
    if isinstance(a, csr_matrix):
        return a.dot(b)
    return np.dot(a, b)
```

Este c√≥digo implementa o Multinomial Na√Øve Bayes de forma eficiente, utilizando matrizes esparsas para lidar com grandes conjuntos de dados de texto [12].

> üí° **Dica**: A implementa√ß√£o usa logaritmos para evitar underflow num√©rico e melhorar a estabilidade computacional.

## Vantagens e Desvantagens

| üëç Vantagens                                          | üëé Desvantagens                                               |
| ---------------------------------------------------- | ------------------------------------------------------------ |
| Eficiente em termos computacionais e de mem√≥ria [13] | Suposi√ß√£o de independ√™ncia condicional pode ser irrealista [14] |
| Funciona bem com conjuntos de dados pequenos [15]    | Sens√≠vel a caracter√≠sticas irrelevantes ou correlacionadas [16] |
| Naturalmente multiclasse                             | Pode ser superado por modelos mais complexos em grandes conjuntos de dados [17] |
| F√°cil de implementar e interpretar                   | Estimativas de probabilidade podem ser imprecisas [18]       |

## An√°lise Te√≥rica Avan√ßada

### Rela√ß√£o com Modelos Log-lineares

O Multinomial Na√Øve Bayes pode ser visto como um caso especial de um modelo log-linear [19]. Considerando o logaritmo da probabilidade condicional:

$$
\log p(y|x; \theta) = \theta \cdot f(x,y) - \log \sum_{y' \in Y} \exp(\theta \cdot f(x,y'))
$$

Onde $f(x,y)$ √© uma fun√ß√£o de caracter√≠sticas que retorna um vetor de contagens de palavras para a classe $y$, e $\theta$ s√£o os pesos do modelo. Esta formula√ß√£o mostra a conex√£o entre Na√Øve Bayes e modelos discriminativos como regress√£o log√≠stica.

### An√°lise de Complexidade

A complexidade de tempo de treinamento do Multinomial Na√Øve Bayes √© $O(ND)$, onde $N$ √© o n√∫mero de documentos de treinamento e $D$ √© o n√∫mero m√©dio de palavras √∫nicas por documento. A complexidade de espa√ßo √© $O(CV)$, onde $C$ √© o n√∫mero de classes e $V$ √© o tamanho do vocabul√°rio [20].

### Teorema de Bayes e M√°xima Verossimilhan√ßa

A estima√ß√£o dos par√¢metros $\phi_{y,j}$ pode ser justificada como uma estimativa de m√°xima verossimilhan√ßa. A log-verossimilhan√ßa √© dada por [21]:

$$
\mathcal{L}(\phi, \mu) = \sum_{i=1}^N \log p_{\text{mult}}(x^{(i)}; \phi_{y^{(i)}}) + \log p_{\text{cat}}(y^{(i)}; \mu)
$$

Maximizar esta fun√ß√£o leva √†s estimativas de frequ√™ncia relativa para $\phi_{y,j}$ e $\mu_y$.

#### Perguntas Te√≥ricas

1. Prove que a estimativa de m√°xima verossimilhan√ßa para $\phi_{y,j}$ no Multinomial Na√Øve Bayes √© equivalente √† frequ√™ncia relativa das palavras na classe.

2. Analise teoricamente o impacto da suaviza√ß√£o de Laplace na vari√¢ncia dos estimadores de $\phi_{y,j}$. Como isso afeta o vi√©s-vari√¢ncia do modelo?

3. Derive a forma fechada da estimativa de m√°xima a posteriori (MAP) para $\phi_{y,j}$ assumindo uma distribui√ß√£o a priori Dirichlet sobre os par√¢metros.

## Conclus√£o

O Multinomial Na√Øve Bayes √© um classificador poderoso e eficiente para tarefas de classifica√ß√£o de texto, especialmente quando usado com a representa√ß√£o bag-of-words. Sua simplicidade matem√°tica, efici√™ncia computacional e surpreendente efic√°cia em muitas aplica√ß√µes pr√°ticas o tornam uma escolha popular em aprendizado de m√°quina e processamento de linguagem natural [22].

Apesar de suas limita√ß√µes, como a suposi√ß√£o de independ√™ncia condicional, o Multinomial Na√Øve Bayes continua sendo um benchmark importante e uma ferramenta valiosa no arsenal de qualquer cientista de dados, especialmente para tarefas de classifica√ß√£o de texto em larga escala ou quando os recursos computacionais s√£o limitados [23].

## Perguntas Te√≥ricas Avan√ßadas

1. Derive a forma da fronteira de decis√£o entre duas classes no espa√ßo de caracter√≠sticas para o Multinomial Na√Øve Bayes. Como essa fronteira se compara com a de um classificador de regress√£o log√≠stica?

2. Analise teoricamente o comportamento assint√≥tico do Multinomial Na√Øve Bayes √† medida que o n√∫mero de amostras de treinamento tende ao infinito. Sob quais condi√ß√µes o classificador converge para o classificador de Bayes √≥timo?

3. Desenvolva uma prova formal da consist√™ncia do estimador de m√°xima verossimilhan√ßa para os par√¢metros do Multinomial Na√Øve Bayes, assumindo que os dados s√£o gerados pelo modelo verdadeiro.

4. Derive a express√£o para a informa√ß√£o m√∫tua entre as caracter√≠sticas e o r√≥tulo da classe no contexto do Multinomial Na√Øve Bayes. Como isso se relaciona com a capacidade preditiva do modelo?

5. Formule e prove um teorema que estabele√ßa limites superiores no erro de generaliza√ß√£o do Multinomial Na√Øve Bayes em termos do n√∫mero de amostras de treinamento e da dimensionalidade do espa√ßo de caracter√≠sticas.

## Refer√™ncias

[1] "To predict a label from a bag-of-words, we can assign a score to each word in the vocabulary, measuring the compatibility with the label." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[2] "Suppose that x is a bag-of-words vector such that ‚àë·µ•‚±º‚Çå‚ÇÅ x‚±º = 1." *(Trecho de Exercises)*

[3] "The multinomial distribution involves a product over words, with each term in the product equal to the probability œÜj, exponentiated by the count xj." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[4] "Because the multinomial distribution treats each word token independently, conditioned on the class: the probability mass function factorizes across the counts." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[5] "The "na√Øve" independence assumption implied by the multinomial distribution, and as a result, the optimal parameters for this model are identical to those in multinomial Na√Øve Bayes." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[6] "p(y | x; Œ∏) = exp (Œ∏ ¬∑ f(x, y)) / ‚àëy'‚ààY exp (Œ∏ ¬∑ f(x, y'))." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[7] "p_mult(x; œÜ) = B(x) ‚àè(j=1 to V) œÜ(x_j)_(y,j)" *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[8] "œÜ_{y,j} = (count(y, j)) / (‚àë_{j'=1}^V count(y, j')) = (‚àë_{i:y^{(i)}=y} x_j^{(i)}) / (‚àë_{j'=1}^V ‚àë_{i:y^{(i)}=y} x_{j'}^{(i)})" *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[9] "One solution is to smooth the probabilities, by adding a "pseudocount" of Œ± to each count, and then normalizing" *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[10] "œÜ_y,j = (Œ± + count(y, j)) / (VŒ± + ‚àë^V_j'=1 count(y, j'))" *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[11] "log p(x | y; œÜ) + log p(y; Œº) = log [B(x) ‚àè(j=1 to V) œÜ(x_j)_(y,j)] + log Œº_y" *(Trecho de CHAPTER