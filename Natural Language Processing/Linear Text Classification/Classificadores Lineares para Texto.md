# Classificadores Lineares para Texto

<imagem: Um diagrama mostrando um espa√ßo vetorial de alta dimens√£o com vetores de palavras e um hiperplano separador representando um classificador linear>

## Introdu√ß√£o

Os **classificadores lineares** s√£o uma classe fundamental de algoritmos de aprendizado de m√°quina amplamente utilizados para a classifica√ß√£o de texto. ==Eles operam com base em uma fun√ß√£o de pontua√ß√£o linear, calculada como o produto interno entre um vetor de pesos e um vetor de caracter√≠sticas== [1]. Este resumo explora em profundidade os conceitos, teorias e aplica√ß√µes dos classificadores lineares no contexto da classifica√ß√£o de texto, fornecendo uma vis√£o abrangente e avan√ßada para cientistas de dados e pesquisadores na √°rea de processamento de linguagem natural.

A classifica√ß√£o de texto √© uma tarefa crucial em muitas aplica√ß√µes, como ==filtragem de spam, categoriza√ß√£o de not√≠cias e an√°lise de sentimentos==. Os classificadores lineares oferecem uma abordagem eficiente e interpret√°vel para essas tarefas, ==baseando-se na representa√ß√£o do texto como um vetor de caracter√≠sticas e na aprendizagem de um conjunto de pesos que determinam a import√¢ncia relativa de cada caracter√≠stica para a classifica√ß√£o [2].==

> ‚ö†Ô∏è **Nota Importante**: A efic√°cia dos classificadores lineares ==depende fortemente da escolha adequada das caracter√≠sticas e da capacidade de capturar rela√ß√µes lineares nos dados.== Em cen√°rios onde as rela√ß√µes s√£o altamente n√£o-lineares, podem ser necess√°rias abordagens mais complexas [3].

## Conceitos Fundamentais

| Conceito                     | Explica√ß√£o                                                   |
| ---------------------------- | ------------------------------------------------------------ |
| **Vetor de Caracter√≠sticas** | ==Representa√ß√£o num√©rica de um documento de texto, geralmente usando a abordagem bag-of-words.== Cada dimens√£o corresponde a uma palavra do vocabul√°rio, e o valor indica a contagem ou presen√ßa da palavra no documento [4]. |
| **Vetor de Pesos**           | ==Um vetor Œ∏ que atribui um peso a cada caracter√≠stica, indicando sua import√¢ncia para a classifica√ß√£o==. Estes pesos s√£o aprendidos durante o treinamento do modelo [5]. |
| **Fun√ß√£o de Pontua√ß√£o**      | ==Uma fun√ß√£o linear que calcula a compatibilidade entre um documento e uma classe==, definida como o produto interno entre o vetor de caracter√≠sticas e o vetor de pesos: Œ®(x, y) = Œ∏ ¬∑ f(x, y) [6]. |
| **Margem**                   | A diferen√ßa entre a pontua√ß√£o da classe correta e a pontua√ß√£o da classe incorreta mais pr√≥xima. Maximizar a margem √© um objetivo comum em muitos algoritmos de classifica√ß√£o linear [7]. |

### Representa√ß√£o Matem√°tica

A fun√ß√£o de pontua√ß√£o para um classificador linear √© definida como:

$$
\Psi(x, y) = \theta \cdot f(x, y) = \sum_{j} \theta_j f_j(x, y)
$$

Onde:
- $x$ √© o vetor de caracter√≠sticas do documento
- $y$ √© a classe
- $\theta$ √© o vetor de pesos
- $f(x, y)$ √© a fun√ß√£o de caracter√≠sticas que mapeia o par (documento, classe) para um vetor [8]

> üí° **Destaque**: A fun√ß√£o de caracter√≠sticas $f(x, y)$ permite incorporar informa√ß√µes espec√≠ficas da classe na representa√ß√£o do documento, o que √© crucial para classifica√ß√£o multiclasse eficiente [9].

### Perguntas Te√≥ricas

1. Derive a express√£o para a margem em um classificador linear bin√°rio e explique como ela se relaciona com a no√ß√£o de separabilidade linear.

2. Demonstre matematicamente por que um classificador linear n√£o pode resolver o problema do XOR. Como isso se relaciona com a limita√ß√£o dos perceptrons discutida por Minsky e Papert?

3. Analise teoricamente o impacto da dimensionalidade do espa√ßo de caracter√≠sticas na capacidade de generaliza√ß√£o de um classificador linear. Como isso se relaciona com o fen√¥meno conhecido como "maldi√ß√£o da dimensionalidade"?

## Algoritmos de Classifica√ß√£o Linear

### Naive Bayes

==O classificador Naive Bayes √© um modelo probabil√≠stico que, apesar de sua simplicidade, pode ser interpretado como um classificador linear no espa√ßo de log-probabilidades [10].==

A probabilidade condicional de uma classe y dado um documento x √© calculada como:

$$
p(y|x; \phi) = \frac{\prod_{j=1}^V \phi_{y,j}^{x_j}}{\sum_{y'\in Y} \prod_{j=1}^V \phi_{y',j}^{x_j}}
$$

Onde:
- $\phi_{y,j}$ √© a probabilidade da palavra j na classe y
- $x_j$ √© a contagem da palavra j no documento
- $V$ √© o tamanho do vocabul√°rio [11]

> ‚ùó **Ponto de Aten√ß√£o**: ==A suposi√ß√£o de independ√™ncia condicional entre as caracter√≠sticas, conhecida como a suposi√ß√£o "naive"==, √© crucial para a efici√™ncia computacional do Naive Bayes, mas pode limitar sua precis√£o em alguns cen√°rios [12].

### Perceptron

O algoritmo Perceptron √© um dos classificadores lineares mais antigos e fundamentais. Ele atualiza iterativamente os pesos com base nos erros de classifica√ß√£o [13].

O algoritmo de aprendizagem do Perceptron √© definido como:

```python
def perceptron(x, y, max_iterations):
    theta = np.zeros(len(x[0]))
    for _ in range(max_iterations):
        for i in range(len(x)):
            y_pred = np.sign(np.dot(theta, x[i]))
            if y_pred != y[i]:
                theta += y[i] * x[i]
    return theta
```

> ‚úîÔ∏è **Destaque**: O Perceptron √© garantido convergir para uma solu√ß√£o se os dados forem linearmente separ√°veis. Esta propriedade √© conhecida como o Teorema de Converg√™ncia do Perceptron [14].

### Support Vector Machine (SVM)

O SVM busca encontrar o hiperplano que maximiza a margem entre as classes. Para dados n√£o linearmente separ√°veis, introduz-se vari√°veis de folga Œæ·µ¢ [15].

A formula√ß√£o do problema de otimiza√ß√£o para o SVM √©:

$$
\min_{\theta, \xi} \frac{1}{2} ||\theta||^2 + C \sum_{i=1}^N \xi_i
$$

Sujeito a:
$$
y^{(i)}(\theta \cdot x^{(i)} + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad \forall i
$$

Onde C √© um hiperpar√¢metro que controla o trade-off entre maximizar a margem e minimizar o erro de treinamento [16].

### Regress√£o Log√≠stica

A regress√£o log√≠stica modela a probabilidade de uma classe usando a fun√ß√£o log√≠stica (sigmoid) [17]:

$$
p(y|x; \theta) = \frac{1}{1 + e^{-\theta \cdot x}}
$$

O treinamento √© realizado maximizando a log-verossimilhan√ßa:

$$
\mathcal{L}(\theta) = \sum_{i=1}^N [y^{(i)} \log p(y^{(i)}|x^{(i)}; \theta) + (1-y^{(i)}) \log (1-p(y^{(i)}|x^{(i)}; \theta))]
$$

> üí° **Destaque**: A regress√£o log√≠stica pode ser vista como uma vers√£o probabil√≠stica do SVM, oferecendo n√£o apenas classifica√ß√µes, mas tamb√©m probabilidades bem calibradas [18].

### Perguntas Te√≥ricas

1. Derive a atualiza√ß√£o de gradiente para o algoritmo Perceptron e mostre como ela se relaciona com a minimiza√ß√£o do erro de classifica√ß√£o.

2. Prove que a fun√ß√£o de perda da regress√£o log√≠stica √© convexa em rela√ß√£o aos par√¢metros Œ∏. Como isso afeta o processo de otimiza√ß√£o?

3. Compare teoricamente a capacidade de generaliza√ß√£o do SVM com margens r√≠gidas versus margens suaves. Em que condi√ß√µes cada variante seria prefer√≠vel?

## Otimiza√ß√£o e Regulariza√ß√£o

A otimiza√ß√£o dos par√¢metros em classificadores lineares geralmente envolve a minimiza√ß√£o de uma fun√ß√£o de perda, frequentemente com um termo de regulariza√ß√£o para evitar overfitting [19].

### Gradiente Descendente Estoc√°stico (SGD)

O SGD √© um m√©todo de otimiza√ß√£o amplamente utilizado devido √† sua efici√™ncia em grandes conjuntos de dados. A atualiza√ß√£o dos pesos √© dada por:

$$
\theta^{(t+1)} = \theta^{(t)} - \eta^{(t)} \nabla_\theta \ell(\theta^{(t)}; x^{(i)}, y^{(i)})
$$

Onde $\eta^{(t)}$ √© a taxa de aprendizado na itera√ß√£o t [20].

### Regulariza√ß√£o L2

A regulariza√ß√£o L2 adiciona um termo de penalidade √† fun√ß√£o objetivo:

$$
\mathcal{L}(\theta) = \sum_{i=1}^N \ell(\theta; x^{(i)}, y^{(i)}) + \frac{\lambda}{2} ||\theta||_2^2
$$

Onde $\lambda$ √© o par√¢metro de regulariza√ß√£o que controla a for√ßa da penalidade [21].

> ‚ö†Ô∏è **Nota Importante**: A escolha do par√¢metro de regulariza√ß√£o Œª √© crucial. Um valor muito alto pode levar a underfitting, enquanto um valor muito baixo pode resultar em overfitting [22].

### Perguntas Te√≥ricas

1. Derive a express√£o para o gradiente da fun√ß√£o de perda regularizada L2 para a regress√£o log√≠stica. Como a regulariza√ß√£o afeta a atualiza√ß√£o dos pesos?

2. Analise teoricamente o comportamento assint√≥tico do SGD em um problema de otimiza√ß√£o convexa. Sob quais condi√ß√µes a converg√™ncia √© garantida?

3. Compare matematicamente os efeitos da regulariza√ß√£o L1 e L2 na esparsidade dos pesos aprendidos. Como isso se relaciona com a sele√ß√£o de caracter√≠sticas?

## Avalia√ß√£o e Interpreta√ß√£o

A avalia√ß√£o de classificadores lineares envolve m√©tricas como acur√°cia, precis√£o, recall e F1-score. A interpreta√ß√£o dos pesos aprendidos pode fornecer insights valiosos sobre a import√¢ncia relativa das caracter√≠sticas [23].

### An√°lise de Pesos

Os pesos Œ∏_j podem ser interpretados como a import√¢ncia relativa da caracter√≠stica j para a classifica√ß√£o. Caracter√≠sticas com pesos positivos altos s√£o indicativas da classe positiva, enquanto pesos negativos altos s√£o indicativos da classe negativa [24].

### Calibra√ß√£o de Probabilidades

Para modelos que produzem probabilidades (como regress√£o log√≠stica), a calibra√ß√£o √© importante. T√©cnicas como Platt Scaling podem ser usadas para melhorar a calibra√ß√£o [25].

> üí° **Destaque**: A interpretabilidade dos classificadores lineares √© uma de suas principais vantagens, permitindo insights diretos sobre o processo de decis√£o do modelo [26].

## Conclus√£o

Os classificadores lineares s√£o ferramentas poderosas e vers√°teis para a classifica√ß√£o de texto, oferecendo um equil√≠brio entre simplicidade, efici√™ncia computacional e interpretabilidade. Sua formula√ß√£o matem√°tica elegante permite an√°lises te√≥ricas profundas, enquanto sua aplicabilidade pr√°tica os torna indispens√°veis em muitas tarefas de processamento de linguagem natural [27].

Embora tenham limita√ß√µes em capturar rela√ß√µes n√£o-lineares complexas, os avan√ßos em t√©cnicas de engenharia de caracter√≠sticas e m√©todos de kernel expandiram significativamente sua aplicabilidade. A compreens√£o profunda dos fundamentos te√≥ricos dos classificadores lineares √© essencial para o desenvolvimento e aplica√ß√£o eficaz de t√©cnicas mais avan√ßadas em aprendizado de m√°quina e processamento de linguagem natural [28].

## Perguntas Te√≥ricas Avan√ßadas

1. Derive a express√£o para o dual do problema de otimiza√ß√£o do SVM e explique como isso leva √† formula√ß√£o do "kernel trick". Como isso se relaciona com o Teorema de Representer?

2. Analise teoricamente o trade-off entre vi√©s e vari√¢ncia em classificadores lineares. Como a complexidade do modelo (por exemplo, atrav√©s da regulariza√ß√£o) afeta este trade-off?

3. Desenvolva uma prova formal para o Teorema de Converg√™ncia do Perceptron. Quais s√£o as implica√ß√µes deste teorema para dados n√£o linearmente separ√°veis?

4. Compare teoricamente a capacidade expressiva de classificadores lineares com redes neurais de uma camada oculta. Em que condi√ß√µes um classificador linear pode aproximar arbitrariamente bem uma fun√ß√£o de decis√£o n√£o-linear?

5. Analise o impacto da maldi√ß√£o da dimensionalidade na performance de classificadores lineares. Como t√©cnicas de redu√ß√£o de dimensionalidade, como PCA, afetam teoricamente a capacidade de generaliza√ß√£o destes modelos?

## Refer√™ncias

[1] "To predict a label from a bag-of-words, we can assign a score to each word in the vocabulary, measuring the compatibility with the label." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[2] "For example, for the label FICTION, we might assign a positive score to the word whale, and a negative score to the word molybdenum. These scores are called weights, and they are arranged in a column vector Œ∏." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[3] "In a linear bag-of-words classifier, this score is the vector inner product between the weights Œ∏ and the output of a feature function f(x, y)." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[4] "As the notation suggests, f is a function of two arguments, the word counts x and the label y, and it returns a vector output." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[5] "The corresponding weight Œ∏_j then scores the compatibility of the word whale with the label FICTION." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[6] "Œ®(x, y) = Œ∏ ¬∑ f(x, y) = ‚àë Œ∏_j f_j(x, y)." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[7] "The output of the feature function can be formalized as a vector:" *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[8] "This is identical to the "na√Øve" independence assumption implied by the multinomial distribution, and as a result, the optimal parameters for this model are identical to those in multinomial Na√Øve Bayes." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[9] "For example, man bites dog and dog bites man correspond to an identical count vector, {bites : 1, dog : 1, man : 1}, and B(x) is equal to the total number of possible word orderings for count vector x." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[10] "The Na√Øve Bayes prediction rule is to choose the label y which maximizes log p(x, y; Œº, œï):" *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[11] "= log B(x) + ‚àë(j=1 to V) x_j log œï_y,j + log Œº_y" *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[12] "This is called Laplace smoothing." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[13] "Algorithm 3 Perceptron learning algorithm" *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[14] "Linear separability is important because of the following guarantee: if your data is linearly separable, then the perceptron algorithm will find a separator (Novikoff, 1962)." *(Trecho de CHAPTER 2. LINEAR TEXT CLASSIFICATION)*

[15] "In the support vector machine, the objective is the regularized margin loss," *(Tr