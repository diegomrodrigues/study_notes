# Classifica√ß√£o de Texto com Multinomial Na√Øve Bayes

```mermaid
graph TB
    subgraph "Pr√©-processamento"
        A["Documentos de Texto"] --> B["Tokeniza√ß√£o"]
        B --> C["Remo√ß√£o de Stop Words"]
        C --> D["Stemming/Lematiza√ß√£o"]
    end

    subgraph "Cria√ß√£o do Vocabul√°rio"
        D --> E["Extra√ß√£o de Caracter√≠sticas"]
        E --> F["Cria√ß√£o do Vocabul√°rio"]
    end

    subgraph "Representa√ß√£o dos Documentos"
        F --> G["Vetoriza√ß√£o (Bag-of-Words)"]
    end

    subgraph "Treinamento do Modelo"
        G --> H["C√°lculo das Probabilidades a Priori P(y)"]
        G --> I["Estima√ß√£o dos Par√¢metros œÜ_y,j"]
        J["Suaviza√ß√£o de Laplace"] --> I
    end

    subgraph "Fase de Predi√ß√£o"
        K["Novo Documento"] --> L["Pr√©-processamento"]
        L --> M["Vetoriza√ß√£o"]
        M --> N["C√°lculo do Score para cada Classe"]
        H --> N
        I --> N
        N --> O["Sele√ß√£o da Classe com Maior Score"]
    end
```

## Introdu√ß√£o

O **Multinomial Na√Øve Bayes** √© um algoritmo fundamental em **classifica√ß√£o de texto**, ==particularmente eficaz quando utilizamos a representa√ß√£o de **bag-of-words**==. Este m√©todo combina o princ√≠pio do **Na√Øve Bayes** com a **distribui√ß√£o multinomial** para ==modelar a ocorr√™ncia de palavras em documentos, assumindo a independ√™ncia condicional das palavras dado o r√≥tulo do documento.== Essa suposi√ß√£o simplifica significativamente o processo de infer√™ncia, tornando o algoritmo computacionalmente eficiente.

A efic√°cia do Multinomial Na√Øve Bayes em tarefas de classifica√ß√£o de texto, como ==filtragem de spam, categoriza√ß√£o de not√≠cias e an√°lise de sentimentos==, deve-se √† sua capacidade de lidar com dados de alta dimensionalidade e ao fato de que, em muitas aplica√ß√µes, ==as depend√™ncias entre palavras podem ser consideradas negligenci√°veis para efeitos pr√°ticos==. Al√©m disso, apesar de sua simplicidade, o Multinomial Na√Øve Bayes frequentemente alcan√ßa desempenho competitivo em compara√ß√£o com modelos mais complexos.

## Conceitos Fundamentais

| Conceito                      | Explica√ß√£o                                                   |
| ----------------------------- | ------------------------------------------------------------ |
| **Bag-of-Words**              | Representa√ß√£o de um documento como um vetor de contagem de palavras, ignorando a ordem e a estrutura gramatical. Cada posi√ß√£o do vetor corresponde a uma palavra do vocabul√°rio, e o valor √© a frequ√™ncia dessa palavra no documento. |
| **Distribui√ß√£o Multinomial**  | ==Modelo probabil√≠stico que descreve a probabilidade de contagens de eventos discretos em m√∫ltiplas categorias==, adequado para modelar a frequ√™ncia de palavras em documentos. ==√â uma generaliza√ß√£o da distribui√ß√£o binomial para mais de duas categorias.== |
| **Independ√™ncia Condicional** | Suposi√ß√£o de que as ocorr√™ncias de palavras s√£o independentes entre si, dado o r√≥tulo do documento. ==Isso permite que a probabilidade conjunta das palavras seja fatorada como o produto das probabilidades individuais.== |

> ‚ö†Ô∏è **Nota Importante**: Embora a suposi√ß√£o de independ√™ncia condicional n√£o seja realista em muitos contextos (j√° que palavras em um texto geralmente est√£o correlacionadas), ela √© crucial para a tratabilidade computacional do modelo Na√Øve Bayes, permitindo uma fatora√ß√£o eficiente da probabilidade conjunta. Na pr√°tica, essa simplifica√ß√£o muitas vezes n√£o compromete significativamente o desempenho do classificador.

### Formula√ß√£o Matem√°tica do Multinomial Na√Øve Bayes

O Multinomial Na√Øve Bayes modela a probabilidade de um documento $x$ pertencer a uma classe espec√≠fica $y$ usando o **Teorema de Bayes**:

$$
p(y|x) = \frac{p(x|y)p(y)}{p(x)}
$$

Como $p(x)$ √© constante para todas as classes durante a classifica√ß√£o, podemos focar no numerador $p(x|y)p(y)$.

#### Verossimilhan√ßa

==A verossimilhan√ßa $p(x|y)$ √© modelada usando a **distribui√ß√£o multinomial**, que captura a probabilidade de observar uma certa combina√ß√£o de contagens de palavras em um documento da classe $y$:==
$$
p_{\text{mult}}(x; \phi_y) = \frac{N_{x}!}{\prod_{j=1}^V x_j!} \prod_{j=1}^V \phi_{y,j}^{x_j}
$$

Onde:

- $\phi_{y,j} = p(w_j|y)$ √© a probabilidade de observar a palavra $w_j$ na classe $y$.
- $x_j$ √© a contagem da palavra $w_j$ no documento $x$.
- $V$ √© o tamanho do vocabul√°rio.
- $N_{x} = \sum_{j=1}^V x_j$ √© o n√∫mero total de palavras no documento $x$.
- O coeficiente multinomial $\frac{N_{x}!}{\prod_{j=1}^V x_j!}$ √© constante dado $x$ e pode ser ignorado durante a classifica√ß√£o.

#### Probabilidade a Priori

A probabilidade a priori $p(y)$ √© a propor√ß√£o de documentos pertencentes √† classe $y$ no conjunto de treinamento.

#### Classifica√ß√£o

A predi√ß√£o √© feita selecionando a classe que maximiza a probabilidade a posteriori:

$$
\hat{y} = \arg\max_{y} \left[ \log p(y) + \sum_{j=1}^V x_j \log \phi_{y,j} \right]
$$

==O uso do logaritmo transforma o produto em uma soma, facilitando os c√°lculos e evitando problemas de underflow num√©rico.==

### Estima√ß√£o de Par√¢metros

==Os par√¢metros $\phi_{y,j}$ representam a probabilidade de observar a palavra $w_j$ em documentos da classe $y$.== Eles s√£o ==estimados a partir do conjunto de treinamento usando a **estimativa de m√°xima verossimilhan√ßa (MLE)**:==
$$
\phi_{y,j} = \frac{\text{contagem}(w_j, y)}{\sum_{k=1}^V \text{contagem}(w_k, y)}
$$

Onde:

- $\text{contagem}(w_j, y)$ √© o n√∫mero total de ocorr√™ncias da palavra $w_j$ em todos os documentos de classe $y$.
- O denominador √© o n√∫mero total de palavras em todos os documentos de classe $y$.

#### Problema das Contagens Zero

Quando uma palavra n√£o aparece em nenhum documento de uma classe no conjunto de treinamento, a estimativa $\phi_{y,j}$ ser√° zero. ==Isso √© problem√°tico porque qualquer documento contendo essa palavra receberia probabilidade zero para essa classe.==

> ‚úîÔ∏è **Destaque**: Para evitar probabilidades zero e melhorar a generaliza√ß√£o do modelo, √© comum usar **suaviza√ß√£o de Laplace** (add-one smoothing).

### Suaviza√ß√£o de Laplace

A **suaviza√ß√£o de Laplace** adiciona um valor constante $\alpha > 0$ (tipicamente $\alpha = 1$) a cada contagem de palavra, ajustando a estimativa dos par√¢metros:

$$
\phi_{y,j} = \frac{\alpha + \text{contagem}(w_j, y)}{V \alpha + \sum_{k=1}^V \text{contagem}(w_k, y)}
$$

==Isso garante que todas as probabilidades $\phi_{y,j}$ sejam positivas e que a soma das probabilidades sobre todo o vocabul√°rio seja igual a 1.==

#### Impacto da Suaviza√ß√£o

A suaviza√ß√£o de Laplace tem o efeito de incorporar uma distribui√ß√£o a priori uniforme sobre as palavras, o que √© particularmente √∫til quando se lida com palavras raras ou n√£o vistas no conjunto de treinamento. Ela introduz um trade-off entre o vi√©s e a vari√¢ncia do estimador:

- **Redu√ß√£o de Vari√¢ncia**: A suaviza√ß√£o reduz a vari√¢ncia dos estimadores $\phi_{y,j}$ ao evitar estimativas extremas (0 ou 1).
- **Aumento de Vi√©s**: Ao adicionar contagens artificiais, introduz-se um pequeno vi√©s nas estimativas.

No geral, a suaviza√ß√£o melhora a capacidade de generaliza√ß√£o do modelo em dados n√£o vistos.

#### Perguntas Te√≥ricas

1. **Derive a estimativa de m√°xima verossimilhan√ßa para o par√¢metro $\phi_{y,j}$ no modelo Na√Øve Bayes, considerando a distribui√ß√£o multinomial.**

   **Resposta**: A fun√ß√£o de verossimilhan√ßa para a classe $y$ √©:

   $$
   L(\phi_{y}) = \prod_{i:y^{(i)}=y} p_{\text{mult}}(x^{(i)}; \phi_{y})
   $$

   Onde $x^{(i)}$ s√£o os documentos de treinamento pertencentes √† classe $y$. Tomando o logaritmo da fun√ß√£o de verossimilhan√ßa e derivando em rela√ß√£o a $\phi_{y,j}$, aplicando a restri√ß√£o de que $\sum_{j=1}^V \phi_{y,j} = 1$, obtemos:

   $$
   \phi_{y,j} = \frac{\sum_{i:y^{(i)}=y} x_j^{(i)}}{\sum_{k=1}^V \sum_{i:y^{(i)}=y} x_k^{(i)}}
   $$

   Que √© a frequ√™ncia relativa da palavra $w_j$ nos documentos de classe $y$.

2. **Demonstre matematicamente por que a suaviza√ß√£o de Laplace √© necess√°ria e como ela afeta a estimativa dos par√¢metros $\phi_{y,j}$.**

   **Resposta**: Sem suaviza√ß√£o, se $\text{contagem}(w_j, y) = 0$, ent√£o $\phi_{y,j} = 0$. Isso leva a problemas porque $\log \phi_{y,j}$ ser√° indefinido (tende a $-\infty$), anulando a probabilidade total do documento se $x_j > 0$. A suaviza√ß√£o de Laplace adiciona uma pseudocontagem $\alpha$ a cada $\text{contagem}(w_j, y)$, garantindo que $\phi_{y,j} > 0$ para todas as palavras. Matematicamente, a suaviza√ß√£o afeta a estimativa dos par√¢metros ao ajustar as contagens:

   $$
   \phi_{y,j} = \frac{\alpha + \text{contagem}(w_j, y)}{V \alpha + \sum_{k=1}^V \text{contagem}(w_k, y)}
   $$

3. **Analise teoricamente o impacto da suposi√ß√£o de independ√™ncia condicional na performance do Multinomial Na√Øve Bayes em compara√ß√£o com modelos que n√£o fazem essa suposi√ß√£o.**

   **Resposta**: A suposi√ß√£o de independ√™ncia condicional simplifica o modelo, permitindo estimar as probabilidades de forma eficiente. No entanto, em muitos casos, as palavras em um texto n√£o s√£o realmente independentes; h√° correla√ß√µes e depend√™ncias sem√¢nticas importantes. Modelos que capturam essas depend√™ncias, como modelos de linguagem baseados em redes neurais ou modelos de n-gramas, podem potencialmente oferecer melhor performance ao custo de maior complexidade computacional e necessidade de mais dados para treinamento. O Multinomial Na√Øve Bayes pode apresentar desempenho inferior quando as depend√™ncias entre palavras s√£o cr√≠ticas para a tarefa de classifica√ß√£o.

## Implementa√ß√£o e Algoritmo

A implementa√ß√£o do Multinomial Na√Øve Bayes envolve duas fases principais: **treinamento** e **predi√ß√£o**.

### Fase de Treinamento

1. **Calcular as probabilidades a priori $p(y)$**:

   $$ p(y) = \frac{\text{n√∫mero de documentos da classe } y}{\text{n√∫mero total de documentos}} $$

2. **Estimar os par√¢metros $\phi_{y,j}$** para cada classe $y$ e palavra $w_j$ usando a suaviza√ß√£o de Laplace:

   $$
   \phi_{y,j} = \frac{\alpha + \text{contagem}(w_j, y)}{V \alpha + \sum_{k=1}^V \text{contagem}(w_k, y)}
   $$

### Fase de Predi√ß√£o

Para um novo documento $x$, representado como um vetor de contagens de palavras, calcular o score para cada classe $y$:

$$
\text{score}(y) = \log p(y) + \sum_{j=1}^V x_j \log \phi_{y,j}
$$

A classe predita $\hat{y}$ √© aquela com o maior score:

$$
\hat{y} = \arg\max_{y} \text{score}(y)
$$

> üí° **Dica**: Trabalhar no logaritmo das probabilidades evita problemas de underflow num√©rico e converte produtos em somas, facilitando os c√°lculos.

### Implementa√ß√£o em Python

Abaixo, uma implementa√ß√£o eficiente do Multinomial Na√Øve Bayes usando o Scikit-learn e matrizes esparsas para lidar com grandes conjuntos de dados de texto:

```python
import numpy as np
from scipy.sparse import csr_matrix

class MultinomialNB:
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        
    def fit(self, X: csr_matrix, y):
        n_samples, n_features = X.shape
        self.classes = np.unique(y)
        n_classes = len(self.classes)
        
        # Calcular as probabilidades a priori log p(y)
        class_count = np.bincount(y)
        self.class_log_prior_ = np.log(class_count) - np.log(n_samples)
        
        # Inicializar matrizes
        self.feature_log_prob_ = np.zeros((n_classes, n_features))
        
        # Calcular log œÜ_{y,j} para cada classe
        for idx, c in enumerate(self.classes):
            X_c = X[y == c]
            # Contagens de palavras com suaviza√ß√£o
            smoothed_wc = X_c.sum(axis=0) + self.alpha
            smoothed_total_wc = smoothed_wc.sum()
            self.feature_log_prob_[idx, :] = np.log(smoothed_wc) - np.log(smoothed_total_wc)
        
    def predict(self, X: csr_matrix):
        jll = self._joint_log_likelihood(X)
        return self.classes[np.argmax(jll, axis=1)]
        
    def _joint_log_likelihood(self, X: csr_matrix):
        return X.dot(self.feature_log_prob_.T) + self.class_log_prior_
```

Nesta implementa√ß√£o:

- Utilizamos matrizes esparsas (`csr_matrix`) para efici√™ncia com dados de alta dimensionalidade.
- As probabilidades s√£o calculadas no logaritmo para estabilidade num√©rica.
- A suaviza√ß√£o de Laplace √© aplicada nas contagens de palavras.

## Vantagens e Desvantagens

| üëç **Vantagens**                                              | üëé **Desvantagens**                                           |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Simplicidade e efici√™ncia computacional**: O algoritmo √© f√°cil de implementar e r√°pido, mesmo em grandes conjuntos de dados. | **Suposi√ß√£o de independ√™ncia condicional**: Pode ser irrealista, j√° que palavras em um texto geralmente n√£o s√£o independentes, afetando potencialmente a performance. |
| **Requer poucos dados para treinamento**: Funciona bem mesmo com conjuntos de dados relativamente pequenos. | **Sensibilidade a caracter√≠sticas irrelevantes ou correlacionadas**: Caracter√≠sticas que n√£o s√£o informativas ou que est√£o correlacionadas podem afetar negativamente o modelo. |
| **Escalabilidade**: Escala bem com o n√∫mero de classes e caracter√≠sticas (palavras no vocabul√°rio). | **Inferior a modelos complexos em algumas tarefas**: Modelos mais sofisticados podem superar o Multinomial Na√Øve Bayes em tarefas onde as depend√™ncias entre palavras s√£o importantes. |
| **Interpreta√ß√£o direta das probabilidades**: As probabilidades estimadas t√™m uma interpreta√ß√£o clara. | **Estimativas de probabilidade podem ser imprecisas**: Devido √†s suposi√ß√µes simplificadoras, as probabilidades preditas podem n√£o refletir com precis√£o as probabilidades reais. |

## An√°lise Te√≥rica Avan√ßada

### Rela√ß√£o com Modelos Log-lineares

O Multinomial Na√Øve Bayes pode ser interpretado como um caso especial de um modelo **log-linear generativo**. Embora o Na√Øve Bayes seja um modelo generativo (modela $p(x|y)$), sua forma logar√≠tmica das probabilidades a posteriori √© linear em rela√ß√£o √†s caracter√≠sticas, similar a um modelo discriminativo como a **regress√£o log√≠stica**. Especificamente, podemos escrever:

$$
\log p(y|x) = \log p(y) + \sum_{j=1}^V x_j \log \phi_{y,j} + \text{constante}
$$

Isso mostra que o classificador toma a forma de um **classificador linear**, onde os pesos s√£o $\log \phi_{y,j}$ e o bias √© $\log p(y)$. A diferen√ßa fundamental √© que, no Na√Øve Bayes, os par√¢metros $\phi_{y,j}$ s√£o estimados de forma generativa a partir de $p(x|y)$, enquanto na regress√£o log√≠stica os par√¢metros s√£o ajustados discriminativamente para maximizar diretamente $p(y|x)$.

### An√°lise de Complexidade

- **Complexidade de Tempo de Treinamento**: $O(N D)$, onde $N$ √© o n√∫mero de documentos e $D$ √© o n√∫mero de caracter√≠sticas (tamanho do vocabul√°rio). O tempo √© linear no n√∫mero de documentos e palavras.
- **Complexidade de Tempo de Predi√ß√£o**: Tamb√©m $O(D)$ por documento, pois envolve calcular um score linear em rela√ß√£o √†s caracter√≠sticas.
- **Complexidade de Espa√ßo**: $O(C D)$, onde $C$ √© o n√∫mero de classes. Precisamos armazenar os par√¢metros $\phi_{y,j}$ para cada classe e caracter√≠stica.

### Teorema de Bayes e M√°xima Verossimilhan√ßa

A estimativa dos par√¢metros $\phi_{y,j}$ por m√°xima verossimilhan√ßa pode ser vista como a maximiza√ß√£o da probabilidade conjunta dos dados observados:

$$
\mathcal{L}(\phi, p(y)) = \prod_{i=1}^N p(y^{(i)}) p(x^{(i)}|y^{(i)}; \phi_{y^{(i)}})
$$

Tomando o logaritmo:

$$
\log \mathcal{L} = \sum_{i=1}^N \left[ \log p(y^{(i)}) + \sum_{j=1}^V x_j^{(i)} \log \phi_{y^{(i)},j} \right] + \text{constante}
$$

Maximizar $\log \mathcal{L}$ em rela√ß√£o aos par√¢metros $\phi_{y,j}$ sob a restri√ß√£o $\sum_{j=1}^V \phi_{y,j} = 1$ leva √†s estimativas de frequ√™ncia relativa das palavras nas classes.

#### Estimativa de M√°xima a Posteriori (MAP)

Se assumirmos uma distribui√ß√£o a priori Dirichlet sobre $\phi_{y}$, a estimativa MAP dos par√¢metros coincide com a suaviza√ß√£o de Laplace, onde $\alpha$ s√£o os par√¢metros da distribui√ß√£o a priori.

#### Perguntas Te√≥ricas Avan√ßadas

1. **Derive a forma da fronteira de decis√£o entre duas classes no espa√ßo de caracter√≠sticas para o Multinomial Na√Øve Bayes. Como essa fronteira se compara com a de um classificador de regress√£o log√≠stica?**

   **Discuss√£o**: No Multinomial Na√Øve Bayes, a fronteira de decis√£o entre duas classes √© definida pela equa√ß√£o onde as probabilidades a posteriori s√£o iguais:

   $$
   \log p(y_1) + \sum_{j=1}^V x_j \log \phi_{y_1,j} = \log p(y_2) + \sum_{j=1}^V x_j \log \phi_{y_2,j}
   $$

   Essa fronteira √© linear nas contagens de palavras $x_j$, similar √† regress√£o log√≠stica. No entanto, enquanto a regress√£o log√≠stica ajusta os pesos para maximizar a separa√ß√£o entre as classes nos dados de treinamento, o Na√Øve Bayes estima os pesos de forma generativa.

2. **Analise teoricamente o comportamento assint√≥tico do Multinomial Na√Øve Bayes √† medida que o n√∫mero de amostras de treinamento tende ao infinito. Sob quais condi√ß√µes o classificador converge para o classificador de Bayes √≥timo?**

   **Discuss√£o**: √Ä medida que o n√∫mero de amostras tende ao infinito, as estimativas de $\phi_{y,j}$ convergem para as verdadeiras probabilidades condicionais $p(w_j|y)$. Se a suposi√ß√£o de independ√™ncia condicional for satisfeita nos dados reais, o classificador Multinomial Na√Øve Bayes convergir√° para o classificador de Bayes √≥timo. No entanto, se essa suposi√ß√£o for violada, o classificador pode n√£o alcan√ßar a performance √≥tima, mesmo com dados infinitos.

3. **Desenvolva uma prova formal da consist√™ncia do estimador de m√°xima verossimilhan√ßa para os par√¢metros do Multinomial Na√Øve Bayes, assumindo que os dados s√£o gerados pelo modelo verdadeiro.**

   **Discuss√£o**: A consist√™ncia do estimador de m√°xima verossimilhan√ßa (MLE) implica que, conforme o n√∫mero de amostras tende ao infinito, o estimador converge em probabilidade para o valor verdadeiro do par√¢metro. Isso pode ser demonstrado mostrando que a fun√ß√£o de verossimilhan√ßa √© concava e que as condi√ß√µes regulares para a consist√™ncia do MLE s√£o satisfeitas no caso multinomial.

4. **Derive a express√£o para a informa√ß√£o m√∫tua entre as caracter√≠sticas e o r√≥tulo da classe no contexto do Multinomial Na√Øve Bayes. Como isso se relaciona com a capacidade preditiva do modelo?**

   **Discuss√£o**: A informa√ß√£o m√∫tua entre uma caracter√≠stica $w_j$ e a classe $y$ √© dada por:

   $$
   I(w_j; y) = \sum_{y} \sum_{w_j} p(w_j, y) \log \frac{p(w_j, y)}{p(w_j) p(y)}
   $$

   A informa√ß√£o m√∫tua mede a redu√ß√£o de incerteza sobre $y$ dado o conhecimento de $w_j$. Caracter√≠sticas com alta informa√ß√£o m√∫tua s√£o mais informativas para a classifica√ß√£o, e essa medida pode ser usada para sele√ß√£o de caracter√≠sticas no modelo.

5. **Formule e prove um teorema que estabele√ßa limites superiores no erro de generaliza√ß√£o do Multinomial Na√Øve Bayes em termos do n√∫mero de amostras de treinamento e da dimensionalidade do espa√ßo de caracter√≠sticas.**

   **Discuss√£o**: Um poss√≠vel teorema poderia relacionar o erro de generaliza√ß√£o com a **complexidade de Vapnik-Chervonenkis (VC)** do modelo. No caso do Na√Øve Bayes, como se trata de um classificador linear, podemos usar resultados da teoria da aprendizagem estat√≠stica para estabelecer que, com alta probabilidade, o erro de generaliza√ß√£o √© limitado pelo erro no conjunto de treinamento mais um termo que decai com o aumento do n√∫mero de amostras e cresce com a dimensionalidade.

## Conclus√£o

O **Multinomial Na√Øve Bayes** √© um classificador poderoso e eficiente para tarefas de **classifica√ß√£o de texto**, especialmente adequado para dados de alta dimensionalidade e com representa√ß√µes esparsas, como √© comum em processamento de linguagem natural. Sua simplicidade matem√°tica e efici√™ncia computacional permitem treinamento e predi√ß√£o r√°pidos, tornando-o uma escolha atraente em cen√°rios onde os recursos computacionais s√£o limitados ou quando se necessita de um modelo b√°sico de refer√™ncia.

Apesar de suas limita√ß√µes, como a suposi√ß√£o de independ√™ncia condicional entre as palavras, o Multinomial Na√Øve Bayes frequentemente alcan√ßa desempenho competitivo em compara√ß√£o com modelos mais complexos, especialmente quando o volume de dados √© limitado. Al√©m disso, sua interpretabilidade e facilidade de implementa√ß√£o fazem dele uma ferramenta valiosa no arsenal de qualquer cientista de dados.

Em suma, o Multinomial Na√Øve Bayes permanece relevante e √∫til em muitas aplica√ß√µes pr√°ticas, servindo tanto como um ponto de partida para o desenvolvimento de modelos mais sofisticados quanto como uma solu√ß√£o eficiente para problemas de classifica√ß√£o de texto em larga escala.