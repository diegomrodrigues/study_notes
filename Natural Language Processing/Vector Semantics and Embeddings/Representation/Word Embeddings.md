# Word Embeddings: Representa√ß√£o de Palavras em Espa√ßos Sem√¢nticos Multidimensionais

<imagem: Uma visualiza√ß√£o tridimensional de um espa√ßo vetorial sem√¢ntico, mostrando palavras como pontos coloridos em diferentes clusters, com setas indicando rela√ß√µes entre palavras semanticamente pr√≥ximas.>

## Introdu√ß√£o

Os **word embeddings** representam uma revolu√ß√£o na forma como processamos e entendemos a linguagem natural computacionalmente. ==Essas representa√ß√µes vetoriais densas de palavras em espa√ßos sem√¢nticos multidimensionais s√£o fundamentais para muitas aplica√ß√µes modernas de **Processamento de Linguagem Natural (NLP)** e aprendizado de m√°quina [1]==. O conceito central de **sem√¢ntica vetorial** baseia-se na ideia de que ==o significado de uma palavra pode ser capturado por sua distribui√ß√£o em um corpus de texto==, uma no√ß√£o que tem ra√≠zes na **lingu√≠stica distribucional** dos anos 1950 [2].

> ‚úîÔ∏è **Destaque**: A sem√¢ntica vetorial modela cada palavra como um vetor ‚Äî um ponto em um espa√ßo de alta dimens√£o, tamb√©m chamado de **embedding** [3].

Essa abordagem permite que modelos computacionais capturem nuances sem√¢nticas e rela√ß√µes entre palavras de forma muito mais eficaz do que m√©todos tradicionais baseados em dicion√°rios ou regras expl√≠citas. Ao transformar palavras em vetores num√©ricos, torna-se poss√≠vel aplicar t√©cnicas matem√°ticas e algor√≠tmicas avan√ßadas para analisar a linguagem natural.

## Conceitos Fundamentais

| Conceito                        | Explica√ß√£o                                                   |
| ------------------------------- | ------------------------------------------------------------ |
| **Embedding**                   | Representa√ß√£o de uma palavra como um vetor denso em um espa√ßo multidimensional. Tipicamente, esses vetores t√™m entre 50 e 1000 dimens√µes [4]. |
| **Espa√ßo Sem√¢ntico**            | O espa√ßo multidimensional no qual as palavras s√£o representadas como pontos. A proximidade nesse espa√ßo indica similaridade sem√¢ntica [5]. |
| **Vetores Esparsos vs. Densos** | ==Vetores esparsos t√™m muitos zeros e poucas dimens√µes n√£o nulas, enquanto vetores densos t√™m valores em todas as dimens√µes==. Embeddings modernos s√£o geralmente densos [6]. |

### Hip√≥tese Distribucional

A base te√≥rica para os word embeddings √© a **hip√≥tese distribucional**, formulada por linguistas como Joos (1950), Harris (1954) e Firth (1957) [7]. Essa hip√≥tese afirma que:

> ‚ùó **Ponto de Aten√ß√£o**: *"Voc√™ conhecer√° uma palavra pela companhia que ela mant√©m."* ‚Äî J.R. Firth [8].

Em termos pr√°ticos, isso significa que palavras que ocorrem em contextos similares tendem a ter significados similares. Essa intui√ß√£o √© formalizada matematicamente nos modelos de embedding, onde a similaridade entre vetores de palavras (geralmente medida pelo cosseno do √¢ngulo entre eles) corresponde √† similaridade sem√¢ntica entre as palavras.

### Motiva√ß√£o para Word Embeddings

Tradicionalmente, a representa√ß√£o de palavras em NLP era feita atrav√©s de vetores de caracter√≠sticas esparsos, como o **one-hot encoding**, onde cada palavra √© representada por um vetor bin√°rio com dimens√£o igual ao tamanho do vocabul√°rio. Essa abordagem tem v√°rias limita√ß√µes:

- **Alta Dimensionalidade**: O tamanho do vocabul√°rio pode ser muito grande, resultando em vetores de alta dimensionalidade, o que √© computacionalmente ineficiente.
- **Esparsidade**: A maioria dos valores nos vetores √© zero, dificultando a captura de rela√ß√µes sem√¢nticas entre palavras.
- **Falta de Generaliza√ß√£o**: N√£o captura similaridades entre palavras diferentes.

Os word embeddings superam essas limita√ß√µes ao representar palavras em vetores densos de dimens√µes mais baixas, onde a posi√ß√£o de uma palavra no espa√ßo vetorial reflete seu significado sem√¢ntico.

### Tipos de Modelos de Embedding

1. **Modelos Baseados em Contagem**:
   - Utilizam estat√≠sticas de coocorr√™ncia de palavras em um corpus.
   - Exemplos incluem **tf-idf** e **PPMI** (Positive Pointwise Mutual Information) [9].
   - **Vantagens**: Intuitivos, capturam informa√ß√µes globais do corpus.
   - **Desvantagens**: Resultam em vetores esparsos e de alta dimensionalidade, menos eficientes.

2. **Modelos Preditivos**:
   - Aprendem embeddings atrav√©s de tarefas de previs√£o em redes neurais.
   - Exemplos incluem **Word2Vec**, **GloVe** e **FastText** [10].
   - **Vantagens**: Produzem vetores densos e capturam rela√ß√µes sem√¢nticas complexas.
   - **Desvantagens**: Requerem mais recursos computacionais para treinamento.

## Matriz Termo-Documento e Termo-Termo

==As representa√ß√µes vetoriais de palavras s√£o frequentemente baseadas em dois tipos principais de matrizes que refletem diferentes abordagens para capturar informa√ß√µes sem√¢nticas.==

### Matriz Termo-Documento

$$
M_{TD} = \begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1D} \\
w_{21} & w_{22} & \cdots & w_{2D} \\
\vdots & \vdots & \ddots & \vdots \\
w_{V1} & w_{V2} & \cdots & w_{VD}
\end{bmatrix}
$$

Onde:

- $w_{ij}$ √© o peso da palavra $i$ no documento $j$, que pode ser a frequ√™ncia ou uma medida ponderada.
- $V$ √© o tamanho do vocabul√°rio.
- $D$ √© o n√∫mero de documentos.

Esta matriz captura a frequ√™ncia (ou uma fun√ß√£o ponderada dela) de cada palavra em cada documento do corpus [11]. √â amplamente utilizada em tarefas de recupera√ß√£o de informa√ß√£o.

### Matriz Termo-Termo

$$
M_{TT} = \begin{bmatrix}
c_{11} & c_{12} & \cdots & c_{1V} \\
c_{21} & c_{22} & \cdots & c_{2V} \\
\vdots & \vdots & \ddots & \vdots \\
c_{V1} & c_{V2} & \cdots & c_{VV}
\end{bmatrix}
$$

Onde:

- ==$c_{ij}$ √© uma medida de coocorr√™ncia entre as palavras $i$ e $j$, como a frequ√™ncia com que aparecem pr√≥ximas no texto.==
- $V$ √© o tamanho do vocabul√°rio.

Esta matriz captura as rela√ß√µes entre palavras baseadas em suas coocorr√™ncias em contextos similares [12]. √â fundamental para modelos baseados em contagem.

> üí° **Insight**: A matriz termo-termo permite capturar rela√ß√µes sem√¢nticas mais refinadas entre palavras, incluindo similaridades de segunda ordem e estruturas latentes no corpus [13].

### Perguntas Te√≥ricas

1. **Deriva√ß√£o Matem√°tica da Similaridade do Cosseno**:
   - Derive como a similaridade do cosseno entre dois vetores de palavras na matriz termo-documento se relaciona com a frequ√™ncia relativa dessas palavras nos mesmos documentos.
   - *Dica*: Considere a normaliza√ß√£o dos vetores de frequ√™ncia e como o produto escalar reflete a coocorr√™ncia.

2. **Autovetores e Espa√ßo de Embeddings**:
   - Prove que, para uma matriz termo-termo sim√©trica normalizada, os autovetores da matriz fornecem uma base ortogonal para o espa√ßo de embeddings.
   - *Dica*: Utilize a decomposi√ß√£o espectral e relacione com t√©cnicas de redu√ß√£o de dimensionalidade como **SVD** (Singular Value Decomposition).

3. **Medidas de Associa√ß√£o Robusta**:
   - Considerando a hip√≥tese distribucional, proponha e justifique matematicamente uma medida de associa√ß√£o entre palavras que seja mais robusta a varia√ß√µes na frequ√™ncia das palavras do que a contagem de coocorr√™ncia simples.
   - *Dica*: Explore medidas como **PMI** (Pointwise Mutual Information) e suas variantes suavizadas.

## T√©cnicas de Pondera√ß√£o: tf-idf e PPMI

Para melhorar a qualidade das representa√ß√µes, s√£o aplicadas t√©cnicas de pondera√ß√£o que ajustam os valores das matrizes para refletir melhor a import√¢ncia das palavras.

### tf-idf (Term Frequency-Inverse Document Frequency)

O **tf-idf** √© uma t√©cnica de pondera√ß√£o utilizada principalmente em matrizes termo-documento. Ela combina duas m√©tricas:

1. **Term Frequency (tf)**: Frequ√™ncia da palavra no documento, refletindo sua import√¢ncia local.
2. **Inverse Document Frequency (idf)**: Logaritmo inverso da frequ√™ncia de documentos que cont√™m a palavra, refletindo sua raridade global.

Matematicamente, o tf-idf √© definido como [14]:

$$
\text{tf-idf}_{t,d} = \text{tf}_{t,d} \times \text{idf}_t
$$

Onde:

$$
\text{tf}_{t,d} = \begin{cases}
1 + \log_{10}(\text{count}(t,d)), & \text{se } \text{count}(t,d) > 0 \\
0, & \text{caso contr√°rio}
\end{cases}
$$

$$
\text{idf}_t = \log_{10}\left(\frac{N}{\text{df}_t}\right)
$$

- $\text{count}(t,d)$ √© o n√∫mero de ocorr√™ncias do termo $t$ no documento $d$.
- $N$ √© o n√∫mero total de documentos.
- $\text{df}_t$ √© o n√∫mero de documentos que cont√™m o termo $t$.

> ‚ö†Ô∏è **Nota Importante**: O tf-idf d√° maior peso a palavras que s√£o frequentes em um documento espec√≠fico, mas raras no corpus como um todo [15].

### PPMI (Positive Pointwise Mutual Information)

**PPMI** √© uma medida de associa√ß√£o entre palavras usada principalmente em matrizes termo-termo. √â definida como [16]:

$$
\text{PPMI}(w,c) = \max\left(0, \log_2\left(\frac{P(w,c)}{P(w)P(c)}\right)\right)
$$

Onde:

- $P(w,c)$ √© a probabilidade de coocorr√™ncia das palavras $w$ e $c$.
- $P(w)$ e $P(c)$ s√£o as probabilidades marginais de $w$ e $c$, respectivamente.

A PPMI ajusta a **Pointwise Mutual Information (PMI)** para zero nos casos em que a PMI √© negativa.

> üí° **Insight**: PPMI captura associa√ß√µes positivas entre palavras, ignorando associa√ß√µes negativas que s√£o geralmente menos confi√°veis em corpora pequenos [17].

### Compara√ß√£o entre tf-idf e PPMI

| Caracter√≠stica                 | tf-idf                                | PPMI                                          |
| ------------------------------ | ------------------------------------- | --------------------------------------------- |
| Tipo de Matriz                 | Termo-Documento                       | Termo-Termo                                   |
| Captura                        | Import√¢ncia de palavras em documentos | Associa√ß√µes entre palavras                    |
| Valores                        | Sempre n√£o negativos                  | N√£o negativos (valores negativos s√£o zerados) |
| Sensibilidade a palavras raras | Alta (atrav√©s do componente idf)      | Alta (tende a superestimar associa√ß√µes raras) |

## Word2Vec: Um Modelo Preditivo de Embedding

**Word2Vec**, introduzido por Mikolov et al. (2013), √© um modelo neural que aprende embeddings de palavras atrav√©s de uma tarefa de previs√£o [18]. Existem duas variantes principais:

1. **Continuous Bag-of-Words (CBOW)**: Prev√™ a palavra alvo dado o contexto.
2. **Skip-gram**: Prev√™ o contexto dado uma palavra alvo.

Focaremos no modelo **Skip-gram**, que √© mais eficaz para palavras raras e captura melhor as rela√ß√µes sem√¢nticas.

### Arquitetura Skip-gram

O modelo Skip-gram treina uma rede neural rasa para maximizar a probabilidade de palavras de contexto dado uma palavra alvo. A fun√ß√£o objetivo √©:

$$
\arg\max_\theta \sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j}|w_t; \theta)
$$

Onde:

- $w_t$ √© a palavra alvo no tempo $t$.
- $c$ √© o tamanho da janela de contexto.
- $\theta$ s√£o os par√¢metros do modelo (os embeddings).

A probabilidade $P(w_{t+j}|w_t)$ √© modelada usando softmax:

$$
P(w_O|w_I) = \frac{\exp(v_{w_O}^\top v_{w_I})}{\sum_{w \in V} \exp(v_w^\top v_{w_I})}
$$

Onde:

- $v_{w_I}$ √© o vetor de embedding da palavra de entrada (input word).
- $v_{w_O}$ √© o vetor de embedding da palavra de sa√≠da (output word).
- $V$ √© o tamanho do vocabul√°rio [19].

O c√°lculo direto dessa express√£o √© computacionalmente custoso devido √† soma sobre todo o vocabul√°rio.

### Negative Sampling

Para tornar o treinamento computacionalmente vi√°vel, o Word2Vec utiliza o **Negative Sampling**, que simplifica a fun√ß√£o objetivo ao considerar apenas um pequeno n√∫mero de palavras negativas em cada atualiza√ß√£o.

A fun√ß√£o de perda simplificada √©:

$$
\log \sigma(v_{w_O}^\top v_{w_I}) + \sum_{i=1}^k \mathbb{E}_{w_i \sim P_n(w)}\left[ \log \sigma(-v_{w_i}^\top v_{w_I}) \right]
$$

Onde:

- $\sigma(x) = \frac{1}{1 + \exp(-x)}$ √© a fun√ß√£o sigmoide.
- $k$ √© o n√∫mero de amostras negativas.
- $P_n(w)$ √© a distribui√ß√£o de ru√≠do utilizada para amostrar palavras negativas [20].

> ‚úîÔ∏è **Destaque**: Negative Sampling permite o treinamento eficiente de Word2Vec em corpora de grande escala, mantendo a qualidade dos embeddings [21].

### Rela√ß√£o com Informa√ß√£o M√∫tua

O Word2Vec, especialmente com Negative Sampling, pode ser interpretado como uma forma de fatora√ß√£o impl√≠cita de uma matriz de coocorr√™ncia ponderada, relacionando-se com medidas como a PMI. Essa interpreta√ß√£o ajuda a conectar modelos preditivos com modelos baseados em contagem.

### Perguntas Te√≥ricas

1. **Deriva√ß√£o da Fun√ß√£o de Perda com Negative Sampling**:
   - Derive a fun√ß√£o de perda para o modelo Skip-gram com Negative Sampling e mostre como ela se relaciona com a maximiza√ß√£o da informa√ß√£o m√∫tua entre palavras e seus contextos.
   - *Dica*: Analise a fun√ß√£o de perda como uma aproxima√ß√£o da maximiza√ß√£o da probabilidade de dados observados.

2. **Efeito do Tamanho da Janela de Contexto**:
   - Analise teoricamente como o tamanho da janela de contexto no Skip-gram afeta as propriedades sem√¢nticas dos embeddings resultantes.
   - *Dica*: Janelas maiores capturam rela√ß√µes sem√¢nticas mais globais, enquanto janelas menores capturam rela√ß√µes sint√°ticas locais.

3. **Melhoria para Palavras Raras**:
   - Proponha e justifique matematicamente uma modifica√ß√£o no algoritmo de Negative Sampling que poderia melhorar a qualidade dos embeddings para palavras raras sem aumentar significativamente o custo computacional.
   - *Dica*: Considere ajustar a distribui√ß√£o de ru√≠do $P_n(w)$ para dar mais √™nfase a palavras raras.

## Propriedades e Aplica√ß√µes de Word Embeddings

### Captura de Rela√ß√µes Sem√¢nticas

Os word embeddings s√£o capazes de capturar v√°rias rela√ß√µes sem√¢nticas e sint√°ticas entre palavras, que podem ser reveladas atrav√©s de opera√ß√µes vetoriais [22]:

1. **Similaridade**: Medida pelo cosseno entre vetores.

   $$
   \text{similaridade}(w_1, w_2) = \frac{v_{w_1} \cdot v_{w_2}}{\|v_{w_1}\| \|v_{w_2}\|}
   $$

   Onde $\|v_{w}\|$ √© a norma do vetor $v_{w}$.

2. **Analogias Sem√¢nticas**: Capturadas por opera√ß√µes vetoriais aritm√©ticas.

   $$
   v_{\text{rei}} - v_{\text{homem}} + v_{\text{mulher}} \approx v_{\text{rainha}}
   $$

   Essa propriedade permite resolver tarefas de analogias sem√¢nticas e sint√°ticas.

3. **Rela√ß√µes Hier√°rquicas e Clusters**: Palavras com significados relacionados tendem a formar clusters no espa√ßo vetorial.

> üí° **Insight**: Essas propriedades emergem do treinamento n√£o supervisionado dos embeddings, demonstrando a capacidade dos modelos em capturar estruturas lingu√≠sticas complexas [23].

### Aplica√ß√µes

Os word embeddings s√£o aplicados em uma variedade de tarefas em NLP:

1. **An√°lise de Sentimento**: Embeddings capturam nuances emocionais, melhorando classificadores.
2. **Tradu√ß√£o Autom√°tica**: Embeddings multilingues permitem o alinhamento sem√¢ntico entre l√≠nguas.
3. **Sistemas de Recomenda√ß√£o**: Representa√ß√£o de itens e usu√°rios em um espa√ßo comum facilita a recomenda√ß√£o personalizada.
4. **An√°lise Temporal de Linguagem**: Rastreamento de mudan√ßas sem√¢nticas ao longo do tempo usando embeddings hist√≥ricos [24].
5. **Reconhecimento de Entidades Nomeadas**: Melhora a identifica√ß√£o de entidades como nomes pr√≥prios, lugares, etc.

## Desafios e Limita√ß√µes

Apesar dos avan√ßos, os word embeddings enfrentam desafios importantes:

1. **Palavras Poliss√™micas**: Embeddings est√°ticos n√£o distinguem entre diferentes sentidos de uma mesma palavra (e.g., "banco" como institui√ß√£o financeira ou assento).
2. **Vi√©s**: Embeddings podem perpetuar e amplificar vieses presentes nos dados de treinamento, como estere√≥tipos de g√™nero ou ra√ßa [25].
3. **Depend√™ncia de Contexto**: N√£o capturam varia√ß√µes contextuais no significado das palavras.

### Abordagens para Supera√ß√£o

- **Embeddings Contextuais**: Modelos como **ELMo**, **BERT** e **GPT** geram embeddings que dependem do contexto espec√≠fico em que a palavra aparece.
- **T√©cnicas de Debiasing**: M√©todos para identificar e remover componentes de vi√©s nos embeddings.
- **Representa√ß√µes Multi-sentido**: Modelos que aprendem diferentes vetores para diferentes sentidos de uma palavra.

## Conclus√£o

Os word embeddings representam um avan√ßo fundamental na representa√ß√£o computacional do significado das palavras. Ao mapear palavras para espa√ßos vetoriais densos, eles capturam rela√ß√µes sem√¢nticas complexas e permitem uma variedade de aplica√ß√µes em NLP e √°reas correlatas. Modelos como **Word2Vec** e **GloVe** estabeleceram o padr√£o para embeddings est√°ticos, mas a √°rea continua a evoluir com o desenvolvimento de representa√ß√µes cada vez mais sofisticadas e contextuais [26].

Uma compreens√£o profunda dos fundamentos matem√°ticos e estat√≠sticos por tr√°s dos word embeddings √© crucial para avan√ßar o estado da arte em NLP. Al√©m disso, √© importante abordar os desafios e limita√ß√µes atuais para desenvolver aplica√ß√µes que fazem uso efetivo dessas representa√ß√µes, de forma √©tica e respons√°vel.

## Perguntas Te√≥ricas Avan√ßadas

1. **Word2Vec e Informa√ß√£o M√∫tua**:
   - Considerando a teoria da informa√ß√£o, demonstre como a otimiza√ß√£o da fun√ß√£o objetivo do Word2Vec se relaciona com a maximiza√ß√£o da informa√ß√£o m√∫tua entre palavras e seus contextos.
   - *Dica*: Relacione a fun√ß√£o de perda do Negative Sampling com a maximiza√ß√£o da PMI entre palavras e contextos.

2. **Compara√ß√£o de Arquiteturas de Embedding**:
   - Proponha um framework te√≥rico para quantificar e comparar a capacidade de diferentes arquiteturas de embedding (e.g., Skip-gram, GloVe, FastText) em capturar diferentes tipos de rela√ß√µes sem√¢nticas (sinon√≠mia, anton√≠mia, hiperon√≠mia).
   - *Dica*: Considere m√©tricas baseadas em tarefas espec√≠ficas e medidas intr√≠nsecas de qualidade.

3. **Dimensionalidade e Trade-off**:
   - Analise matematicamente como a dimensionalidade do espa√ßo de embedding afeta o trade-off entre a capacidade de representa√ß√£o e a generaliza√ß√£o.
   - *Dica*: Explore conceitos de teoria da informa√ß√£o e complexidade do modelo.

4. **Embeddings Din√¢micos e Mudan√ßas Sem√¢nticas**:
   - Desenvolva um modelo te√≥rico para embeddings din√¢micos que possam capturar mudan√ßas sem√¢nticas ao longo do tempo ou em diferentes dom√≠nios.
   - *Dica*: Considere incorporar a dimens√£o temporal nos embeddings ou utilizar t√©cnicas de modelagem de t√≥picos din√¢micos.

## Refer√™ncias

[1] Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation of Word Representations in Vector Space*. arXiv preprint arXiv:1301.3781.

[2] Harris, Z. S. (1954). *Distributional Structure*. Word, 10(2-3), 146-162.

[3] Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). *A Neural Probabilistic Language Model*. Journal of Machine Learning Research, 3(Feb), 1137-1155.

[4] Turian, J., Ratinov, L., & Bengio, Y. (2010). *Word Representations: A Simple and General Method for Semi-Supervised Learning*. In Proceedings of the 48th annual meeting of the association for computational linguistics.

[5] Pennington, J., Socher, R., & Manning, C. D. (2014). *GloVe: Global Vectors for Word Representation*. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP).

[6] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT press.

[7] Firth, J. R. (1957). *A synopsis of linguistic theory 1930‚Äì1955*. Studies in linguistic analysis.

[8] Firth, J. R. (1957). *Papers in Linguistics 1934-1951*. Oxford University Press.

[9] Church, K. W., & Hanks, P. (1990). *Word Association Norms, Mutual Information, and Lexicography*. Computational Linguistics, 16(1), 22-29.

[10] Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). *Enriching Word Vectors with Subword Information*. Transactions of the Association for Computational Linguistics, 5, 135-146.

[11] Salton, G., & Buckley, C. (1988). *Term-weighting Approaches in Automatic Text Retrieval*. Information Processing & Management, 24(5), 513-523.

[12] Levy, O., & Goldberg, Y. (2014). *Neural Word Embedding as Implicit Matrix Factorization*. In Advances in neural information processing systems.

[13] Turney, P. D., & Pantel, P. (2010). *From Frequency to Meaning: Vector Space Models of Semantics*. Journal of artificial intelligence research, 37, 141-188.

[14] Ramos, J. (2003). *Using tf-idf to Determine Word Relevance in Document Queries*. In Proceedings of the first instructional conference on machine learning.

[15] Manning, C. D., Raghavan, P., & Sch√ºtze, H. (2008). *Introduction to Information Retrieval*. Cambridge University Press.

[16] Bullinaria, J. A., & Levy, J. P. (2007). *Extracting Semantic Representations from Word Co-occurrence Statistics: A Computational Study*. Behavior Research Methods, 39(3), 510-526.

[17] Levy, O., & Goldberg, Y. (2014). *Linguistic Regularities in Sparse and Explicit Word Representations*. In Proceedings of the eighteenth conference on computational natural language learning.

[18] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). *Distributed Representations of Words and Phrases and Their Compositionality*. In Advances in neural information processing systems.

[19] Goldberg, Y., & Levy, O. (2014). *Word2Vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method*. arXiv preprint arXiv:1402.3722.

[20] Mnih, A., & Kavukcuoglu, K. (2013). *Learning Word Embeddings Efficiently with Noise-contrastive Estimation*. In Advances in neural information processing systems.

[21] Rong, X. (2014). *Word2Vec Parameter Learning Explained*. arXiv preprint arXiv:1411.2738.

[22] Mikolov, T., Yih, W. T., & Zweig, G. (2013). *Linguistic Regularities in Continuous Space Word Representations*. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies.

[23] Baroni, M., Dinu, G., & Kruszewski, G. (2014). *Don't Count, Predict! A Systematic Comparison of Context-counting vs. Context-predicting Semantic Vectors*. In Proceedings of the 52nd annual meeting of the association for computational linguistics.

[24] Hamilton, W. L., Leskovec, J., & Jurafsky, D. (2016). *Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change*. arXiv preprint arXiv:1605.09096.

[25] Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. (2016). *Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings*. In Advances in neural information processing systems.

[26] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). *Deep Contextualized Word Representations*. In Proceedings of the 2018 conference of the north american chapter of the association for computational linguistics.