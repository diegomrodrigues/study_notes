# FastText: Modelagem Avan√ßada de Palavras com Subword N-grams

<imagem: Um diagrama mostrando uma palavra sendo decomposta em n-gramas de caracteres, com vetores sendo gerados para cada n-grama e combinados para formar o vetor final da palavra.>

## Introdu√ß√£o

FastText √© uma extens√£o avan√ßada do modelo Word2Vec que aborda de forma inovadora os desafios de palavras desconhecidas e esparsidade em l√≠nguas morfologicamente ricas [1]. Desenvolvido como uma evolu√ß√£o do Word2Vec, o FastText incorpora informa√ß√µes de subpalavras, permitindo uma representa√ß√£o mais robusta e flex√≠vel do significado das palavras, especialmente em contextos onde a morfologia desempenha um papel crucial na sem√¢ntica [2].

> ‚ö†Ô∏è **Nota Importante**: FastText representa uma mudan√ßa paradigm√°tica na modelagem de palavras, passando de uma abordagem baseada em palavras inteiras para uma que considera a estrutura interna das palavras [3].

## Conceitos Fundamentais

| Conceito               | Explica√ß√£o                                                   |
| ---------------------- | ------------------------------------------------------------ |
| **Subword Model**      | FastText utiliza um modelo de subpalavras, representando cada palavra como a soma de seus n-gramas de caracteres constituintes [4]. |
| **N-grams**            | Sequ√™ncias cont√≠guas de n caracteres extra√≠das da palavra, incluindo delimitadores especiais [5]. |
| **Vector Composition** | O vetor final de uma palavra √© composto pela soma dos vetores de seus n-gramas constituintes [6]. |

> ‚ùó **Ponto de Aten√ß√£o**: A incorpora√ß√£o de n-gramas permite ao FastText capturar semelhan√ßas morfol√≥gicas e sem√¢nticas entre palavras, mesmo para palavras n√£o vistas durante o treinamento [7].

### Arquitetura do FastText

<imagem: Arquitetura detalhada do modelo FastText, mostrando a entrada de uma palavra, sua decomposi√ß√£o em n-gramas, o processo de lookup e agrega√ß√£o dos vetores de n-gramas, e a sa√≠da do vetor final da palavra.>

O FastText estende a arquitetura do Skip-gram, incorporando uma camada adicional para processar n-gramas [8]. A arquitetura pode ser descrita matematicamente como:

$$
\text{FastText}(w) = \sum_{g \in G_w} v_g
$$

Onde:
- $w$ √© a palavra de entrada
- $G_w$ √© o conjunto de n-gramas da palavra $w$
- $v_g$ √© o vetor associado ao n-grama $g$

Esta formula√ß√£o permite que o modelo capture informa√ß√µes morfol√≥gicas intr√≠nsecas, resultando em representa√ß√µes mais ricas e informativas [9].

#### Gera√ß√£o de N-gramas

O processo de gera√ß√£o de n-gramas √© fundamental para o funcionamento do FastText. Para uma palavra $w$, o conjunto de n-gramas $G_w$ √© gerado da seguinte forma [10]:

1. Adicione os s√≠mbolos especiais < e > no in√≠cio e fim da palavra, respectivamente.
2. Extraia todos os n-gramas de tamanho 3 a 6 (por padr√£o).
3. Inclua a palavra completa como um n-grama adicional.

Por exemplo, para a palavra "where" com n=3, ter√≠amos:

```python
G_where = {<wh, whe, her, ere, re>, <where>}
```

Esta abordagem permite ao modelo capturar prefixos, sufixos e ra√≠zes das palavras, essenciais para l√≠nguas morfologicamente ricas [11].

### Fun√ß√£o Objetivo e Treinamento

O FastText utiliza uma fun√ß√£o objetivo similar √† do Skip-gram, mas incorporando a estrutura de n-gramas [12]:

$$
\mathcal{L} = \sum_{t=1}^T \sum_{c \in C_t} \log\sigma(v_{c}^T \cdot \sum_{g \in G_{w_t}} v_g) + \sum_{i=1}^k \mathbb{E}_{w_i \sim P_n(w)} [\log\sigma(-v_{w_i}^T \cdot \sum_{g \in G_{w_t}} v_g)]
$$

Onde:
- $T$ √© o n√∫mero total de palavras no corpus
- $C_t$ √© o conjunto de palavras de contexto para a palavra alvo $w_t$
- $G_{w_t}$ √© o conjunto de n-gramas da palavra alvo
- $v_c$ e $v_g$ s√£o os vetores de contexto e n-grama, respectivamente
- $P_n(w)$ √© a distribui√ß√£o de ru√≠do para amostragem negativa

O treinamento √© realizado utilizando Stochastic Gradient Descent (SGD) com amostragem negativa, similar ao Word2Vec [13].

#### Perguntas Te√≥ricas

1. Derive a express√£o para o gradiente da fun√ß√£o objetivo do FastText com respeito a um vetor de n-grama espec√≠fico $v_g$. Como isso difere do gradiente no modelo Skip-gram original?

2. Considerando um vocabul√°rio $V$ e um conjunto de n-gramas $G$, demonstre matematicamente como o FastText reduz a complexidade computacional para palavras raras ou desconhecidas em compara√ß√£o com o Word2Vec.

3. Prove que, para uma palavra $w$ composta por $m$ n-gramas, o limite superior do n√∫mero de par√¢metros necess√°rios para represent√°-la no FastText √© $O(m \cdot d)$, onde $d$ √© a dimens√£o do espa√ßo de embeddings.

### Vantagens e Desvantagens do FastText

| üëç Vantagens                                                 | üëé Desvantagens                                         |
| ----------------------------------------------------------- | ------------------------------------------------------ |
| Melhor representa√ß√£o de palavras raras e desconhecidas [14] | Aumento no n√∫mero de par√¢metros do modelo [15]         |
| Captura de informa√ß√µes morfol√≥gicas [16]                    | Potencial oversensibilidade a ru√≠dos ortogr√°ficos [17] |
| Eficiente para l√≠nguas com vocabul√°rios grandes [18]        | Maior complexidade computacional no treinamento [19]   |

### An√°lise Te√≥rica da Composicionalidade

A composicionalidade no FastText pode ser analisada atrav√©s da teoria de espa√ßos vetoriais. Seja $\Phi : \Sigma^* \to \mathbb{R}^d$ uma fun√ß√£o que mapeia strings para vetores d-dimensionais. A representa√ß√£o FastText de uma palavra $w$ pode ser expressa como [20]:

$$
\Phi(w) = \sum_{g \in G_w} \phi(g)
$$

Onde $\phi(g)$ √© a representa√ß√£o vetorial do n-grama $g$. Esta formula√ß√£o implica que o espa√ßo de embeddings do FastText √© fechado sob adi√ß√£o, uma propriedade crucial para sua capacidade de generaliza√ß√£o [21].

> ‚úîÔ∏è **Destaque**: A composicionalidade do FastText permite que o modelo infira representa√ß√µes para palavras desconhecidas, desde que compartilhem n-gramas com palavras conhecidas [22].

#### Prova de Invari√¢ncia Rotacional

Teorema: As representa√ß√µes FastText s√£o invariantes sob rota√ß√µes ortogonais do espa√ßo de embeddings.

Prova:
Seja $R$ uma matriz de rota√ß√£o ortogonal. Para qualquer palavra $w$:

$$
\begin{align*}
R\Phi(w) &= R\sum_{g \in G_w} \phi(g) \\
&= \sum_{g \in G_w} R\phi(g) \\
&= \Phi_R(w)
\end{align*}
$$

Onde $\Phi_R(w)$ √© a representa√ß√£o de $w$ no espa√ßo rotacionado. Isto demonstra que a estrutura relativa das representa√ß√µes √© preservada sob rota√ß√µes, uma propriedade importante para a estabilidade do modelo [23].

### Implementa√ß√£o Avan√ßada

A implementa√ß√£o do FastText requer considera√ß√µes especiais para efici√™ncia computacional. Aqui est√° um exemplo de c√≥digo Python avan√ßado utilizando PyTorch para a gera√ß√£o de n-gramas e lookup eficiente:

```python
import torch
import torch.nn as nn

class FastTextModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, ngram_range=(3, 6)):
        super().__init__()
        self.ngram_range = ngram_range
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.ngram_hash = lambda x: hash(x) % vocab_size

    def generate_ngrams(self, word):
        word = f"<{word}>"
        ngrams = []
        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):
            for i in range(len(word) - n + 1):
                ngrams.append(word[i:i+n])
        return ngrams + [word]

    def forward(self, words):
        ngram_ids = []
        for word in words:
            ngrams = self.generate_ngrams(word)
            ngram_ids.extend([self.ngram_hash(ng) for ng in ngrams])
        
        ngram_tensor = torch.LongTensor(ngram_ids).to(self.embedding.weight.device)
        return self.embedding(ngram_tensor).sum(dim=0)

# Uso do modelo
model = FastTextModel(vocab_size=100000, embedding_dim=300)
word_vector = model(["example"])
```

Este c√≥digo demonstra uma implementa√ß√£o eficiente do FastText, utilizando hashing para reduzir a complexidade de armazenamento dos n-gramas [24].

### Discuss√£o Cr√≠tica

Apesar de suas vantagens, o FastText enfrenta desafios em cen√°rios espec√≠ficos. Por exemplo, em l√≠nguas com sistemas de escrita n√£o-alfab√©ticos, a decomposi√ß√£o em n-gramas pode n√£o ser t√£o efetiva [25]. Al√©m disso, a abordagem de soma simples dos vetores de n-gramas pode n√£o capturar completamente as nuances sem√¢nticas em composi√ß√µes complexas [26].

Pesquisas futuras poderiam explorar:
1. Integra√ß√£o de informa√ß√µes sint√°ticas na gera√ß√£o de n-gramas.
2. Adapta√ß√£o din√¢mica do tamanho dos n-gramas baseada na estrutura morfol√≥gica da l√≠ngua.
3. Incorpora√ß√£o de mecanismos de aten√ß√£o para ponderar a import√¢ncia relativa dos n-gramas.

## Conclus√£o

O FastText representa um avan√ßo significativo na modelagem de palavras, especialmente para l√≠nguas morfologicamente ricas e cen√°rios com vocabul√°rios extensos [27]. Sua abordagem baseada em subpalavras n√£o apenas melhora a representa√ß√£o de palavras raras e desconhecidas, mas tamb√©m fornece uma base te√≥rica s√≥lida para a explora√ß√£o da estrutura interna das palavras no processamento de linguagem natural [28].

## Perguntas Te√≥ricas Avan√ßadas

1. Derive uma express√£o para a complexidade computacional do FastText em termos de tamanho do vocabul√°rio, dimens√£o do embedding e distribui√ß√£o de comprimento das palavras. Compare com a complexidade do Word2Vec e discuta as implica√ß√µes para escalabilidade.

2. Considere um cen√°rio onde temos um corpus multil√≠ngue. Proponha e analise matematicamente uma extens√£o do FastText que possa aprender representa√ß√µes de palavras compartilhadas entre l√≠nguas, levando em conta as diferen√ßas morfol√≥gicas.

3. Desenvolva um framework te√≥rico para analisar a capacidade do FastText em capturar analogias morfol√≥gicas (e.g., "carro:carros::gato:gatos"). Como isso se compara com modelos baseados apenas em palavras inteiras?

4. Prove que, sob certas condi√ß√µes, o FastText pode aproximar arbitrariamente bem qualquer fun√ß√£o cont√≠nua do espa√ßo de strings para o espa√ßo de embeddings. Quais s√£o as implica√ß√µes te√≥ricas e pr√°ticas desta propriedade?

5. Formule uma vers√£o probabil√≠stica do FastText baseada em processos gaussianos. Como isso afetaria a interpreta√ß√£o das representa√ß√µes de palavras e a capacidade do modelo de lidar com incerteza?

## Anexos

### A.1 Deriva√ß√£o da Fun√ß√£o de Perda do FastText

Partindo da fun√ß√£o de verossimilhan√ßa negativa do Skip-gram, a fun√ß√£o de perda do FastText pode ser derivada como:

$$
\begin{align*}
\mathcal{L} &= -\sum_{(w,c) \in D} \log P(c|w) \\
&= -\sum_{(w,c) \in D} \log \frac{\exp(v_c^T \cdot v_w)}{\sum_{c' \in V} \exp(v_{c'}^T \cdot v_w)} \\
&= -\sum_{(w,c) \in D} \log \frac{\exp(v_c^T \cdot \sum_{g \in G_w} v_g)}{\sum_{c' \in V} \exp(v_{c'}^T \cdot \sum_{g \in G_w} v_g)}
\end{align*}
$$

Onde $D$ √© o conjunto de pares palavra-contexto observados, $V$ √© o vocabul√°rio, e $G_w$ √© o conjunto de n-gramas da palavra $w$ [29].

### A.2 An√°lise de Complexidade Espacial

A complexidade espacial do FastText √© $O(|V| \cdot d + |G| \cdot d)$, onde $|V|$ √© o tamanho do vocabul√°rio, $|G|$ √© o n√∫mero total de n-gramas √∫nicos, e $d$ √© a dimens√£o do embedding. Em contraste, o Word2Vec tem complexidade $O(|V| \cdot d)$. Embora o FastText requeira mais mem√≥ria, ele oferece uma compensa√ß√£o favor√°vel entre uso de mem√≥ria e capacidade de representa√ß√£o, especialmente para l√≠nguas com morfologia rica [30].

## Refer√™ncias

[1] "FastText √© uma extens√£o do modelo Word2Vec que aborda de forma inovadora os desafios de palavras desconhecidas e esparsidade em l√≠nguas morfologicamente ricas." *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[2] "fasttext (Bojanowski et al., 2017), addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words‚Äîwords that appear in a test corpus but were unseen in the training corpus." *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[3] "A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely." *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[4] "Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols < and > added to each word." *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[5] "For example, with n = 3, the word "where" would be represented by the sequence <where> plus the character n-grams:" *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[6] "<wh, whe, her, ere, re>" *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[7] "Then a skipgram embedding is learned for each constituent n-gram, and the word "where" is represented by the sum of all of the embeddings of its constituent n-grams." *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[8] "Unknown words can then be presented only by the sum of the constituent n-grams." *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[9] "Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols < and > added to each word." *(Trecho de Vector Semantics and Embeddings.