# Skip-gram with Negative Sampling (SGNS): Fundamentos e Aplica√ß√µes Avan√ßadas em Word Embeddings

<imagem: Uma rede neural representando o modelo Skip-gram, com uma camada de entrada para a palavra-alvo, uma camada oculta, e m√∫ltiplas sa√≠das para palavras de contexto e ru√≠do, ilustrando o processo de amostragem negativa.>

## Introdu√ß√£o

O Skip-gram with Negative Sampling (SGNS) √© um algoritmo fundamental no campo do processamento de linguagem natural (NLP) e representa uma evolu√ß√£o significativa na cria√ß√£o de word embeddings. Este m√©todo, parte central do modelo Word2Vec, revolucionou a forma como representamos palavras em espa√ßos vetoriais densos, permitindo capturar rela√ß√µes sem√¢nticas complexas de maneira eficiente [1]. 

O SGNS baseia-se na ideia de que palavras que ocorrem em contextos similares tendem a ter significados semelhantes, um princ√≠pio conhecido como hip√≥tese distribucional. Esta abordagem n√£o apenas superou m√©todos anteriores em termos de efici√™ncia computacional, mas tamb√©m demonstrou uma not√°vel capacidade de capturar nuances sem√¢nticas e sint√°ticas da linguagem [2].

## Conceitos Fundamentais

| Conceito              | Explica√ß√£o                                                   |
| --------------------- | ------------------------------------------------------------ |
| **Word Embedding**    | Representa√ß√£o vetorial densa de palavras em um espa√ßo multidimensional. No contexto do SGNS, estas s√£o aprendidas atrav√©s de um processo de classifica√ß√£o bin√°ria [3]. |
| **Skip-gram**         | Modelo que prev√™ palavras de contexto dada uma palavra-alvo, invertendo a abordagem tradicional dos modelos de linguagem [4]. |
| **Negative Sampling** | T√©cnica de otimiza√ß√£o que seleciona aleatoriamente "palavras de ru√≠do" para contrastar com as palavras de contexto reais, melhorando a efici√™ncia do treinamento [5]. |

> ‚ö†Ô∏è **Nota Importante**: O SGNS difere de modelos anteriores por n√£o requerer a normaliza√ß√£o sobre todo o vocabul√°rio, tornando o treinamento significativamente mais eficiente para vocabul√°rios grandes [6].

> ‚ùó **Ponto de Aten√ß√£o**: A qualidade dos embeddings gerados pelo SGNS √© altamente dependente da escolha dos hiperpar√¢metros, especialmente o tamanho da janela de contexto e o n√∫mero de amostras negativas [7].

> ‚úîÔ∏è **Destaque**: O SGNS demonstrou capacidade de capturar rela√ß√µes anal√≥gicas complexas entre palavras, como "rei - homem + mulher ‚âà rainha", um marco na representa√ß√£o sem√¢ntica vetorial [8].

### Fundamentos Te√≥ricos do SGNS

<imagem: Diagrama ilustrando a arquitetura do modelo Skip-gram, destacando a entrada (palavra-alvo), a camada oculta (embedding), e as m√∫ltiplas sa√≠das (palavras de contexto e ru√≠do), com setas indicando o fluxo de informa√ß√£o e o processo de otimiza√ß√£o.>

O SGNS fundamenta-se na maximiza√ß√£o da probabilidade de palavras de contexto verdadeiras enquanto minimiza a probabilidade de palavras de ru√≠do. Matematicamente, para uma palavra-alvo $w$ e uma palavra de contexto $c$, o modelo define:

$$
P(+|w,c) = \sigma(c \cdot w) = \frac{1}{1 + e^{-c \cdot w}}
$$

Onde $\sigma$ √© a fun√ß√£o sigmoide, $c$ e $w$ s√£o vetores de embedding para as palavras de contexto e alvo, respectivamente [9].

A fun√ß√£o objetivo do SGNS para um par $(w,c)$ e $k$ palavras de ru√≠do $c_{neg}$ √©:

$$
L = -\log\sigma(c \cdot w) - \sum_{i=1}^{k} \log\sigma(-c_{neg_i} \cdot w)
$$

Esta fun√ß√£o √© otimizada usando descida de gradiente estoc√°stica (SGD) [10].

#### Processo de Treinamento

1. Para cada palavra-alvo $w$, selecione palavras de contexto $c$ dentro de uma janela.
2. Para cada par $(w,c)$, gere $k$ palavras de ru√≠do $c_{neg}$.
3. Atualize os embeddings para maximizar $\sigma(c \cdot w)$ e minimizar $\sigma(c_{neg} \cdot w)$.

A atualiza√ß√£o dos vetores segue as equa√ß√µes:

$$
c_{pos}^{t+1} = c_{pos}^t - \eta[\sigma(c_{pos}^t \cdot w^t) - 1]w^t
$$

$$
c_{neg}^{t+1} = c_{neg}^t - \eta[\sigma(c_{neg}^t \cdot w^t)]w^t
$$

$$
w^{t+1} = w^t - \eta([\sigma(c_{pos}^t \cdot w^t) - 1]c_{pos}^t + \sum_{i=1}^k [\sigma(c_{neg_i}^t \cdot w^t)]c_{neg_i}^t)
$$

Onde $\eta$ √© a taxa de aprendizado [11].

#### Perguntas Te√≥ricas

1. Derive a express√£o para o gradiente da fun√ß√£o de perda $L$ em rela√ß√£o ao vetor de embedding $w$ para uma √∫nica palavra de contexto e uma palavra de ru√≠do.

2. Demonstre matematicamente por que o SGNS √© computacionalmente mais eficiente que modelos que requerem normaliza√ß√£o sobre todo o vocabul√°rio.

3. Analise teoricamente como o tamanho da janela de contexto afeta a captura de rela√ß√µes sem√¢nticas vs. sint√°ticas nos embeddings resultantes.

### Amostragem Negativa: Teoria e Implementa√ß√£o

A amostragem negativa √© crucial para a efici√™ncia do SGNS. Palavras de ru√≠do s√£o amostradas de acordo com uma distribui√ß√£o de probabilidade modificada:

$$
P_\alpha(w) = \frac{\sum_{w'} \text{count}(w')^\alpha \text{count}(w)^\alpha}{\sum_{w'} \text{count}(w')^\alpha}
$$

Onde $\alpha = 0.75$ √© comumente usado para dar maior probabilidade a palavras raras [12].

> üí° **Insight**: A escolha de $\alpha = 0.75$ equilibra a frequ√™ncia das palavras, permitindo que palavras menos comuns tenham maior chance de serem selecionadas como amostras negativas, o que melhora a qualidade dos embeddings [13].

#### Implementa√ß√£o em Python

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

class SkipGramNegativeSampling(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)
        
    def forward(self, target_word, context_word, negative_samples):
        target_emb = self.target_embeddings(target_word)
        context_emb = self.context_embeddings(context_word)
        neg_embs = self.context_embeddings(negative_samples)
        
        pos_score = torch.sum(target_emb * context_emb, dim=1)
        neg_scores = torch.sum(target_emb.unsqueeze(1) * neg_embs, dim=2)
        
        pos_loss = -F.logsigmoid(pos_score)
        neg_loss = -torch.sum(F.logsigmoid(-neg_scores), dim=1)
        
        return pos_loss + neg_loss

# Implementa√ß√£o da amostragem negativa
def negative_sampling(word_freqs, alpha=0.75, k=5):
    adjusted_freqs = word_freqs ** alpha
    probs = adjusted_freqs / np.sum(adjusted_freqs)
    return np.random.choice(len(word_freqs), size=k, p=probs)
```

Este c√≥digo implementa o modelo SGNS usando PyTorch, incluindo a l√≥gica de amostragem negativa [14].

#### Perguntas Te√≥ricas

1. Derive a express√£o para a complexidade computacional do SGNS em fun√ß√£o do tamanho do vocabul√°rio e compare com a complexidade de um modelo softmax completo.

2. Analise matematicamente como diferentes valores de $\alpha$ na distribui√ß√£o de amostragem negativa afetam a converg√™ncia do modelo e a qualidade dos embeddings resultantes.

3. Proponha e justifique teoricamente uma modifica√ß√£o na fun√ß√£o de perda que poderia melhorar a captura de rela√ß√µes sem√¢nticas espec√≠ficas (por exemplo, hiperon√≠mia).

### An√°lise Avan√ßada de Propriedades dos Embeddings SGNS

Os embeddings gerados pelo SGNS exibem propriedades matem√°ticas interessantes que refletem rela√ß√µes sem√¢nticas e sint√°ticas entre palavras. Uma propriedade not√°vel √© a capacidade de resolver analogias atrav√©s de opera√ß√µes vetoriais [15].

Para uma analogia $a:b::c:d$, onde $d$ √© desconhecido, podemos aproximar:

$$
\vec{d} \approx \vec{c} - \vec{a} + \vec{b}
$$

Esta propriedade emerge da estrutura linear capturada pelo espa√ßo de embeddings [16].

> ‚ö†Ô∏è **Nota Importante**: Apesar de sua efic√°cia em muitos casos, o m√©todo do paralelogramo para analogias tem limita√ß√µes, especialmente para rela√ß√µes sem√¢nticas complexas ou palavras raras [17].

#### An√°lise de Componentes Principais (PCA) dos Embeddings

A aplica√ß√£o de PCA aos embeddings SGNS revela estruturas sem√¢nticas latentes:

1. Os primeiros componentes principais frequentemente capturam dimens√µes sem√¢nticas gerais (por exemplo, polaridade, formalidade).
2. Componentes subsequentes podem representar campos sem√¢nticos espec√≠ficos ou rela√ß√µes gramaticais [18].

#### Visualiza√ß√£o t-SNE

A t√©cnica t-SNE (t-Distributed Stochastic Neighbor Embedding) √© frequentemente usada para visualizar embeddings SGNS em duas ou tr√™s dimens√µes, revelando clusters sem√¢nticos [19].

<imagem: Gr√°fico de dispers√£o 2D mostrando clusters de palavras ap√≥s aplica√ß√£o de t-SNE em embeddings SGNS, com diferentes cores representando campos sem√¢nticos distintos.>

#### Perguntas Te√≥ricas

1. Derive matematicamente a rela√ß√£o entre a similaridade do cosseno de dois vetores de embedding e a probabilidade condicional usada no treinamento do SGNS.

2. Proponha e justifique um m√©todo te√≥rico para identificar e quantificar vieses (por exemplo, de g√™nero ou ra√ßa) nos embeddings SGNS usando an√°lise de componentes principais.

3. Desenvolva uma prova formal demonstrando por que o m√©todo do paralelogramo para analogias funciona no espa√ßo de embeddings SGNS, e identifique condi√ß√µes sob as quais ele pode falhar.

### Extens√µes e Variantes do SGNS

O sucesso do SGNS inspirou v√°rias extens√µes e variantes:

1. **FastText**: Incorpora informa√ß√µes de subpalavras, melhorando representa√ß√µes para l√≠nguas morfologicamente ricas e lidando com palavras fora do vocabul√°rio [20].

2. **GloVe**: Combina as vantagens de factoriza√ß√£o de matrizes globais com a aprendizagem de contexto local do SGNS [21].

3. **CBOW (Continuous Bag of Words)**: Uma variante do Word2Vec que prediz a palavra-alvo dado o contexto, em vez do contr√°rio [22].

#### Compara√ß√£o de Modelos

| Modelo   | Vantagens                                                    | Desvantagens                               |
| -------- | ------------------------------------------------------------ | ------------------------------------------ |
| SGNS     | Eficiente, bom para analogias                                | Limitado a palavras no vocabul√°rio         |
| FastText | Lida com palavras OOV, bom para l√≠nguas morfologicamente ricas | Maior complexidade computacional           |
| GloVe    | Captura estat√≠sticas globais e locais                        | Requer mais dados para treinamento efetivo |
| CBOW     | Mais r√°pido que SGNS para treinar                            | Geralmente inferior em tarefas sem√¢nticas  |

### Conclus√£o

O Skip-gram with Negative Sampling representa um marco fundamental no desenvolvimento de t√©cnicas de word embedding. Sua efici√™ncia computacional, combinada com a capacidade de capturar rela√ß√µes sem√¢nticas e sint√°ticas complexas, tornou-o uma base para in√∫meras aplica√ß√µes em NLP [23].

A compreens√£o profunda do SGNS n√£o apenas fornece insights sobre como as m√°quinas podem aprender representa√ß√µes significativas de linguagem, mas tamb√©m abre caminhos para o desenvolvimento de modelos ainda mais sofisticados. √Ä medida que o campo do NLP continua a evoluir, os princ√≠pios fundamentais estabelecidos pelo SGNS permanecem cruciais para o avan√ßo de tecnologias de processamento de linguagem [24].

### Perguntas Te√≥ricas Avan√ßadas

1. Desenvolva uma prova matem√°tica formal demonstrando que, sob certas condi√ß√µes, o SGNS implicitamente fatoriza uma matriz de Informa√ß√£o M√∫tua Pontual (PMI) entre palavras e contextos.

2. Proponha e justifique teoricamente uma extens√£o do SGNS que incorpore informa√ß√µes de grafos de conhecimento externos durante o treinamento. Como isso afetaria a fun√ß√£o objetivo e o processo de otimiza√ß√£o?

3. Analise matematicamente o impacto da dimensionalidade dos embeddings na capacidade do modelo de capturar rela√ß√µes sem√¢nticas. Existe um "ponto √≥timo" te√≥rico para a dimensionalidade, e como ele se relaciona com o tamanho do vocabul√°rio e a complexidade do corpus?

4. Derive uma express√£o para a complexidade amostral do SGNS em termos de propriedades estat√≠sticas do corpus de treinamento (por exemplo, distribui√ß√£o de frequ√™ncia de palavras, entropia do texto).

5. Proponha um framework te√≥rico para quantificar a "interpretabilidade" dos embeddings SGNS. Como isso se compara com outras t√©cnicas de word embedding em termos de trade-off entre interpretabilidade e performance em tarefas downstream?

### Refer√™ncias

[1] "O Skip-gram with Negative Sampling (SGNS) √© um algoritmo fundamental no campo do processamento de linguagem natural (NLP) e representa uma evolu√ß√£o significativa na cria√ß√£o de word embeddings." *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[2] "O SGNS baseia-se na ideia de que palavras que ocorrem em contextos similares tendem a ter significados semelhantes, um princ√≠pio conhecido como hip√≥tese distribucional." *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[3] "No contexto do SGNS, estas s√£o aprendidas atrav√©s de um processo de classifica√ß√£o bin√°ria" *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[4] "Modelo que prev√™ palavras de contexto dada uma palavra-alvo, invertendo a abordagem tradicional dos modelos de linguagem" *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[5] "T√©cnica de otimiza√ß√£o que seleciona aleatoriamente "palavras de ru√≠do" para contrastar com as palavras de contexto reais, melhorando a efici√™ncia do treinamento" *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[6] "O SGNS difere de modelos anteriores por n√£o requerer a normaliza√ß√£o sobre todo o vocabul√°rio, tornando o treinamento significativamente mais eficiente para vocabul√°rios grandes" *(Trecho de Vector Semantics and Embeddings.pdf.md)*

[7] "A qualidade dos embeddings gerados pelo SGNS √© altamente dependente da escolha dos hiperpar√¢metros,