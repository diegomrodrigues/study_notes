# Probabilidade de Palavra de Contexto: $P(+|w,c)$

<imagem: Um gr√°fico 3D mostrando a distribui√ß√£o de probabilidade $P(+|w,c)$ em fun√ß√£o de vetores de palavras $w$ e $c$ no espa√ßo de embeddings, com cores representando valores de probabilidade de 0 a 1>

## Introdu√ß√£o

A probabilidade de palavra de contexto, denotada como $P(+|w,c)$, √© um conceito fundamental no modelo skip-gram, uma t√©cnica avan√ßada de aprendizado de m√°quina para a cria√ß√£o de embeddings de palavras. Este conceito est√° no cerne da tarefa de classifica√ß√£o bin√°ria que o skip-gram utiliza para aprender representa√ß√µes vetoriais de palavras [1]. A import√¢ncia deste conceito reside na sua capacidade de capturar rela√ß√µes sem√¢nticas entre palavras em um corpus de texto, permitindo a cria√ß√£o de representa√ß√µes vetoriais densas e significativas [2].

## Conceitos Fundamentais

| Conceito      | Explica√ß√£o                                                   |
| ------------- | ------------------------------------------------------------ |
| **Skip-gram** | Modelo de aprendizado de m√°quina que aprende embeddings de palavras prevendo palavras de contexto dado uma palavra-alvo [3]. |
| **Embedding** | Representa√ß√£o vetorial densa de uma palavra em um espa√ßo multidimensional [4]. |
| **Contexto**  | Palavras que ocorrem pr√≥ximas √† palavra-alvo em um texto [5]. |

> ‚ö†Ô∏è **Nota Importante**: A probabilidade $P(+|w,c)$ √© fundamental para o treinamento do modelo skip-gram, pois quantifica a probabilidade de uma palavra $c$ ser um verdadeiro contexto para uma palavra-alvo $w$ [6].

> ‚ùó **Ponto de Aten√ß√£o**: O modelo skip-gram utiliza uma tarefa de classifica√ß√£o bin√°ria para aprender embeddings, diferentemente de modelos baseados em contagem como tf-idf ou PPMI [7].

> ‚úîÔ∏è **Destaque**: A formula√ß√£o matem√°tica de $P(+|w,c)$ permite a otimiza√ß√£o eficiente do modelo atrav√©s de t√©cnicas como amostragem negativa [8].

## Formula√ß√£o Matem√°tica

<imagem: Diagrama mostrando a arquitetura do modelo skip-gram, com vetores de entrada e sa√≠da, e a fun√ß√£o sigmoid aplicada ao produto escalar>

A probabilidade $P(+|w,c)$ √© definida matematicamente como:

$$
P(+|w,c) = \sigma(c \cdot w) = \frac{1}{1 + \exp(-c \cdot w)}
$$

Onde:
- $w$ √© o vetor de embedding da palavra-alvo
- $c$ √© o vetor de embedding da palavra de contexto
- $\sigma$ √© a fun√ß√£o sigmoid
- $\cdot$ denota o produto escalar entre vetores [9]

Esta formula√ß√£o √© derivada da intui√ß√£o de que palavras similares devem ter vetores de embedding com alto produto escalar [10]. A fun√ß√£o sigmoid √© utilizada para mapear o produto escalar para o intervalo [0, 1], interpret√°vel como uma probabilidade [11].

### Prova da Equival√™ncia entre Sigmoid e Softmax Bin√°rio

Podemos demonstrar que a fun√ß√£o sigmoid usada em $P(+|w,c)$ √© equivalente a um softmax bin√°rio:

$$
\begin{align*}
P(+|w,c) &= \frac{\exp(c \cdot w)}{\exp(c \cdot w) + \exp(0)} \\
&= \frac{1}{1 + \exp(-c \cdot w)} \\
&= \sigma(c \cdot w)
\end{align*}
$$

Esta equival√™ncia justifica o uso da sigmoid como uma escolha computacionalmente eficiente para a tarefa de classifica√ß√£o bin√°ria no skip-gram [12].

### Perguntas Te√≥ricas

1. Derive a express√£o para o gradiente de $P(+|w,c)$ em rela√ß√£o a $w$ e $c$. Como isso √© utilizado no processo de aprendizagem do skip-gram?

2. Demonstre matematicamente por que o produto escalar $c \cdot w$ √© uma medida adequada de similaridade entre palavras no espa√ßo de embeddings.

3. Como a escolha da fun√ß√£o sigmoid afeta a estabilidade num√©rica do treinamento do modelo? Existem alternativas que poderiam melhorar este aspecto?

## Amostragem Negativa e Fun√ß√£o Objetivo

A probabilidade $P(+|w,c)$ √© utilizada em conjunto com a t√©cnica de amostragem negativa para formar a fun√ß√£o objetivo do skip-gram. A fun√ß√£o de perda para um par $(w,c)$ √© definida como:

$$
L = -\log\sigma(c \cdot w) - \sum_{i=1}^k \log\sigma(-c_{neg_i} \cdot w)
$$

Onde $c_{neg_i}$ s√£o $k$ amostras negativas (palavras que n√£o s√£o contexto de $w$) [13].

Esta formula√ß√£o permite o treinamento eficiente do modelo, evitando o c√°lculo custoso do denominador do softmax completo sobre todo o vocabul√°rio [14].

> üí° **Insight**: A amostragem negativa transforma o problema de predi√ß√£o multiclasse em v√°rias tarefas de classifica√ß√£o bin√°ria, tornando o treinamento mais eficiente e escal√°vel [15].

### Algoritmo de Treinamento

O algoritmo de treinamento do skip-gram com amostragem negativa pode ser resumido nos seguintes passos:

1. Inicializar aleatoriamente os vetores de embedding $w$ e $c$ para todas as palavras.
2. Para cada palavra-alvo $w$ no corpus:
   a. Selecionar palavras de contexto $c$ dentro de uma janela.
   b. Amostrar $k$ palavras negativas $c_{neg}$.
   c. Calcular a perda $L$ usando a equa√ß√£o acima.
   d. Atualizar os vetores $w$ e $c$ usando gradiente descendente:
      $$w_{t+1} = w_t - \eta \frac{\partial L}{\partial w_t}$$
      $$c_{t+1} = c_t - \eta \frac{\partial L}{\partial c_t}$$
   Onde $\eta$ √© a taxa de aprendizagem [16].

```python
import torch
import torch.nn as nn
import torch.optim as optim

class SkipGramModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGramModel, self).__init__()
        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)
    
    def forward(self, target_word, context_word):
        target_emb = self.target_embeddings(target_word)
        context_emb = self.context_embeddings(context_word)
        return torch.sum(target_emb * context_emb, dim=1)

def train_step(model, optimizer, target_word, context_word, negative_samples):
    model.train()
    optimizer.zero_grad()
    
    # Positive sample
    pos_loss = -torch.log(torch.sigmoid(model(target_word, context_word)))
    
    # Negative samples
    neg_loss = -torch.sum(torch.log(torch.sigmoid(-model(target_word, negative_samples))), dim=1)
    
    loss = torch.mean(pos_loss + neg_loss)
    loss.backward()
    optimizer.step()
    
    return loss.item()

# Uso do modelo
vocab_size = 10000
embedding_dim = 300
model = SkipGramModel(vocab_size, embedding_dim)
optimizer = optim.Adam(model.parameters())

# Exemplo de um passo de treinamento
target_word = torch.LongTensor([1])
context_word = torch.LongTensor([2])
negative_samples = torch.LongTensor([3, 4, 5])
loss = train_step(model, optimizer, target_word, context_word, negative_samples)
print(f"Loss: {loss}")
```

Este c√≥digo implementa o modelo skip-gram usando PyTorch, demonstrando como $P(+|w,c)$ √© utilizado na pr√°tica para treinar embeddings de palavras [17].

### Perguntas Te√≥ricas

1. Como a escolha do n√∫mero $k$ de amostras negativas afeta o tradeoff entre velocidade de treinamento e qualidade dos embeddings aprendidos?

2. Derive a express√£o para o gradiente da fun√ß√£o de perda $L$ em rela√ß√£o aos vetores de embedding $w$ e $c$. Como isso se relaciona com a atualiza√ß√£o dos par√¢metros no algoritmo de treinamento?

3. Analise teoricamente o impacto da inicializa√ß√£o dos vetores de embedding na converg√™ncia do modelo. Existe uma inicializa√ß√£o √≥tima?

## Propriedades e Implica√ß√µes Te√≥ricas

A formula√ß√£o de $P(+|w,c)$ no skip-gram leva a v√°rias propriedades interessantes dos embeddings resultantes:

1. **Captura de Similaridade Sem√¢ntica**: Palavras com significados similares tendem a ter vetores de embedding pr√≥ximos no espa√ßo vetorial [18].

2. **Preserva√ß√£o de Analogias**: Os embeddings aprendidos podem capturar rela√ß√µes anal√≥gicas, como "rei - homem + mulher ‚âà rainha" [19].

3. **Dimensionalidade Reduzida**: Comparado com representa√ß√µes one-hot, os embeddings s√£o muito mais compactos, tipicamente com 50-300 dimens√µes [20].

4. **Generaliza√ß√£o**: Os embeddings podem generalizar para palavras n√£o vistas durante o treinamento, baseando-se em contextos similares [21].

> ‚ö†Ô∏è **Nota Importante**: Apesar dessas propriedades √∫teis, os embeddings podem tamb√©m capturar e amplificar vieses presentes nos dados de treinamento, como estere√≥tipos de g√™nero ou √©tnicos [22].

### An√°lise Te√≥rica da Converg√™ncia

A converg√™ncia do modelo skip-gram pode ser analisada atrav√©s da teoria de otimiza√ß√£o n√£o-convexa. Considerando que a fun√ß√£o objetivo √© n√£o-convexa devido √† sua natureza neural, podemos utilizar resultados recentes em otimiza√ß√£o estoc√°stica para garantir a converg√™ncia para um m√≠nimo local [23].

Seja $f(w,c)$ a fun√ß√£o objetivo do skip-gram. Podemos mostrar que, sob certas condi√ß√µes de regularidade e com uma taxa de aprendizado apropriada $\eta_t$, o algoritmo de gradiente estoc√°stico converge em expectativa:

$$
\mathbb{E}[\|\nabla f(w_t, c_t)\|^2] \leq \frac{C}{\sqrt{T}}
$$

Onde $T$ √© o n√∫mero de itera√ß√µes e $C$ √© uma constante que depende das propriedades da fun√ß√£o e da distribui√ß√£o dos dados [24].

### Perguntas Te√≥ricas

1. Como a dimensionalidade do espa√ßo de embedding afeta a capacidade do modelo de capturar rela√ß√µes sem√¢nticas complexas? Existe um limite te√≥rico para a informa√ß√£o que pode ser codificada?

2. Demonstre matematicamente como a propriedade de analogia emerge da estrutura do modelo skip-gram e da fun√ß√£o objetivo utilizada.

3. Analise teoricamente o impacto do tamanho do vocabul√°rio na complexidade computacional e na qualidade dos embeddings aprendidos. Existe um tradeoff √≥timo?

## Extens√µes e Variantes

O conceito de $P(+|w,c)$ tem sido estendido e modificado em v√°rias dire√ß√µes:

1. **GloVe**: Utiliza uma abordagem baseada em contagem, mas mant√©m a ideia de produto escalar entre vetores [25].

2. **FastText**: Estende o skip-gram para incorporar informa√ß√µes de subpalavras, melhorando a representa√ß√£o de palavras raras e fora do vocabul√°rio [26].

3. **BERT**: Utiliza uma arquitetura de transformador para aprender representa√ß√µes contextuais, onde $P(+|w,c)$ √© substitu√≠da por uma fun√ß√£o mais complexa que considera o contexto bidirecionalmente [27].

```python
import torch
import torch.nn as nn

class FastTextModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, ngram_range=(3,6)):
        super(FastTextModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.ngram_range = ngram_range
        
    def forward(self, word, subwords):
        word_emb = self.embedding(word)
        subword_emb = self.embedding(subwords).mean(dim=1)
        return word_emb + subword_emb

# Uso do modelo
vocab_size = 100000
embedding_dim = 300
model = FastTextModel(vocab_size, embedding_dim)

# Exemplo de forward pass
word = torch.LongTensor([42])
subwords = torch.LongTensor([[1, 2, 3], [4, 5, 6]])  # Exemplo de n-gramas
embedding = model(word, subwords)
print(f"Embedding shape: {embedding.shape}")
```

Este c√≥digo demonstra uma implementa√ß√£o simplificada do modelo FastText, que estende o conceito de $P(+|w,c)$ para incluir informa√ß√µes de subpalavras [28].

## Conclus√£o

A probabilidade de palavra de contexto $P(+|w,c)$ √© um conceito central no modelo skip-gram e tem implica√ß√µes profundas na aprendizagem de representa√ß√µes de palavras. Sua formula√ß√£o matem√°tica elegante e eficiente computacionalmente permitiu avan√ßos significativos em v√°rias tarefas de processamento de linguagem natural [29]. Compreender as nuances te√≥ricas e pr√°ticas deste conceito √© crucial para desenvolver e aplicar modelos de linguagem mais avan√ßados e eficazes [30].

## Perguntas Te√≥ricas Avan√ßadas

1. Derive uma express√£o para a complexidade de amostra do modelo skip-gram em termos do tamanho do vocabul√°rio, dimens√£o do embedding e n√∫mero de exemplos de treinamento. Como isso se compara com m√©todos baseados em contagem como LSA?

2. Analise teoricamente o impacto da distribui√ß√£o de frequ√™ncia de palavras no corpus de treinamento na qualidade dos embeddings aprendidos. Como podemos modificar $P(+|w,c)$ para mitigar o vi√©s em dire√ß√£o a palavras frequentes?

3. Demonstre matematicamente como a propriedade de composicionalidade (e.g., vetor("king") - vetor("man") + vetor("woman") ‚âà vetor("queen")) emerge da estrutura do modelo skip-gram e da fun√ß√£o objetivo utilizada.

4. Desenvolva uma prova formal de que, sob certas condi√ß√µes, os embeddings aprendidos pelo skip-gram convergem para a fatora√ß√£o da matriz de informa√ß√£o m√∫tua pontual (PMI) entre palavras e contextos.

5. Proponha e analise teoricamente uma extens√£o do modelo skip-gram que incorpore informa√ß√µes sint√°ticas expl√≠citas (e.g., depend√™ncias gramaticais) na defini√ß√£o de $P(+|w,c)$. Como isso afetaria as propriedades dos embeddings resultantes?

## Anexos

### A.1 Deriva√ß√£o do Gradiente de $P(+|w,c)$

O gradiente de $P(+|w,c)$ em rela√ß√£o a $w$ √© dado por:

$$
\begin{align*}
\frac{\partial P(+|w,c)}{\partial w} &= \frac{\partial}{\partial w} \sigma(c \cdot w) \\
&= \sigma(c \cdot w)(1 - \sigma(c \cdot w)) \cdot c \\
&= P(+|w,c)(1 - P(+|w,c)) \cdot c
\