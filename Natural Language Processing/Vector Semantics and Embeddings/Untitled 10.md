# Fun√ß√µes de Perda e Otimiza√ß√£o para Word Embeddings

<imagem: Uma visualiza√ß√£o de um espa√ßo vetorial multidimensional com vetores de palavras sendo gradualmente ajustados para minimizar uma fun√ß√£o de perda, representada por uma superf√≠cie tridimensional complexa>

## Introdu√ß√£o

As fun√ß√µes de perda e os algoritmos de otimiza√ß√£o s√£o componentes fundamentais no treinamento de modelos de word embeddings, como o Skip-gram. Esses elementos permitem que os modelos aprendam representa√ß√µes vetoriais densas e significativas das palavras a partir de grandes corpora de texto [1]. Neste resumo, exploraremos em profundidade a fun√ß√£o de perda baseada na negative log-likelihood e o processo de otimiza√ß√£o por meio do Stochastic Gradient Descent (SGD) no contexto do modelo Skip-gram, uma das abordagens mais influentes para a gera√ß√£o de word embeddings [2].

## Conceitos Fundamentais

| Conceito              | Explica√ß√£o                                                   |
| --------------------- | ------------------------------------------------------------ |
| **Word Embeddings**   | Representa√ß√µes vetoriais densas de palavras em um espa√ßo multidimensional, capturando rela√ß√µes sem√¢nticas e sint√°ticas entre as palavras [3]. |
| **Skip-gram Model**   | Um modelo neural que aprende word embeddings prevendo as palavras de contexto dada uma palavra-alvo [4]. |
| **Negative Sampling** | T√©cnica de amostragem que seleciona palavras "negativas" aleat√≥rias para contrastar com as palavras de contexto positivas reais durante o treinamento [5]. |

> ‚ö†Ô∏è **Nota Importante**: A fun√ß√£o de perda e o algoritmo de otimiza√ß√£o trabalham em conjunto para ajustar iterativamente os embeddings, minimizando a discrep√¢ncia entre as previs√µes do modelo e os dados observados [6].

## Fun√ß√£o de Perda: Negative Log-Likelihood

A fun√ß√£o de perda utilizada no treinamento do modelo Skip-gram com negative sampling √© baseada na maximiza√ß√£o da probabilidade de ocorr√™ncia das palavras de contexto observadas e na minimiza√ß√£o da probabilidade de ocorr√™ncia das palavras negativas amostradas [7]. 

### Formula√ß√£o Matem√°tica

A fun√ß√£o objetivo para um par de palavras (palavra-alvo $w$ e palavra de contexto $c$) pode ser expressa como:

$$
L = -\log\left[P(+|w, c_{pos}) \prod_{i=1}^{k} P(-|w, c_{neg_i})\right]
$$

onde:
- $P(+|w, c_{pos})$ √© a probabilidade de $c_{pos}$ ser uma palavra de contexto real para $w$
- $P(-|w, c_{neg_i})$ √© a probabilidade de $c_{neg_i}$ n√£o ser uma palavra de contexto para $w$
- $k$ √© o n√∫mero de amostras negativas [8]

Expandindo esta equa√ß√£o, obtemos:

$$
L = -\left[\log \sigma(c_{pos} \cdot w) + \sum_{i=1}^{k} \log \sigma(-c_{neg_i} \cdot w)\right]
$$

onde $\sigma$ √© a fun√ß√£o sigmoide e $\cdot$ denota o produto escalar [9].

> üí° **Destaque**: Esta formula√ß√£o da fun√ß√£o de perda incentiva o modelo a aumentar a similaridade entre embeddings de palavras que co-ocorrem frequentemente, enquanto diminui a similaridade com palavras negativas amostradas [10].

### An√°lise Te√≥rica

A escolha desta fun√ß√£o de perda √© fundamentada em princ√≠pios de teoria da informa√ß√£o e aprendizado de m√°quina:

1. **Maximiza√ß√£o da verossimilhan√ßa**: O termo negativo na equa√ß√£o corresponde √† maximiza√ß√£o da log-verossimilhan√ßa dos dados observados [11].

2. **Contrastive Learning**: A inclus√£o de amostras negativas permite que o modelo aprenda por contraste, distinguindo entre contextos v√°lidos e inv√°lidos [12].

3. **Efici√™ncia Computacional**: O uso de negative sampling reduz significativamente o custo computacional em compara√ß√£o com a softmax completa sobre todo o vocabul√°rio [13].

#### Perguntas Te√≥ricas

1. Derive a express√£o para o gradiente da fun√ß√£o de perda em rela√ß√£o aos embeddings da palavra-alvo e da palavra de contexto.
2. Como a escolha do n√∫mero de amostras negativas $k$ afeta teoricamente o trade-off entre qualidade dos embeddings e efici√™ncia computacional?
3. Demonstre matematicamente por que a fun√ß√£o sigmoide √© uma escolha apropriada para modelar as probabilidades neste contexto.

## Stochastic Gradient Descent (SGD)

O Stochastic Gradient Descent √© o algoritmo de otimiza√ß√£o utilizado para minimizar a fun√ß√£o de perda e atualizar os embeddings no modelo Skip-gram [14].

### Formula√ß√£o Matem√°tica

As equa√ß√µes de atualiza√ß√£o para os embeddings da palavra-alvo $w$ e da palavra de contexto $c$ s√£o:

$$
c_{pos}^{t+1} = c_{pos}^{t} - \eta[\sigma (c_{pos}^{t} \cdot w^{t}) - 1]w^{t}
$$

$$
c_{neg}^{t+1} = c_{neg}^{t} - \eta[\sigma (c_{neg}^{t} \cdot w^{t})]w^{t}
$$

$$
w^{t+1} = w^{t} - \eta \left( [\sigma (c_{pos}^{t} \cdot w^{t}) - 1]c_{pos}^{t} + \sum_{i=1}^{k} [\sigma (c_{neg_i}^{t} \cdot w^{t})]c_{neg_i}^{t} \right)
$$

onde $\eta$ √© a taxa de aprendizado e $t$ indica a itera√ß√£o atual [15].

> ‚ùó **Ponto de Aten√ß√£o**: A natureza estoc√°stica do SGD, que atualiza os par√¢metros com base em mini-batches ou exemplos individuais, permite uma converg√™ncia mais r√°pida e uma melhor generaliza√ß√£o em compara√ß√£o com o gradiente descendente em lote [16].

### An√°lise Te√≥rica do SGD no Contexto de Word Embeddings

1. **Converg√™ncia**: A converg√™ncia do SGD para um m√≠nimo local ou global da fun√ß√£o de perda √© garantida sob certas condi√ß√µes, como a escolha adequada da taxa de aprendizado e a convexidade da fun√ß√£o objetivo [17].

2. **Regulariza√ß√£o Impl√≠cita**: O ru√≠do introduzido pela natureza estoc√°stica do SGD atua como uma forma de regulariza√ß√£o, ajudando a prevenir o overfitting [18].

3. **Adapta√ß√£o a Grandes Datasets**: O SGD √© particularmente adequado para o treinamento de word embeddings em grandes corpora devido √† sua efici√™ncia computacional e capacidade de lidar com dados em stream [19].

#### Perguntas Te√≥ricas

1. Prove que, sob condi√ß√µes apropriadas, o SGD converge para um ponto estacion√°rio da fun√ß√£o de perda no limite de infinitas itera√ß√µes.
2. Como a escolha da taxa de aprendizado $\eta$ afeta teoricamente a converg√™ncia e a qualidade final dos embeddings? Derive uma express√£o para a taxa de aprendizado √≥tima.
3. Analise teoricamente o impacto do tamanho do mini-batch na vari√¢ncia do gradiente e na velocidade de converg√™ncia do SGD no contexto de treinamento de word embeddings.

## Implementa√ß√£o Avan√ßada

Aqui est√° um exemplo de implementa√ß√£o avan√ßada em Python utilizando PyTorch para o treinamento de word embeddings com o modelo Skip-gram:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np

class SkipgramDataset(Dataset):
    def __init__(self, corpus, window_size, num_negative):
        self.corpus = corpus
        self.window_size = window_size
        self.num_negative = num_negative
        self.vocab = self._build_vocab()
        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}
        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}
        self.word_freqs = np.array([self._get_word_freq(word) for word in self.vocab])
        self.word_freqs = self.word_freqs ** 0.75
        self.word_freqs = self.word_freqs / np.sum(self.word_freqs)
        
    def _build_vocab(self):
        return sorted(set(self.corpus))
    
    def _get_word_freq(self, word):
        return self.corpus.count(word)
    
    def __len__(self):
        return len(self.corpus)
    
    def __getitem__(self, idx):
        center_word = self.corpus[idx]
        pos_context = self._get_context_words(idx)
        neg_context = self._get_negative_samples(len(pos_context))
        
        center_word = torch.tensor([self.word_to_idx[center_word]], dtype=torch.long)
        pos_context = torch.tensor([self.word_to_idx[w] for w in pos_context], dtype=torch.long)
        neg_context = torch.tensor(neg_context, dtype=torch.long)
        
        return center_word, pos_context, neg_context
    
    def _get_context_words(self, idx):
        start = max(0, idx - self.window_size)
        end = min(len(self.corpus), idx + self.window_size + 1)
        context = [self.corpus[i] for i in range(start, end) if i != idx]
        return context
    
    def _get_negative_samples(self, num_pos):
        neg_samples = np.random.choice(
            len(self.vocab),
            size=(num_pos, self.num_negative),
            p=self.word_freqs
        )
        return neg_samples

class SkipgramModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipgramModel, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.output = nn.Embedding(vocab_size, embedding_dim)
        
    def forward(self, center_words, context_words):
        center_embeds = self.embeddings(center_words)
        context_embeds = self.output(context_words)
        return torch.sum(center_embeds * context_embeds, dim=1)

def train_skipgram(model, dataset, num_epochs, batch_size, lr):
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_function = nn.BCEWithLogitsLoss()
    
    for epoch in range(num_epochs):
        total_loss = 0
        for center_words, pos_context, neg_context in dataloader:
            optimizer.zero_grad()
            
            pos_loss = loss_function(
                model(center_words, pos_context),
                torch.ones(pos_context.shape[0])
            )
            neg_loss = loss_function(
                model(center_words.repeat_interleave(neg_context.shape[1]), neg_context.flatten()),
                torch.zeros(neg_context.numel())
            )
            
            loss = pos_loss + neg_loss
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}")

# Exemplo de uso
corpus = ["the", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]
dataset = SkipgramDataset(corpus, window_size=2, num_negative=5)
model = SkipgramModel(len(dataset.vocab), embedding_dim=100)
train_skipgram(model, dataset, num_epochs=50, batch_size=2, lr=0.001)
```

Este c√≥digo implementa o modelo Skip-gram com negative sampling, utilizando PyTorch para treinamento eficiente em GPU [20]. A implementa√ß√£o inclui:

1. Uma classe `SkipgramDataset` que prepara os dados para treinamento, incluindo a gera√ß√£o de amostras negativas.
2. Uma classe `SkipgramModel` que define a arquitetura do modelo.
3. Uma fun√ß√£o `train_skipgram` que realiza o treinamento utilizando SGD (via otimizador Adam) e a fun√ß√£o de perda BCE with Logits [21].

> ‚úîÔ∏è **Destaque**: Esta implementa√ß√£o incorpora t√©cnicas avan√ßadas como subsampling de palavras frequentes e exponencia√ß√£o das frequ√™ncias das palavras para melhorar a qualidade dos embeddings resultantes [22].

## Discuss√£o Cr√≠tica

### Desafios e Limita√ß√µes

1. **Escala do Vocabul√°rio**: Para vocabul√°rios muito grandes, o custo computacional de atualizar todos os embeddings pode ser proibitivo, mesmo com negative sampling [23].

2. **Palavras Raras**: Palavras com poucas ocorr√™ncias no corpus podem n√£o ter embeddings de boa qualidade devido √† falta de contextos de treinamento [24].

3. **Polissemia**: O modelo Skip-gram padr√£o n√£o lida bem com palavras poliss√™micas, atribuindo um √∫nico vetor para todos os sentidos de uma palavra [25].

### Perspectivas Futuras

1. **Embeddings Contextuais**: Modelos como BERT e GPT que geram embeddings din√¢micos baseados no contexto podem superar algumas limita√ß√µes dos embeddings est√°ticos [26].

2. **Incorpora√ß√£o de Conhecimento Externo**: Integrar informa√ß√µes de ontologias ou bases de conhecimento para melhorar a qualidade sem√¢ntica dos embeddings [27].

3. **Embeddings Multil√≠ngues**: Desenvolver t√©cnicas para criar espa√ßos de embeddings unificados para m√∫ltiplas l√≠nguas, facilitando tarefas de tradu√ß√£o e transfer√™ncia de conhecimento entre idiomas [28].

## Conclus√£o

A fun√ß√£o de perda baseada na negative log-likelihood e o algoritmo de otimiza√ß√£o Stochastic Gradient Descent s√£o componentes cruciais no treinamento de modelos de word embeddings como o Skip-gram. Estes elementos permitem a aprendizagem eficiente de representa√ß√µes vetoriais densas que capturam rela√ß√µes sem√¢nticas e sint√°ticas complexas entre palavras [29]. 

A compreens√£o profunda destes conceitos √© fundamental para o desenvolvimento e aprimoramento de t√©cnicas de processamento de linguagem natural, com aplica√ß√µes que v√£o desde a an√°lise de sentimentos at√© a tradu√ß√£o autom√°tica [30]. √Ä medida que o campo evolui, novas abordagens para fun√ß√µes de perda e algoritmos de otimiza√ß√£o continuar√£o a desempenhar um papel central na melhoria da qualidade e efici√™ncia dos modelos de linguagem [31].

## Perguntas Te√≥ricas Avan√ßadas

1. Derive uma express√£o para a complexidade computacional do treinamento do modelo Skip-gram com negative sampling em fun√ß√£o do tamanho do vocabul√°rio, dimens√£o dos embeddings e n√∫mero de √©pocas. Compare esta complexidade com a do modelo original sem negative sampling.

2. Formule uma prova matem√°tica demonstrando que os embeddings aprendidos pelo modelo Skip-gram preservam certas propriedades lineares observadas empiricamente, como a rela√ß√£o "rei - homem + mulher ‚âà rainha".

3. Desenvolva uma extens√£o te√≥rica do modelo Skip-gram que incorpore informa√ß√µes de subpalavras (por exemplo, n-gramas de caracteres) na fun√ß√£o de perda. Derive as equa√ß√µes de atualiza√ß√£o correspondentes e analise o impacto te√≥rico desta modifica√ß√£o na qualidade dos embeddings para palavras raras e desconhecidas.

4. Proponha e analise teoricamente uma fun√ß√£o de perda alternativa que explicitamente otimize a preserva√ß√£o