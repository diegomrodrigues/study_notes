# A Hip√≥tese de Markov em Modelos de Linguagem N-gram

<imagem: Uma representa√ß√£o visual de uma cadeia de Markov de ordem $n$, mostrando como a probabilidade de uma palavra depende apenas das $n$ palavras anteriores, com setas indicando as depend√™ncias limitadas.>

## Introdu√ß√£o

A **Hip√≥tese de Markov** √© um conceito fundamental na modelagem estat√≠stica de linguagem, especialmente em modelos n-gram. Esta hip√≥tese simplifica significativamente o c√°lculo de probabilidades em sequ√™ncias de palavras, permitindo aproxima√ß√µes eficientes e computacionalmente vi√°veis [1]. Neste resumo, exploraremos em profundidade a Hip√≥tese de Markov, sua aplica√ß√£o em modelos n-gram e suas implica√ß√µes no processamento de linguagem natural.

==A ess√™ncia da Hip√≥tese de Markov em modelos de linguagem √© a suposi√ß√£o de que a probabilidade de ocorr√™ncia de uma palavra depende apenas de um n√∫mero limitado de palavras anteriores==, ao inv√©s de toda a hist√≥ria precedente. Isso possibilita a simplifica√ß√£o de modelos complexos de linguagem em c√°lculos mais manej√°veis, mantendo um n√≠vel aceit√°vel de precis√£o [2].

> ‚ö†Ô∏è **Nota Importante**: Embora poderosa, a Hip√≥tese de Markov implica em limita√ß√µes na capacidade do modelo de capturar depend√™ncias de longo alcance na linguagem [3].

## Conceitos Fundamentais

| Conceito                      | Explica√ß√£o                                                   |
| ----------------------------- | ------------------------------------------------------------ |
| **Hip√≥tese de Markov**        | Sup√µe que a probabilidade de uma palavra depende apenas de um n√∫mero fixo de palavras anteriores, n√£o de toda a sequ√™ncia anterior [4]. |
| **N-gram**                    | Modelo de linguagem baseado na Hip√≥tese de Markov, onde $n$ representa o n√∫mero de palavras consideradas no contexto [5]. |
| **Probabilidade Condicional** | Base matem√°tica para calcular a probabilidade de uma palavra dado seu contexto em modelos n-gram [6]. |

### Formula√ß√£o Matem√°tica da Hip√≥tese de Markov

A Hip√≥tese de Markov pode ser expressa matematicamente como [7]:

$$
P(w_n \mid w_1^{n-1}) \approx P(w_n \mid w_{n-N+1}^{n-1})
$$

Onde:

- $w_n$ √© a palavra atual.
- $w_1^{n-1}$ representa todas as palavras anteriores.
- $w_{n-N+1}^{n-1}$ representa as $N-1$ palavras anteriores em um modelo n-gram.

Esta aproxima√ß√£o √© o n√∫cleo da simplifica√ß√£o que permite a implementa√ß√£o eficiente de modelos n-gram [8].

> üí° **Insight**: A Hip√≥tese de Markov reduz drasticamente o espa√ßo de par√¢metros a serem estimados, tornando o treinamento de modelos de linguagem computacionalmente vi√°vel [9].

### Implica√ß√µes da Hip√≥tese de Markov

1. **Redu√ß√£o de Complexidade**: Ao limitar o contexto considerado, reduz-se exponencialmente o n√∫mero de probabilidades que precisam ser estimadas [10]. Em vez de considerar todas as poss√≠veis sequ√™ncias de palavras anteriores, consideram-se apenas as √∫ltimas $N-1$ palavras.
   
2. **Perda de Informa√ß√£o**: Ignora depend√™ncias de longo alcance, o que pode levar √† perda de informa√ß√µes contextuais importantes [11]. Isso afeta a capacidade do modelo de capturar nuances e rela√ß√µes sem√¢nticas distantes na linguagem.

3. **Efici√™ncia Computacional**: Permite c√°lculos r√°pidos e eficientes de probabilidades de sequ√™ncias, viabilizando a utiliza√ß√£o pr√°tica em aplica√ß√µes de processamento de linguagem natural [12].

## Modelos N-gram e a Hip√≥tese de Markov

Os modelos n-gram s√£o uma aplica√ß√£o direta da Hip√≥tese de Markov na modelagem de linguagem [13]. Eles aproximam a probabilidade de uma palavra com base nas $N-1$ palavras anteriores:

1. **Unigram ($n=1$)**: Assume independ√™ncia completa entre palavras.

   $$
   P(w_n) = \frac{\text{count}(w_n)}{N}
   $$

2. **Bigram ($n=2$)**: Considera a palavra imediatamente anterior.

   $$
   P(w_n \mid w_{n-1}) = \frac{\text{count}(w_{n-1}, w_n)}{\text{count}(w_{n-1})}
   $$

3. **Trigram ($n=3$)**: Considera as duas palavras anteriores.

   $$
   P(w_n \mid w_{n-2}, w_{n-1}) = \frac{\text{count}(w_{n-2}, w_{n-1}, w_n)}{\text{count}(w_{n-2}, w_{n-1})}
   $$

> ‚úîÔ∏è **Destaque**: A escolha do valor de $n$ representa um trade-off entre a capacidade de capturar contexto e a esparsidade dos dados [14]. Valores maiores de $n$ capturam mais contexto, mas aumentam a esparsidade.

### Exemplo Pr√°tico

Considere a sequ√™ncia "I am Sam":

1. **Probabilidade Unigram**:

   $$
   P(\text{I}) \times P(\text{am}) \times P(\text{Sam})
   $$

2. **Probabilidade Bigram**:

   $$
   P(\text{I} \mid \langle s \rangle) \times P(\text{am} \mid \text{I}) \times P(\text{Sam} \mid \text{am})
   $$

3. **Probabilidade Trigram**:

   $$
   P(\text{I} \mid \langle s \rangle \langle s \rangle) \times P(\text{am} \mid \langle s \rangle \text{I}) \times P(\text{Sam} \mid \text{I am})
   $$

==Onde $\langle s \rangle$ representa o in√≠cio da senten√ßa [15].==

### 1. **Perplexidade e Entropia Cruzada**

A **perplexidade** √© uma medida que avalia o qu√£o bem um modelo de linguagem prev√™ uma sequ√™ncia de palavras. Ela est√° diretamente relacionada √† **entropia cruzada** do modelo.

**Deriva√ß√£o da Perplexidade:**

A entropia cruzada $H(p)$ de um modelo de linguagem √© definida como:

$$
H(p) = -\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i \mid w_{i-n+1}^{i-1})
$$

Onde:

- $N$ √© o n√∫mero total de palavras na sequ√™ncia de teste.
- $P(w_i \mid w_{i-n+1}^{i-1})$ √© a probabilidade predita pelo modelo para a palavra $w_i$ dado o contexto das $n-1$ palavras anteriores.

A perplexidade √© ent√£o definida como:

$$
\text{Perplexidade} = 2^{H(p)} = 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i \mid w_{i-n+1}^{i-1})}
$$

Simplificando, temos:

$$
\text{Perplexidade} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \ln P(w_i \mid w_{i-n+1}^{i-1})\right)
$$

(Usando a base natural de logaritmos, onde $\ln 2$ √© a constante de convers√£o entre logaritmos de base 2 e logaritmos naturais.)

**Rela√ß√£o entre Perplexidade e Entropia Cruzada:**

- ==A entropia cruzada $H(p)$ mede a m√©dia da incerteza ou surpresa associada √†s previs√µes do modelo em rela√ß√£o √† distribui√ß√£o real dos dados.==
- A perplexidade √© a ==exponencia√ß√£o da entropia cruzada==, servindo como uma medida mais intuitiva: ==representa o n√∫mero efetivo de escolhas poss√≠veis que o modelo considera ao prever a pr√≥xima palavra.==
- ==Uma perplexidade menor indica que o modelo √© mais certo em suas previs√µes==, ou seja, atribui probabilidades maiores √†s palavras corretas.

**Interpreta√ß√£o:**

- **Perplexidade como M√©trica de Desempenho**: Avalia a capacidade do modelo de prever sequ√™ncias de teste n√£o vistas. Um modelo perfeito teria perplexidade igual a 1.
- **Entropia Cruzada como Base Te√≥rica**: Fornece a fundamenta√ß√£o matem√°tica para o c√°lculo da perplexidade, relacionando a distribui√ß√£o predita pelo modelo com a distribui√ß√£o verdadeira dos dados.

### 2. **Limita√ß√µes da Hip√≥tese de Markov**

A **Hip√≥tese de Markov** assume que a probabilidade de uma palavra depende apenas das $n-1$ palavras anteriores, isto √©:

$$
P(w_n \mid w_1^{n-1}) \approx P(w_n \mid w_{n-N+1}^{n-1})
$$

Onde $N$ √© a ordem do modelo n-gram.

**Demonstra√ß√£o das Limita√ß√µes:**

- **Depend√™ncias de Longo Alcance**: Em linguagens naturais, h√° depend√™ncias que podem abranger mais do que $n-1$ palavras. Por exemplo, em frases complexas, o sujeito e o verbo podem estar distantes, e elementos como pronomes podem referir-se a substantivos mencionados muito antes.
  
- **Propriedades de Cadeias de Markov**: ==Uma cadeia de Markov de ordem $N-1$ n√£o captura transi√ß√µes de estados (palavras) que est√£o al√©m dos $N-1$ estados anteriores.== Assim, informa√ß√µes relevantes fora deste contexto imediato s√£o ignoradas.

**Matematicamente:**

- A probabilidade real deveria considerar toda a hist√≥ria:

  $$
  P(w_n \mid w_1^{n-1}) = P(w_n \mid w_{n-N+1}^{n-1}, w_1^{n-N})
  $$

- A Hip√≥tese de Markov simplifica para:

  $$
  P(w_n \mid w_1^{n-1}) \approx P(w_n \mid w_{n-N+1}^{n-1})
  $$

- **Consequ√™ncia**: Se houver uma forte depend√™ncia entre $w_n$ e alguma palavra em $w_1^{n-N}$, o modelo n-gram n√£o capturar√° essa depend√™ncia, levando a uma estimativa imprecisa de $P(w_n \mid w_1^{n-1})$.

**Exemplo Pr√°tico:**

- Considere a frase: "Se chover amanh√£, o jogo ser√° cancelado, ent√£o traga um guarda-chuva."
- A palavra "guarda-chuva" est√° relacionada √† possibilidade de "chover", mencionada no in√≠cio. Um modelo n-gram com $n$ pequeno n√£o capturar√° essa rela√ß√£o, subestimando a probabilidade de "guarda-chuva" neste contexto.

**Conclus√£o:**

- A Hip√≥tese de Markov, ao limitar o contexto, n√£o consegue modelar depend√™ncias de longo alcance.
- Isso resulta em estimativas de probabilidade imprecisas para sequ√™ncias onde tais depend√™ncias s√£o significativas.
- Portanto, a precis√£o do modelo √© comprometida em contextos onde a linguagem natural exige considera√ß√µes al√©m das $n-1$ palavras anteriores.

### 3. **Esparsidade de Dados**

**An√°lise Te√≥rica do Impacto do Aumento de $n$:**

- **Crescimento Combinatorial**: O n√∫mero total de poss√≠veis n-grams √© proporcional a $V^n$, onde $V$ √© o tamanho do vocabul√°rio.
  
- **Esparsidade**: √Ä medida que $n$ aumenta, o espa√ßo de poss√≠veis n-grams cresce exponencialmente, mas o tamanho do corpus de treinamento permanece fixo. Isso leva a:

  - **Muitos n-grams Inobservados**: A maioria dos n-grams poss√≠veis n√£o aparece no corpus.
  - **Contagens Baixas**: Mesmo os n-grams observados t√™m contagens muito pequenas, frequentemente apenas uma ocorr√™ncia.

**Impacto na Qualidade das Estimativas de Probabilidade:**

- **Estimativas N√£o Confi√°veis**: Probabilidades baseadas em poucas observa√ß√µes s√£o estatisticamente inst√°veis.
  
- **Probabilidades Zero**: N-grams n√£o observados recebem probabilidade zero, o que √© problem√°tico para modelagem, especialmente em aplica√ß√µes como reconhecimento de fala ou tradu√ß√£o autom√°tica.

- **Overfitting**: O modelo pode se ajustar excessivamente aos dados de treinamento, capturando ru√≠do ao inv√©s de padr√µes reais.

**Consequ√™ncias Pr√°ticas:**

- **Necessidade de Suaviza√ß√£o**: T√©cnicas como suaviza√ß√£o de Laplace, Good-Turing e Kneser-Ney s√£o empregadas para ajustar as probabilidades, atribuindo valores n√£o zero a n-grams n√£o observados.

- **Limita√ß√£o no Valor de $n$**: Na pr√°tica, valores de $n$ maiores que 3 ou 4 s√£o raramente usados sem t√©cnicas avan√ßadas devido √† esparsidade extrema.

**Conclus√£o:**

- **Trade-off entre Contexto e Esparsidade**: Aumentar $n$ captura mais contexto, mas agrava a esparsidade dos dados.
  
- **Equil√≠brio Necess√°rio**: √â crucial encontrar um equil√≠brio que permita capturar depend√™ncias relevantes sem comprometer a confiabilidade das estimativas.

- **Import√¢ncia de Dados Abundantes**: Para modelos com $n$ elevado, corpora muito grandes s√£o necess√°rios para mitigar a esparsidade, o que nem sempre √© vi√°vel.

---

**Refer√™ncias Adicionais:**

- **Lei de Zipf**: Em lingu√≠stica computacional, a distribui√ß√£o de frequ√™ncias das palavras segue a Lei de Zipf, indicando que poucas palavras ocorrem com alta frequ√™ncia, enquanto a maioria ocorre raramente. Isso exacerba a esparsidade em modelos n-gram.

- **Modelos Alternativos**: Para lidar com as limita√ß√µes dos modelos n-gram de alto valor de $n$, modelos baseados em redes neurais e t√©cnicas de aprendizagem profunda s√£o utilizados, pois podem capturar depend√™ncias de longo alcance sem sofrer tanto com a esparsidade.

## Limita√ß√µes e Extens√µes da Hip√≥tese de Markov

### Limita√ß√µes

1. **Depend√™ncias de Longo Alcance**: ==A Hip√≥tese de Markov falha em capturar rela√ß√µes entre palavras distantes==, o que √© crucial para a compreens√£o de contextos complexos [16].

2. **Esparsidade de Dados**: Com o aumento de $n$, o n√∫mero de poss√≠veis n-grams cresce exponencialmente, resultando em muitos n-grams n√£o observados no corpus de treinamento [17].

3. **Necessidade de Suaviza√ß√£o**: Atribui probabilidade zero a sequ√™ncias n√£o observadas, exigindo t√©cnicas de suaviza√ß√£o para ajustar as probabilidades [18].

### Extens√µes e Melhorias

1. **Interpola√ß√£o**: ==Combina modelos de diferentes ordens para melhorar as estimativas [19]. A probabilidade interpolada √© dada por:==
   $$
   P_{\text{interp}}(w_n \mid w_{n-2}, w_{n-1}) = \lambda_1 P(w_n) + \lambda_2 P(w_n \mid w_{n-1}) + \lambda_3 P(w_n \mid w_{n-2}, w_{n-1})
   $$
   
   Onde $\lambda_i$ s√£o pesos que somam 1.
   
2. **Backoff**: Recorre a modelos de ordem inferior quando n√£o h√° dados suficientes para estimar um n-gram de ordem superior [20]. Usa-se a probabilidade de um (n-1)-gram quando o n-gram √© desconhecido.

3. **T√©cnicas de Suaviza√ß√£o**: M√©todos como Add-one, Good-Turing e Kneser-Ney s√£o utilizados para ajustar as probabilidades de n-grams n√£o observados [21]. Por exemplo, a suaviza√ß√£o de Kneser-Ney √© especialmente eficaz para modelos de linguagem.

> ‚ùó **Ponto de Aten√ß√£o**: Mesmo com essas extens√µes, modelos baseados na Hip√≥tese de Markov t√™m limita√ß√µes fundamentais em capturar a complexidade total da linguagem natural [22]. Depend√™ncias sem√¢nticas e contextuais de longo alcance muitas vezes ficam fora do alcance desses modelos.

## Implementa√ß√£o e Considera√ß√µes Pr√°ticas

A implementa√ß√£o de modelos n-gram baseados na Hip√≥tese de Markov envolve diversas considera√ß√µes pr√°ticas:

1. **Contagem de N-grams**: Efici√™ncia na contagem e armazenamento de n-grams √© crucial, especialmente para valores altos de $n$ [23].

2. **Tratamento de Vocabul√°rio**: Decis√µes sobre tokens desconhecidos (UNK) e o tamanho do vocabul√°rio impactam diretamente na qualidade do modelo [24].

3. **Suaviza√ß√£o e Backoff**: Implementa√ß√£o de t√©cnicas adequadas para lidar com n-grams n√£o observados √© essencial para evitar probabilidades zero [25].

Exemplo de implementa√ß√£o simplificada de um modelo bigram em Python:

```python
import numpy as np
from collections import defaultdict, Counter

class BigramModel:
    def __init__(self):
        self.bigram_counts = defaultdict(Counter)
        self.unigram_counts = Counter()
        self.vocab = set()

    def train(self, corpus):
        for sentence in corpus:
            tokens = ['<s>'] + sentence.split() + ['</s>']
            self.vocab.update(tokens)
            for i in range(len(tokens) - 1):
                self.bigram_counts[tokens[i]][tokens[i + 1]] += 1
                self.unigram_counts[tokens[i]] += 1
        self.vocab_size = len(self.vocab)

    def probability(self, word, context):
        bigram_count = self.bigram_counts[context][word]
        context_count = self.unigram_counts[context]
        # Suaviza√ß√£o Add-one
        return (bigram_count + 1) / (context_count + self.vocab_size)

    def generate(self, num_words=10):
        current_word = '<s>'
        generated = []
        for _ in range(num_words):
            words = list(self.bigram_counts[current_word].keys())
            probs = [self.probability(w, current_word) for w in words]
            total_prob = sum(probs)
            probs = [p / total_prob for p in probs]
            next_word = np.random.choice(words, p=probs)
            if next_word == '</s>':
                break
            generated.append(next_word)
            current_word = next_word
        return ' '.join(generated)

# Uso do modelo
corpus = ["I am Sam", "Sam I am", "I do not like green eggs and ham"]
model = BigramModel()
model.train(corpus)
print(model.generate())
```

Este c√≥digo implementa um modelo bigram com suaviza√ß√£o Add-one, demonstrando os princ√≠pios b√°sicos da Hip√≥tese de Markov em a√ß√£o [26].

#### Perguntas Te√≥ricas

1. **Complexidade Computacional**: Derive a complexidade computacional e espacial para o treinamento e infer√™ncia de um modelo n-gram gen√©rico. Considere o n√∫mero de poss√≠veis n-grams e o custo de armazenamento e busca.

2. **Impacto da Suaviza√ß√£o**: Analise teoricamente o impacto da suaviza√ß√£o Add-one na distribui√ß√£o de probabilidades de um modelo bigram. Discuta como ela afeta a probabilidade de n-grams observados e n√£o observados.

3. **Interpola√ß√£o Linear**: Demonstre matematicamente como a interpola√ß√£o linear de diferentes ordens de n-grams afeta a estimativa de probabilidade final. Mostre como os pesos $\lambda_i$ influenciam o equil√≠brio entre modelos de diferentes ordens.

## Conclus√£o

A Hip√≥tese de Markov, fundamental para os modelos n-gram, representa uma simplifica√ß√£o poderosa que permitiu avan√ßos significativos na modelagem estat√≠stica de linguagem. Ao aproximar a hist√≥ria de uma sequ√™ncia por um contexto limitado, torna-se trat√°vel o problema de estimar probabilidades de sequ√™ncias de palavras [27].

Embora os modelos baseados na Hip√≥tese de Markov apresentem limita√ß√µes, especialmente na captura de depend√™ncias de longo alcance, eles permanecem uma ferramenta valiosa no processamento de linguagem natural. Sua simplicidade e efici√™ncia computacional os tornam adequados para diversas aplica√ß√µes pr√°ticas [28].

√Ä medida que o campo avan√ßa, modelos mais sofisticados, como redes neurais recorrentes e Transformers, t√™m superado algumas das limita√ß√µes dos modelos n-gram tradicionais. No entanto, o entendimento dos princ√≠pios fundamentais da Hip√≥tese de Markov continua sendo crucial para o desenvolvimento e compreens√£o de modelos de linguagem mais avan√ßados [29].

## Perguntas Te√≥ricas Avan√ßadas

1. **Entropia de Processos Estoc√°sticos**: Derive a express√£o para a entropia de um processo estoc√°stico estacion√°rio e erg√≥dico baseado na Hip√≥tese de Markov e relacione-a com a perplexidade de um modelo n-gram.

2. **Comportamento Assint√≥tico**: Analise teoricamente o comportamento assint√≥tico da qualidade de um modelo n-gram √† medida que $n$ tende ao infinito, considerando um corpus de treinamento finito. Discuta a converg√™ncia das estimativas de probabilidade.

3. **Converg√™ncia de Modelos de Markov**: Desenvolva uma prova formal demonstrando que, para qualquer distribui√ß√£o de probabilidade real sobre sequ√™ncias infinitas, existe uma sequ√™ncia de modelos de Markov de ordem crescente que converge para essa distribui√ß√£o.

4. **Compara√ß√£o com Modelos de Aten√ß√£o**: Compare teoricamente a capacidade de modelos baseados na Hip√≥tese de Markov e modelos de aten√ß√£o (como Transformers) em capturar depend√™ncias de longo alcance. Forne√ßa uma an√°lise matem√°tica das limita√ß√µes fundamentais de cada abordagem.

5. **Extens√£o da Hip√≥tese de Markov**: Proponha e analise matematicamente uma extens√£o da Hip√≥tese de Markov que permita capturar depend√™ncias de alcance vari√°vel de forma eficiente, mantendo a tratabilidade computacional.

## Refer√™ncias

[1] "The intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words." *(Trecho de n-gram language models.pdf.md)*

[2] "The bigram model, for example, approximates the probability of a word given all the previous words $P(w_n \mid w_1^{n-1})$ by using only the conditional probability of the preceding word $P(w_n \mid w_{n-1})$." *(Trecho de n-gram language models.pdf.md)*

[3] "The assumption that the probability of a word depends only on the previous word is called a Markov assumption. Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past." *(Trecho de n-gram language models.pdf.md)*

[4] "We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks $n-1$ words into the past)." *(Trecho de n-gram language models.pdf.md)*

[5] "Let's see a general equation for this n-gram approximation to the conditional probability of the next word in a sequence. We'll use N here to mean the n-gram size, so $N = 2$ means bigrams and $N = 3$ means trigrams." *(Trecho de n-gram language models.pdf.md)*

[6] "P(w_n \mid w_1^{n-1}) \approx P(w_n \mid w_{n-N+1}^{n-1})" *(Trecho de n-gram language models.pdf.md)*

[7] "Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4: $P(w_1^n) \approx \prod_{k=1}^n P(w_k \mid w_{k-1})$" *(Trecho de n-gram language models.pdf.md)*

[8] "How do we estimate these bigram or n-gram probabilities? An intuitive way to estimate probabilities is called maximum likelihood estimation or MLE. We get the MLE estimate for the parameters of an n-gram model by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1." *(Trecho de n-gram language models.pdf.md)*

[9] "For example, to compute a particular bigram probability of a word $w_n$ given a previous word $w_{n-1}$, we'll compute the count of the bigram $C(w_{n-1} w_n)$ and normalize by the sum of all the bigrams that share the same first word $w_{n-1}$:" *(Trecho de n-gram language models.pdf.md)*

[10] "P(w_n \mid w_{n-1}) = \frac{C(w_{n-1} w_n)}{\sum_w C(w_{n-1} w)}" *(Trecho de n-gram language models.pdf.md)*

[11] "We can simplify this equation, since the sum of all bigram counts that start with a given word $w_{n-1}$ must be equal to the unigram count for that word $w_{n-1}$ (the reader should take a moment to be convinced of this): $P(w_n \mid w_{n-1}) = \frac{C(w_{n-1} w_n)}{C(w_{n-1})}$" *(Trecho de n-gram language models.pdf.md)*

[12] "For the general case of MLE n-gram parameter estimation: $P(w_n \mid w_{n-N+1}^{n-1}) = \frac{C(w_{n-N+1}^{n-1} w_n)}{C(w_{n-N+1}^{n-1})}$" *(Trecho de n-gram language models.pdf.md)*

[13] "Equation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the observed frequency of a particular sequence by the observed frequency of a prefix. This ratio is called a relative frequency." *(Trecho de n-gram language models.pdf.md)*

[14] "We said above that this use of relative frequencies is an estimate, called the maximum likelihood estimate or MLE, of the probability $P(w_n \mid w_{n-N+1}^{n-1})$." *(Trecho de n-gram language models.pdf.md)*

[15] "To compute the probability of a word sequence, we need to prefix it with $n-1$ start symbols $\langle s \rangle$." *(Trecho de n-gram language models.pdf.md)*

[16] "One of the biggest problems with the MLE estimates is the sparseness of the data. Even with very large corpora, we will have zero counts for a vast number of possible n-grams." *(Trecho de n-gram language models.pdf.md)*

[17] "This data sparsity causes problems when we try to compute probabilities, as we cannot have probabilities of zero for events that might occur." *(Trecho de n-gram language models.pdf.md)*

[18] "To address this issue, we use smoothing techniques to adjust the maximum likelihood estimates to produce more reliable probabilities." *(Trecho de n-gram language models.pdf.md)*

[19] "Interpolation is a method where we combine different order n-gram models by assigning weights to them." *(Trecho de n-gram language models.pdf.md)*

[20] "Backoff models back off to lower-order n-gram models when the higher-order n-gram has zero counts." *(Trecho de n-gram language models.pdf.md)*

[21] "Common smoothing techniques include Laplace (add-one), Good-Turing, and Kneser-Ney smoothing." *(Trecho de n-gram language models.pdf.md)*

[22] "Despite these smoothing techniques, n-gram models have limitations in capturing the complexities of natural language, particularly long-distance dependencies." *(Trecho de n-gram language models.pdf.md)*

[23] "Efficient data structures and algorithms are required to store and retrieve n-gram counts, especially for large corpora and higher-order n-grams." *(Trecho de n-gram language models.pdf.md)*

[24] "Handling unknown words is critical, often involving the use of an unknown word token and careful consideration of the vocabulary size." *(Trecho de n-gram language models.pdf.md)*

[25] "Implementation of smoothing and backoff requires careful programming to ensure that probabilities remain valid and the model performs well." *(Trecho de n-gram language models.pdf.md)*

[26] "This simple bigram model demonstrates how Markov assumptions enable us to build probabilistic models of language." *(Trecho de n-gram language models.pdf.md)*

[27] "The Markov assumption reduces the complexity of modeling language by limiting the dependencies between words." *(Trecho de n-gram language models.pdf.md)*

[28] "While more advanced models have surpassed n-gram models in performance, understanding n-grams is fundamental to the field of computational linguistics." *(Trecho de n-gram language models.pdf.md)*

[29] "Modern models like neural networks and Transformers build upon the foundations established by n-gram models and the Markov assumption." *(Trecho de n-gram language models.pdf.md)*