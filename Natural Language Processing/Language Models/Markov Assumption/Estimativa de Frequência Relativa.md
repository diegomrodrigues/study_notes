## Estimativa de Frequ√™ncia Relativa em Modelos de Linguagem: Desafios e Limita√ß√µes

<imagem: Um gr√°fico mostrando a curva de frequ√™ncia relativa de palavras em um corpus, com uma longa cauda de palavras raras>

### Introdu√ß√£o

A **estimativa de frequ√™ncia relativa** √© uma abordagem fundamental na modelagem probabil√≠stica da linguagem natural, servindo como alicerce para modelos de linguagem (Language Models, LMs). ==Este m√©todo intuitivo busca calcular a probabilidade de sequ√™ncias de tokens com base em suas ocorr√™ncias observadas em um corpus de treinamento [1]==. Embora conceitualmente simples e estatisticamente ==consistente como um estimador de m√°xima verossimilhan√ßa==, essa t√©cnica enfrenta desafios pr√°ticos significativos. ==Problemas relacionados √† **esparsidade de dados** e √† **alta vari√¢ncia** das estimativas comprometem a capacidade dos modelos de generalizar para sequ√™ncias n√£o observadas==, especialmente ao lidar com sequ√™ncias longas ou vocabul√°rios extensos [2].

### Conceitos Fundamentais

| Conceito                 | Explica√ß√£o                                                   |
| ------------------------ | ------------------------------------------------------------ |
| **Frequ√™ncia Relativa**  | A propor√ß√£o de vezes que uma sequ√™ncia espec√≠fica ocorre em rela√ß√£o ao total de sequ√™ncias observadas [3]. √â utilizada como estimativa da probabilidade dessa sequ√™ncia no modelo de linguagem. |
| **Esparsidade de Dados** | ==O fen√¥meno em que, devido ao vasto n√∫mero de poss√≠veis sequ√™ncias em linguagem natural, muitas sequ√™ncias gramaticalmente v√°lidas n√£o aparecem no corpus de treinamento, resultando em probabilidades estimadas iguais a zero para essas sequ√™ncias [4].== Isso dificulta a capacidade do modelo de atribuir probabilidades significativas a eventos n√£o observados. |
| **Vari√¢ncia**            | ==A medida da instabilidade nas estimativas de probabilidade devido ao tamanho limitado do corpus de treinamento, especialmente para eventos raros [5]==. Alta vari√¢ncia nas estimativas pode levar a desempenho inconsistente do modelo em dados n√£o vistos. |

> ‚ö†Ô∏è **Nota Importante**: ==A estimativa de frequ√™ncia relativa √© um estimador n√£o enviesado; no limite te√≥rico de dados infinitos, as estimativas convergem para as probabilidades verdadeiras==. Entretanto, na pr√°tica, com dados finitos, ==precisamos introduzir **vi√©s** nas estimativas para reduzir a vari√¢ncia e obter estimativas mais confi√°veis [6].==

### Formula√ß√£o Matem√°tica

A estimativa de frequ√™ncia relativa para uma sequ√™ncia de tokens $w = (w_1, w_2, ..., w_m)$ √© dada por [7]:

$$
p(w) = \frac{\text{count}(w)}{\text{count}(\text{todas as sequ√™ncias})}
$$

Para um exemplo espec√≠fico, considere a frase atribu√≠da a Picasso [8]:

$$
p(\text{"Computers are useless, they can only give you answers"}) = \frac{\text{count}(\text{"Computers are useless, they can only give you answers"})}{\text{count}(\text{todas as frases j√° ditas})}
$$

Embora esta formula√ß√£o seja matematicamente correta, apresenta desafios pr√°ticos significativos:

1. **Infinidade de Poss√≠veis Sequ√™ncias**: O denominador $\text{count}(\text{todas as sequ√™ncias})$ teoricamente inclui todas as sequ√™ncias poss√≠veis, que s√£o infinitas na linguagem natural devido √† possibilidade de sequ√™ncias de comprimento arbitr√°rio [9].

2. **Esparsidade Extrema**: ==Mesmo ao limitar o comprimento das sequ√™ncias a um valor m√°ximo $M$, o n√∫mero de poss√≠veis sequ√™ncias √© exponencial em rela√ß√£o ao tamanho do vocabul√°rio $V$, dado por $V^M$ [10].== Com um vocabul√°rio de tamanho $V = 10^5$ e $M = 20$, temos $10^{100}$ sequ√™ncias poss√≠veis, tornando impratic√°vel a observa√ß√£o de todas elas.

3. **Probabilidade Zero para Frases N√£o Observadas**: Sequ√™ncias gramaticalmente corretas, mas n√£o presentes no corpus de treinamento, recebem probabilidade zero, o que √© problem√°tico para a generaliza√ß√£o do modelo [11].

### Limita√ß√µes e Desafios

#### üëé Desvantagens

1. **Alta Vari√¢ncia**: As estimativas s√£o altamente inst√°veis para eventos raros devido √†s baixas contagens, levando a grande variabilidade nas probabilidades estimadas [12].

2. **Generaliza√ß√£o Pobre**: ==O modelo falha em atribuir probabilidades significativas a sequ√™ncias n√£o observadas no treinamento==, limitando sua capacidade de lidar com novos dados [13].

3. **Inefici√™ncia Computacional**: Requer armazenamento e processamento de um n√∫mero exponencial de sequ√™ncias, tornando o m√©todo invi√°vel em termos pr√°ticos [14].

#### An√°lise Te√≥rica

Para compreender melhor as limita√ß√µes, consideremos um vocabul√°rio de tamanho $V$ e um corpus de tamanho $M$. A probabilidade de observar uma sequ√™ncia espec√≠fica de comprimento $n$ √© aproximadamente $(1/V)^n$, assumindo distribui√ß√£o uniforme e independ√™ncia entre as palavras. Para um vocabul√°rio de $V = 10^5$ e uma sequ√™ncia de $n = 20$ tokens, temos:

$$
P(\text{sequ√™ncia espec√≠fica}) \approx \left(\frac{1}{10^5}\right)^{20} = 10^{-100}
$$

Essa probabilidade extremamente baixa ilustra a impossibilidade pr√°tica de observar todas as sequ√™ncias gramaticalmente v√°lidas em qualquer corpus de treinamento realista [15].

### Solu√ß√µes e Alternativas

Para mitigar os problemas associados √† estimativa de frequ√™ncia relativa pura, v√°rias t√©cnicas foram desenvolvidas que introduzem vi√©s controlado nas estimativas para reduzir a vari√¢ncia e melhorar a generaliza√ß√£o:

1. **Suaviza√ß√£o (Smoothing)**: ==T√©cnicas como Laplace (add-one) e Lidstone adicionam pseudo-contagens para eventos n√£o observados==, redistribuindo a massa de probabilidade de forma a evitar probabilidades zero [16].

2. **Backoff**: ==O modelo de backoff utiliza n-gramas de ordem inferior quando n-gramas de ordem superior n√£o s√£o observados==, permitindo aproveitar contextos menores para estimar probabilidades [17].

3. **Interpola√ß√£o**: ==Combina estimativas de diferentes ordens de n-gramas atrav√©s de uma m√©dia ponderada==, equilibrando o uso de contextos de tamanhos variados e reduzindo a vari√¢ncia das estimativas [18].

4. **Modelos Neurais de Linguagem**: Utilizam representa√ß√µes densas e cont√≠nuas para palavras e contextos, permitindo generalizar para sequ√™ncias n√£o observadas ao capturar semelhan√ßas sem√¢nticas entre palavras [19].

### Respostas √†s Perguntas Te√≥ricas

1. **Derive a express√£o para a vari√¢ncia da estimativa de frequ√™ncia relativa para um bigrama, assumindo um modelo de linguagem onde as palavras s√£o geradas independentemente com probabilidades fixas.**

   **Solu√ß√£o:**

   Vamos considerar um vocabul√°rio de tamanho $V$ e um corpus com $N$ palavras. As palavras s√£o geradas independentemente com probabilidades fixas $\{p_1, p_2, ..., p_V\}$, onde $\sum_{i=1}^{V} p_i = 1$.

   **Defini√ß√£o do Bigrama:**

   Seja um bigrama espec√≠fico $(w_i, w_j)$, onde $w_i$ e $w_j$ s√£o palavras do vocabul√°rio. A probabilidade te√≥rica deste bigrama √©:

   $$
   p_{ij} = p_i \cdot p_j
   $$

   **Estimativa de Frequ√™ncia Relativa:**

   A estimativa de frequ√™ncia relativa para $p_{ij}$ √© dada por:

   $$
   \hat{p}_{ij} = \frac{C_{ij}}{N - 1}
   $$

   Onde:

   - $C_{ij}$ √© o n√∫mero de ocorr√™ncias do bigrama $(w_i, w_j)$ no corpus.
   - $N - 1$ √© o n√∫mero total de bigramas poss√≠veis no corpus (porque cada bigrama envolve dois tokens consecutivos em uma sequ√™ncia de $N$ palavras).

   **Distribui√ß√£o de $C_{ij}$:**

   Como as palavras s√£o geradas independentemente, a ocorr√™ncia de cada bigrama √© independente, e $C_{ij}$ segue uma distribui√ß√£o binomial:

   $$
   C_{ij} \sim \text{Binomial}(n = N - 1, p = p_i \cdot p_j)
   $$

   **C√°lculo da Vari√¢ncia:**

   A vari√¢ncia da estimativa $\hat{p}_{ij}$ √© dada por:

   $$
   \text{Var}(\hat{p}_{ij}) = \text{Var}\left( \frac{C_{ij}}{N - 1} \right) = \frac{1}{(N - 1)^2} \cdot \text{Var}(C_{ij})
   $$

   Sabendo que a vari√¢ncia de uma vari√°vel binomial √©:

   $$
   \text{Var}(C_{ij}) = (N - 1) \cdot p_{ij} \cdot (1 - p_{ij})
   $$

   Substituindo na express√£o da vari√¢ncia:

   $$
   \text{Var}(\hat{p}_{ij}) = \frac{1}{(N - 1)^2} \cdot (N - 1) \cdot p_{ij} \cdot (1 - p_{ij}) = \frac{p_{ij} \cdot (1 - p_{ij})}{N - 1}
   $$

   **Resposta Final:**

   A vari√¢ncia da estimativa de frequ√™ncia relativa para o bigrama $(w_i, w_j)$ √©:

   $$
   \text{Var}(\hat{p}_{ij}) = \frac{p_i \cdot p_j \cdot (1 - p_i \cdot p_j)}{N - 1}
   $$

   ==Isso mostra que a vari√¢ncia diminui com o aumento do tamanho do corpus $(N)$ e depende das probabilidades individuais das palavras que comp√µem o bigrama.==

---

2. **Prove que, para qualquer corpus finito, existe um comprimento de sequ√™ncia $n$ tal que a probabilidade de observar qualquer sequ√™ncia espec√≠fica de comprimento $n$ √© menor que $\epsilon$, para qualquer $\epsilon > 0$ escolhido arbitrariamente.**

   **Solu√ß√£o:**

   **Hip√≥teses:**

   - O corpus √© finito e tem tamanho $N$ (n√∫mero total de tokens).
   - O vocabul√°rio tem tamanho finito $V$.
   - As palavras t√™m probabilidades positivas e somam 1.

   **Probabilidade de uma Sequ√™ncia Espec√≠fica:**

   Em um modelo de linguagem onde as palavras s√£o geradas independentemente, a probabilidade de uma sequ√™ncia espec√≠fica de comprimento $n$ √©:

   $$
   p_{\text{sequ√™ncia}} = \prod_{k=1}^{n} p_{w_k}
   $$

   Onde $p_{w_k}$ √© a probabilidade da palavra na posi√ß√£o $k$.

   **Estimativa do Limite Superior da Probabilidade:**

   - Seja $p_{\text{max}} = \max_{i} p_i$ a maior probabilidade dentre todas as palavras do vocabul√°rio.
   - Ent√£o, a probabilidade m√°xima de qualquer sequ√™ncia de comprimento $n$ √©:

     $$
     p_{\text{sequ√™ncia m√°xima}} = (p_{\text{max}})^n
     $$

   **Prova:**

   Queremos mostrar que para qualquer $\epsilon > 0$, existe um $n$ tal que:

   $$
   p_{\text{sequ√™ncia m√°xima}} = (p_{\text{max}})^n < \epsilon
   $$

   **Tomando Logaritmo:**

   - Aplicamos o logaritmo natural em ambos os lados:

     $$
     \ln(p_{\text{max}}^n) = n \ln(p_{\text{max}}) < \ln(\epsilon)
     $$

   - Como $0 < p_{\text{max}} < 1$, temos que $\ln(p_{\text{max}}) < 0$.

   **Resolvendo para $n$:**

   $$
   n > \frac{\ln(\epsilon)}{\ln(p_{\text{max}})}
   $$

   - Como $\ln(p_{\text{max}}) < 0$, o quociente $\frac{\ln(\epsilon)}{\ln(p_{\text{max}})}$ √© positivo para $\epsilon < 1$.

   **Conclus√£o:**

   - Para qualquer $\epsilon > 0$, podemos escolher:

     $$
     n = \left\lceil \frac{\ln(\epsilon)}{\ln(p_{\text{max}})} \right\rceil
     $$

     Onde $\lceil x \rceil$ denota o menor inteiro maior ou igual a $x$.

   - Portanto, existe um comprimento $n$ tal que a probabilidade de qualquer sequ√™ncia espec√≠fica de comprimento $n$ √© menor que $\epsilon$.

---

3. **Analise teoricamente como o tamanho do vocabul√°rio $V$ afeta a taxa de converg√™ncia da estimativa de frequ√™ncia relativa para a verdadeira distribui√ß√£o de probabilidade, assumindo um modelo de linguagem simples.**

   **Solu√ß√£o:**

   **Contexto:**

   - Em estimativas de frequ√™ncia relativa, estamos interessados em como a estimativa $\hat{p}_i$ para a probabilidade verdadeira $p_i$ converge √† medida que aumentamos o tamanho do corpus $N$.
   - A taxa de converg√™ncia √© influenciada pelo n√∫mero de par√¢metros a serem estimados (tamanho do vocabul√°rio $V$) e pelo n√∫mero de observa√ß√µes dispon√≠veis para cada par√¢metro.

   **Vari√¢ncia da Estimativa Unigrama:**

   - Para uma palavra $w_i$, a estimativa de frequ√™ncia relativa √©:

     $$
     \hat{p}_i = \frac{C_i}{N}
     $$

     Onde $C_i$ √© o n√∫mero de ocorr√™ncias de $w_i$ no corpus.

   - A vari√¢ncia da estimativa √©:

     $$
     \text{Var}(\hat{p}_i) = \frac{p_i (1 - p_i)}{N}
     $$

   **Impacto do Tamanho do Vocabul√°rio:**

   - √Ä medida que $V$ aumenta, para um corpus de tamanho fixo $N$, o n√∫mero m√©dio de observa√ß√µes por palavra diminui.
   - Assumindo distribui√ß√£o uniforme para simplificar ($p_i = 1/V$):

     - Esperan√ßa de $C_i$:

       $$
       E[C_i] = N \cdot \frac{1}{V}
       $$

     - Vari√¢ncia da estimativa:

       $$
       \text{Var}(\hat{p}_i) = \frac{(1/V) (1 - 1/V)}{N}
       $$

   - Com o aumento de $V$, $E[C_i]$ diminui, levando a estimativas menos confi√°veis.

   **Taxa de Converg√™ncia:**

   - A taxa de converg√™ncia da estimativa para a verdadeira probabilidade √© proporcional a $1/\sqrt{N}$ para cada par√¢metro.
   - Por√©m, o n√∫mero total de par√¢metros √© $V$, ent√£o o erro total pode ser visto como:

     $$
     \text{Erro Total} \propto V \cdot \frac{1}{\sqrt{N}}
     $$

   - Para manter o mesmo n√≠vel de erro ao aumentar $V$, seria necess√°rio aumentar $N$ proporcionalmente.

   **Conclus√£o:**

   - ==**Quanto maior o vocabul√°rio $V$, mais dados s√£o necess√°rios para que as estimativas de frequ√™ncia relativa convergem para as verdadeiras probabilidades.**==
   - O aumento em $V$ leva a um aumento no n√∫mero de par√¢metros a serem estimados, mas o n√∫mero de observa√ß√µes por par√¢metro diminui, retardando a taxa de converg√™ncia.
   - **Implica√ß√£o Pr√°tica:** Em modelos com vocabul√°rios extensos, t√©cnicas adicionais como suaviza√ß√£o ou modelos que compartilham par√¢metros (e.g., modelos neurais) s√£o necess√°rias para obter estimativas confi√°veis com conjuntos de dados de tamanho razo√°vel.

### Conclus√£o

A estimativa de frequ√™ncia relativa, embora seja um estimador de m√°xima verossimilhan√ßa e conceitualmente atraente, apresenta limita√ß√µes fundamentais quando aplicada a modelos de linguagem realistas. A esparsidade de dados e a alta vari√¢ncia das estimativas comprometem a capacidade do modelo de generalizar para sequ√™ncias n√£o observadas, um requisito essencial em tarefas de processamento de linguagem natural. Essas limita√ß√µes motivaram o desenvolvimento de t√©cnicas alternativas que introduzem vi√©s controlado nas estimativas para reduzir a vari√¢ncia, como a suaviza√ß√£o, o backoff e a interpola√ß√£o, bem como a ado√ß√£o de modelos neurais de linguagem que exploram representa√ß√µes cont√≠nuas e contextos mais amplos [20]. Compreender essas limita√ß√µes e as solu√ß√µes propostas √© crucial para o avan√ßo e aplica√ß√£o eficaz de modelos de linguagem em diversos dom√≠nios.

### Perguntas Te√≥ricas Avan√ßadas

1. **Derive a express√£o para o vi√©s e a vari√¢ncia da estimativa de frequ√™ncia relativa para um n-grama gen√©rico, em fun√ß√£o do tamanho do corpus e da ordem do n-grama. Compare analiticamente o trade-off entre vi√©s e vari√¢ncia para diferentes ordens de n-gramas.**

2. **Considerando um modelo de linguagem baseado em frequ√™ncia relativa, prove que a perplexidade no conjunto de treinamento √© sempre menor ou igual √† perplexidade em um conjunto de teste n√£o visto, e identifique as condi√ß√µes sob as quais a igualdade √© alcan√ßada.**

3. **Desenvolva uma prova formal mostrando que, para qualquer distribui√ß√£o de probabilidade de linguagem natural, existe um tamanho de corpus finito a partir do qual a estimativa de frequ√™ncia relativa de n-gramas supera em desempenho (em termos de perplexidade esperada) qualquer modelo de linguagem baseado em unigramas, independentemente do m√©todo de suaviza√ß√£o usado.**

4. **Analise teoricamente o impacto da Lei de Zipf na efic√°cia da estimativa de frequ√™ncia relativa para modelagem de linguagem. Derive uma express√£o para o n√∫mero esperado de n-gramas √∫nicos em fun√ß√£o do tamanho do corpus, assumindo que a frequ√™ncia das palavras segue exatamente a Lei de Zipf.**

5. **Prove que, para qualquer m√©todo de suaviza√ß√£o que distribui uma massa de probabilidade fixa para eventos n√£o observados, existe uma sequ√™ncia de palavras gramaticalmente correta cuja probabilidade sob o modelo suavizado √© menor do que sob a estimativa de frequ√™ncia relativa pura. Discuta as implica√ß√µes deste resultado para o design de t√©cnicas de suaviza√ß√£o.**

### Refer√™ncias

[1] "A simple approach to computing the probability of a sequence of tokens is to use a relative frequency estimate." *(Trecho de Language Models_143-162.pdf.md)*

[2] "Clearly, this estimator is very data-hungry, and suffers from high variance: even grammatical sentences will have probability zero if they have not occurred in the training data." *(Trecho de Language Models_143-162.pdf.md)*

[3] "One way to estimate the probability of this sentence is," *(Trecho de Language Models_143-162.pdf.md)*

[4] "But in practice, we are asking for accurate counts over an infinite number of events, since sequences of words can be arbitrarily long." *(Trecho de Language Models_143-162.pdf.md)*

[5] "This estimator is unbiased: in the theoretical limit of infinite data, the estimate will be correct. But in practice, we are asking for accurate counts over an infinite number of events, since sequences of words can be arbitrarily long." *(Trecho de Language Models_143-162.pdf.md)*

[6] "We therefore need to introduce bias to have a chance of making reliable estimates from finite training data." *(Trecho de Language Models_143-162.pdf.md)*

[7] "p(Computers are useless, they can only give you answers) = count(Computers are useless, they can only give you answers) / count(all sentences ever spoken)" *(Trecho de Language Models_143-162.pdf.md)*

[8] "Consider the quote, attributed to Picasso, 'computers are useless, they can only give you answers.'" *(Trecho de Language Models_143-162.pdf.md)*

[9] "But in practice, we are asking for accurate counts over an infinite number of events, since sequences of words can be arbitrarily long." *(Trecho de Language Models_143-162.pdf.md)*

[10] "Even with an aggressive upper bound of, say, M = 20 tokens in the sequence, the number of possible sequences is V^20, where V = |V|. A small vocabulary for English would have V = 10^5, so there are 10^100 possible sequences." *(Trecho de Language Models_143-162.pdf.md)*

[11] "Clearly, this estimator is very data-hungry, and suffers from high variance: even grammatical sentences will have probability zero if they have not occurred in the training data." *(Trecho de Language Models_143-162.pdf.md)*

[12] "This estimator is unbiased: in the theoretical limit of infinite data, the estimate will be correct. But in practice, we are asking for accurate counts over an infinite number of events, since sequences of words can be arbitrarily long." *(Trecho de Language Models_143-162.pdf.md)*

[13] "We therefore need to introduce bias to have a chance of making reliable estimates from finite training data." *(Trecho de Language Models_143-162.pdf.md)*

[14] "Even with an aggressive upper bound of, say, M = 20 tokens in the sequence, the number of possible sequences is V^20, where V = |V|. A small vocabulary for English would have V = 10^5, so there are 10^100 possible sequences." *(Trecho de Language Models_143-162.pdf.md)*

[15] "A small vocabulary for English would have V = 10^5, so there are 10^100 possible sequences." *(Trecho de Language Models_143-162.pdf.md)*

[16] "The language models that follow in this chapter introduce bias in various ways." *(Trecho de Language Models_143-162.pdf.md)*

[17] "We begin with n-gram language models, which compute the probability of a sequence as the product of probabilities of subsequences." *(Trecho de Language Models_143-162.pdf.md)*

[18] "Instead of choosing a single n for the size of the n-gram, we can take the weighted average across several n-gram probabilities." *(Trecho de Language Models_143-162.pdf.md)*

[19] "N-gram language models have been largely supplanted by neural networks. These models do not make the n-gram assumption of restricted context; indeed, they can incorporate arbitrarily distant contextual information, while remaining computationally and statistically tractable." *(Trecho de Language Models_143-162.pdf.md)*

[20] "The language models that follow in this chapter introduce bias in various ways." *(Trecho de Language Models_143-162.pdf.md)*