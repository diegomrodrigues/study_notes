## A Regra da Cadeia de Probabilidade em Modelos de Linguagem

<imagem: Um diagrama mostrando uma sequ√™ncia de palavras conectadas por setas, representando as depend√™ncias probabil√≠sticas da regra da cadeia>

### Introdu√ß√£o

A **regra da cadeia de probabilidade** √© um conceito fundamental na modelagem de linguagem probabil√≠stica, ==permitindo decompor a probabilidade conjunta de uma sequ√™ncia de palavras em um produto de probabilidades condicionais mais simples [1][2]==. Esta abordagem √© crucial para calcular e estimar as probabilidades de sequ√™ncias de tokens em linguagem natural, ==formando a base te√≥rica para muitos modelos de linguagem modernos.==

Em processamento de linguagem natural, ==√© essencial modelar a probabilidade de sequ√™ncias de palavras para tarefas como previs√£o de palavras, gera√ß√£o de texto e reconhecimento de fala==. A regra da cadeia fornece um m√©todo sistem√°tico para este prop√≥sito, permitindo que modelos computem a probabilidade de uma sequ√™ncia inteira a partir das probabilidades condicionais das palavras individuais, dado seu contexto precedente.

### Conceitos Fundamentais

| Conceito                      | Explica√ß√£o                                                   |
| ----------------------------- | ------------------------------------------------------------ |
| **Regra da Cadeia**           | M√©todo para fatorar a probabilidade conjunta de uma sequ√™ncia em um produto de probabilidades condicionais [2]. |
| **Probabilidade Condicional** | A probabilidade de uma palavra dado o contexto das palavras anteriores [3]. |
| **Modelo N-gram**             | Aproxima√ß√£o que considera apenas as N-1 palavras anteriores como contexto, simplificando o c√°lculo das probabilidades [4]. |

> ‚ö†Ô∏è **Nota Importante**: Embora a regra da cadeia permita modelar sequ√™ncias de comprimento arbitr√°rio, na pr√°tica, limita√ß√µes computacionais e de disponibilidade de dados levam a aproxima√ß√µes como modelos N-gram [5].

### Formula√ß√£o Matem√°tica da Regra da Cadeia

A regra da cadeia para uma sequ√™ncia de palavras $w = (w_1, w_2, \dots, w_M)$ √© expressa matematicamente como [6]:

$$
p(w) = p(w_1, w_2, \dots, w_M) = p(w_1) \times p(w_2 \mid w_1) \times p(w_3 \mid w_1, w_2) \times \dots \times p(w_M \mid w_1, w_2, \dots, w_{M-1})
$$

Onde:

- $p(w)$ √© a probabilidade conjunta da sequ√™ncia completa.
- $p(w_m \mid w_1, w_2, \dots, w_{m-1})$ ==√© a probabilidade condicional da m-√©sima palavra dado todo o contexto anterior.==

Esta formula√ß√£o √© derivada diretamente das defini√ß√µes b√°sicas de probabilidade condicional e permite decompor o problema complexo de estimar a probabilidade de uma sequ√™ncia inteira em subproblemas mais gerenci√°veis de estimar probabilidades condicionais [7].

No entanto, calcular estas probabilidades condicionais para sequ√™ncias longas √© impratic√°vel devido ao n√∫mero exponencial de poss√≠veis contextos que precisam ser considerados, especialmente considerando o tamanho do vocabul√°rio em linguagens naturais.

### Aplica√ß√£o em Modelos de Linguagem N-gram

Para tornar o problema trat√°vel, os modelos N-gram fazem uma aproxima√ß√£o crucial da regra da cadeia, ==limitando o contexto considerado √†s N-1 palavras anteriores [8]:==

$$
p(w_m \mid w_1, w_2, \dots, w_{m-1}) \approx p(w_m \mid w_{m-n+1}, w_{m-n+2}, \dots, w_{m-1})
$$

Esta aproxima√ß√£o resulta na seguinte f√≥rmula para a probabilidade aproximada de uma sequ√™ncia [9]:

$$
p(w_1, w_2, \dots, w_M) \approx \prod_{m=1}^M p(w_m \mid w_{m-n+1}, \dots, w_{m-1})
$$

> üí° **Insight**: Esta aproxima√ß√£o reduz drasticamente o n√∫mero de par√¢metros a serem estimados, de $V^M$ para $V^n$, onde $V$ √© o tamanho do vocabul√°rio, tornando o modelo computacionalmente vi√°vel. No entanto, isto introduz um vi√©s ao ignorar depend√™ncias de longo alcance [10].

### Estima√ß√£o de Par√¢metros

Para estimar as probabilidades condicionais em modelos N-gram, geralmente se utiliza a estimativa de frequ√™ncia relativa, calculada a partir de um corpus de treinamento [11]:

$$
p(w_m \mid w_{m-2}, w_{m-1}) = \frac{\text{count}(w_{m-2}, w_{m-1}, w_m)}{\sum_{w'} \text{count}(w_{m-2}, w_{m-1}, w')}
$$

Onde:

- $\text{count}(w_{m-2}, w_{m-1}, w_m)$ √© a contagem de ocorr√™ncia do trigram $(w_{m-2}, w_{m-1}, w_m)$ no corpus.
- O denominador soma as contagens de todos os trigrams que compartilham o mesmo contexto $(w_{m-2}, w_{m-1})$.

No entanto, esta abordagem pode levar a problemas de esparsidade de dados, especialmente para N-grams de ordem superior. Muitas sequ√™ncias poss√≠veis nunca aparecem no corpus de treinamento, resultando em probabilidades estimadas como zero [12].

Para mitigar este problema, t√©cnicas de suaviza√ß√£o (smoothing) s√£o empregadas, como Laplace smoothing, Good-Turing discounting e m√©todos de interpola√ß√£o, que ajustam as estimativas de probabilidade para dar algum peso a eventos n√£o observados.

### Desafios e Limita√ß√µes

#### üëé Desvantagens da Aproxima√ß√£o N-gram

- **Esparsidade de Dados**: √Ä medida que o valor de $n$ aumenta, o n√∫mero de poss√≠veis N-grams cresce exponencialmente, e muitos deles n√£o s√£o observados no conjunto de treinamento [13].
- **Perda de Depend√™ncias de Longo Alcance**: Os modelos N-gram n√£o capturam depend√™ncias al√©m das N-1 palavras anteriores, ignorando informa√ß√µes que podem ser relevantes para a previs√£o da pr√≥xima palavra [14].

Estas limita√ß√µes refletem um trade-off entre a capacidade do modelo de capturar depend√™ncias contextuais e a viabilidade computacional. Modelos com maior valor de $n$ podem teoricamente capturar mais contexto, mas exigem mais dados para estimativas confi√°veis e maior capacidade computacional.

Para abordar estas limita√ß√µes, t√©cnicas avan√ßadas, como modelos baseados em √°rvores e modelos de linguagem neurais, foram desenvolvidas [15].

### Modelos de Linguagem Neurais e a Regra da Cadeia

Modelos de linguagem baseados em redes neurais, como Redes Neurais Recorrentes (RNNs) e Transformers, estendem a aplica√ß√£o da regra da cadeia ao incorporar mecanismos que podem teoricamente capturar depend√™ncias de longo alcance sem a necessidade de limitar o contexto a N-1 palavras [16].

#### Redes Neurais Recorrentes (RNNs)

Em um modelo RNN, o estado oculto √© atualizado iterativamente, permitindo que informa√ß√µes de todo o contexto anterior influenciem a previs√£o da pr√≥xima palavra. A probabilidade condicional √© calculada como:

$$
p(w_{m+1} \mid w_1, w_2, \dots, w_m) = \text{Softmax}( \mathbf{W} \cdot \mathbf{h}_m + \mathbf{b} )
$$

Onde:

- $\mathbf{h}_m$ √© o estado oculto no passo $m$, que depende de $\mathbf{h}_{m-1}$ e da entrada atual $\mathbf{x}_m$.
- $\mathbf{W}$ e $\mathbf{b}$ s√£o par√¢metros do modelo.
- $\text{Softmax}$ produz uma distribui√ß√£o de probabilidade sobre o vocabul√°rio.

Este mecanismo permite que o modelo aprenda representa√ß√µes contextuais ricas, potencialmente capturando depend√™ncias de longo alcance [17].

#### Transformers

Modelos baseados em Transformers utilizam mecanismos de aten√ß√£o que permitem acesso direto a todos os estados anteriores, superando limita√ß√µes das RNNs em capturar depend√™ncias de longo prazo devido a problemas de gradientes. A probabilidade condicional √© calculada considerando todas as posi√ß√µes anteriores na sequ√™ncia atrav√©s de mecanismos de autoaten√ß√£o.

Embora os modelos neurais sejam mais poderosos em capturar depend√™ncias complexas, eles ainda se baseiam na regra da cadeia para decompor a probabilidade conjunta em probabilidades condicionais.

### Conclus√£o

A regra da cadeia de probabilidade √© um princ√≠pio central na modelagem de linguagem, permitindo a decomposi√ß√£o da probabilidade conjunta de uma sequ√™ncia em probabilidades condicionais. Esta abordagem fundamenta desde modelos N-gram cl√°ssicos at√© arquiteturas neurais avan√ßadas, como RNNs e Transformers [19].

Apesar das limita√ß√µes pr√°ticas que levaram ao desenvolvimento de modelos mais sofisticados, a regra da cadeia continua sendo fundamental para nossa compreens√£o e abordagem da modelagem probabil√≠stica de sequ√™ncias de texto. Compreender suas vantagens e trade-offs √© essencial para desenvolver modelos eficazes em processamento de linguagem natural.

### Perguntas Te√≥ricas

1. **Perplexidade e Log-Verossimilhan√ßa**: Derive a f√≥rmula da perplexidade de um modelo de linguagem baseado na regra da cadeia e explique como ela se relaciona com a log-verossimilhan√ßa.

   *A perplexidade √© uma medida de qu√£o bem um modelo de linguagem prev√™ uma amostra. √â definida como a exponencial da entropia cruzada m√©dia entre a distribui√ß√£o real e a do modelo. Para uma sequ√™ncia de teste $w_1, w_2, \dots, w_N$, a perplexidade PP √© dada por:*

   $$
   PP = \exp\left( -\frac{1}{N} \sum_{i=1}^{N} \log p(w_i \mid w_1, \dots, w_{i-1}) \right)
   $$

   *Isto est√° diretamente relacionado √† log-verossimilhan√ßa, que √© a soma dos logaritmos das probabilidades condicionais preditas pelo modelo.*

2. **Subestima√ß√£o de Depend√™ncias de Longo Alcance**: Demonstre matematicamente por que a aproxima√ß√£o N-gram da regra da cadeia pode levar a uma subestima√ß√£o de depend√™ncias de longo alcance em sequ√™ncias de texto.

   *Como os modelos N-gram consideram apenas as N-1 palavras anteriores, eles assumem independ√™ncia condicional das palavras al√©m deste contexto. Isto significa que qualquer depend√™ncia estat√≠stica al√©m deste escopo √© ignorada, potencialmente subestimando a probabilidade de sequ√™ncias que dependem de contexto distante.*

3. **Incorpora√ß√£o de Contexto Bidirecional**: Como a regra da cadeia poderia ser modificada para incorporar informa√ß√µes bidirecionais do contexto, e quais seriam as implica√ß√µes te√≥ricas dessa modifica√ß√£o?

   *Uma modifica√ß√£o poss√≠vel √© considerar modelos que condicionam n√£o apenas no passado, mas tamb√©m no futuro. No entanto, isto viola a causalidade e n√£o √© adequado para tarefas de gera√ß√£o de texto. Modelos como Conditional Random Fields (CRFs) e modelos baseados em redes neurais bidirecionais podem incorporar contexto bidirecional para tarefas de etiquetagem e classifica√ß√£o, mas n√£o para previs√£o causal.*

### Perguntas Te√≥ricas Avan√ßadas

1. **Complexidade Computacional**: Derive a complexidade computacional e espacial de um modelo de linguagem baseado na regra da cadeia sem aproxima√ß√µes, e compare com a complexidade de um modelo N-gram e um modelo RNN.

   *Sem aproxima√ß√µes, o n√∫mero de par√¢metros necess√°rios para modelar as probabilidades condicionais √© da ordem de $V^M$, onde $V$ √© o tamanho do vocabul√°rio e $M$ √© o comprimento m√°ximo da sequ√™ncia, o que √© computacionalmente intrat√°vel. Em contraste, modelos N-gram requerem $V^n$ par√¢metros, e modelos RNN utilizam um n√∫mero fixo de par√¢metros independentes do tamanho do contexto.*

2. **Incorpora√ß√£o de Lookahead**: Proponha e analise matematicamente uma modifica√ß√£o da regra da cadeia que incorpore informa√ß√µes futuras (lookahead) de forma eficiente, mantendo a causalidade do modelo.

   *Incorporar informa√ß√µes futuras enquanto mant√©m a causalidade √© um desafio. Uma abordagem √© utilizar modelos de encoders-decoder com aten√ß√£o, onde o encoder processa a sequ√™ncia inteira e o decoder gera a sequ√™ncia passo a passo, mas isso n√£o √© causal no sentido estrito. Alternativamente, pode-se usar modelos que preveem m√∫ltiplos passos √† frente, mas isto muda a natureza do problema.*

3. **Modelagem de M√∫ltiplos Passos Futuros**: Demonstre como a regra da cadeia pode ser estendida para modelar n√£o apenas a pr√≥xima palavra, mas distribui√ß√µes sobre m√∫ltiplos passos futuros, e discuta as implica√ß√µes te√≥ricas e pr√°ticas dessa extens√£o.

   *A regra da cadeia pode ser estendida para considerar distribui√ß√µes conjuntas de m√∫ltiplas palavras futuras. No entanto, isto aumenta a complexidade computacional exponencialmente. Em pr√°tica, modelos como Transformers podem gerar distribui√ß√µes para m√∫ltiplos tokens simultaneamente durante o treinamento, mas a gera√ß√£o ainda √© feita token por token para manter a consist√™ncia.*

4. **Ambiguidades Lexicais e Sint√°ticas**: Analise teoricamente como a regra da cadeia poderia ser adaptada para lidar com ambiguidades lexicais e sint√°ticas em linguagens naturais, propondo uma formula√ß√£o matem√°tica que incorpore essas incertezas.

   *Uma abordagem √© introduzir vari√°veis latentes que modelam estados ocultos, como categorias sint√°ticas ou significados lexicais, resultando em modelos como Modelos Ocultos de Markov (HMMs) ou Gram√°ticas Livres de Contexto Probabil√≠sticas (PCFGs). A regra da cadeia √© ent√£o aplicada √†s sequ√™ncias observ√°veis e √†s vari√°veis latentes, incorporando as incertezas inerentes.*

5. **Capacidade de Aproxima√ß√£o de Transformers**: Desenvolva uma prova formal mostrando que, sob certas condi√ß√µes, um modelo de linguagem baseado em aten√ß√£o (como os Transformers) pode aproximar arbitrariamente bem a verdadeira distribui√ß√£o de probabilidade conjunta definida pela regra da cadeia.

   *Esta demonstra√ß√£o envolveria mostrar que, dado poder computacional e dados suficientes, os mecanismos de aten√ß√£o nos Transformers podem modelar qualquer fun√ß√£o mensur√°vel das sequ√™ncias de entrada, incluindo as depend√™ncias definidas pela distribui√ß√£o conjunta original. Isto se baseia na capacidade universal de aproxima√ß√£o das redes neurais profundas.*

### Refer√™ncias

[1] "In probabilistic classification, the problem is to compute the probability of a label, conditioned on the text. Let's now consider the inverse problem: computing the probability of text itself." *(Trecho de Language Models_143-162.pdf.md)*

[2] "The chain rule (see ¬ß A.2): $p(w) = p(w_1, w_2, \dots, w_M) = p(w_1) \times p(w_2 \mid w_1) \times p(w_3 \mid w_1, w_2) \times \dots \times p(w_M \mid w_1, w_2, \dots, w_{M-1})$" *(Trecho de Language Models_143-162.pdf.md)*

[3] "Each element in the product is the probability of a word given all its predecessors." *(Trecho de Language Models_143-162.pdf.md)*

[4] "n-gram models make a crucial simplifying approximation: they condition on only the past n ‚àí 1 words." *(Trecho de Language Models_143-162.pdf.md)*

[5] "To solve this problem, n-gram models make a crucial simplifying approximation: they condition on only the past n ‚àí 1 words." *(Trecho de Language Models_143-162.pdf.md)*

[6] "p(w) = p(w_1, w_2, ..., w_M) = p(w_1) √ó p(w_2 | w_1) √ó p(w_3 | w_1, w_2) √ó ... √ó p(w_M | w_1, w_2, ..., w_{M‚àí1})" *(Trecho de Language Models_143-162.pdf.md)*

[7] "Each element in the product is the probability of a word given all its predecessors. We can think of this as a word prediction task: given the context Computers are, we want to compute a probability over the next token." *(Trecho de Language Models_143-162.pdf.md)*

[8] "n-gram models make a crucial simplifying approximation: they condition on only the past n ‚àí 1 words. $p(w_m \mid w_{m-1}, ..., w_1) \approx p(w_m \mid w_{m-n+1}, ..., w_{m-1})$" *(Trecho de Language Models_143-162.pdf.md)*

[9] "This means that the probability of a sentence w can be approximated as $p(w_1, ..., w_M) \approx \prod_{m=1}^M p(w_m \mid w_{m-n+1}, ..., w_{m-1})$" *(Trecho de Language Models_143-162.pdf.md)*

[10] "This model requires estimating and storing the probability of only $V^n$ events, which is exponential in the order of the n-gram, and not $V^M$, which is exponential in the length of the sentence." *(Trecho de Language Models_143-162.pdf.md)*

[11] "The n-gram probabilities can be computed by relative frequency estimation, $p(w_m \mid w_{m-1}, w_{m-2}) = \frac{\text{count}(w_{m-2}, w_{m-1}, w_m)}{\sum_{w'} \text{count}(w_{m-2}, w_{m-1}, w')}$" *(Trecho de Language Models_143-162.pdf.md)*

[12] "Language is full of long-range dependencies that we cannot capture because n is too small; at the same time, language datasets are full of rare phenomena, whose probabilities we fail to estimate accurately because n is too large." *(Trecho de Language Models_143-162.pdf.md)*

[13] "Limited data is a persistent problem in estimating language models." *(Trecho de Language Models_143-162.pdf.md)*

[14] "In each example, the words written in bold depend on each other: the likelihood of their depends on knowing that gorillas is plural, and the likelihood of crashed depends on knowing that the subject is a computer. If the n-grams are not big enough to capture this context, then the resulting language model would offer probabilities that are too low for these sentences" *(Trecho de Language Models_143-162.pdf.md)*

[15] "It is therefore necessary to add additional inductive biases to n-gram language models. This section covers some of the most intuitive and common approaches, but there are many more (see Chen and Goodman, 1999)." *(Trecho de Language Models_143-162.pdf.md)*

[16] "N-gram language models have been largely supplanted by neural networks. These models do not make the n-gram assumption of restricted context; indeed, they can incorporate arbitrarily distant contextual information, while remaining computationally and statistically tractable." *(Trecho de Language Models_143-162.pdf.md)*

[17] "$p(w_{m+1} | w_1, w_2, ..., w_m) = \text{SoftMax}([\beta_1 \cdot v_u, \beta_2 \cdot v_u, ..., \beta_V \cdot v_u])$" *(Trecho de Language Models_143-162.pdf.md)*

[18] "Using the Pytorch library, train an LSTM language model from the Wikitext training corpus. After each epoch of training, compute its perplexity on the Wikitext validation corpus. Stop training when the perplexity stops improving." *(Trecho de Language Models_143-162.pdf.md)*

[19] "The first insight behind neural language models is to treat word prediction as a discriminative learning task. The goal is to compute the probability $p(w \mid u)$, where $w \in V$ is a word, and $u$ is the context, which depends on the previous words." *(Trecho de Language Models_143-162.pdf.md)*