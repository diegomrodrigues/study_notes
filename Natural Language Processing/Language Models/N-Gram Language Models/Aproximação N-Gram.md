# Aproxima√ß√£o N-Gram em Modelos de Linguagem

<imagem: Um diagrama mostrando uma sequ√™ncia de palavras com janelas deslizantes de tamanho n representando n-gramas>

## Introdu√ß√£o

A **aproxima√ß√£o n-gram** √© uma t√©cnica fundamental em modelagem de linguagem que permite calcular a probabilidade de sequ√™ncias de palavras de forma computacionalmente trat√°vel [1]. Ela aborda o problema da explos√£o combinat√≥ria que ocorre ao tentar modelar diretamente a probabilidade conjunta de longas sequ√™ncias de palavras, tornando vi√°vel a estima√ß√£o de probabilidades em corpora finitos.

## Conceitos Fundamentais

| Conceito                   | Explica√ß√£o                                                   |
| -------------------------- | ------------------------------------------------------------ |
| **N-gram**                 | Subsequ√™ncia cont√≠gua de n itens (geralmente palavras) em uma dada sequ√™ncia de texto [2]. |
| **Aproxima√ß√£o Markoviana** | Suposi√ß√£o de que a probabilidade de uma palavra depende apenas das n-1 palavras anteriores [3]. |
| **Cadeia de Markov**       | Processo estoc√°stico onde a probabilidade de cada evento depende apenas do estado imediatamente anterior [4]. |

> ‚ö†Ô∏è **Nota Importante**: A aproxima√ß√£o n-gram introduz um vi√©s no modelo, mas √© necess√°ria para tornar o problema trat√°vel computacionalmente com dados finitos [5].

## Formula√ß√£o Matem√°tica

A aproxima√ß√£o n-gram baseia-se na aplica√ß√£o da regra da cadeia de probabilidade e na introdu√ß√£o de uma suposi√ß√£o simplificadora [6]:

1. **Regra da cadeia**:

   $$p(w) = p(w_1, w_2, \ldots, w_M) = p(w_1) \times p(w_2 \mid w_1) \times p(w_3 \mid w_1, w_2) \times \ldots \times p(w_M \mid w_1, w_2, \ldots, w_{M-1})$$

2. **Aproxima√ß√£o n-gram**:

   $$p(w_m \mid w_1, w_2, \ldots, w_{m-1}) \approx p(w_m \mid w_{m-n+1}, \ldots, w_{m-1})$$

Combinando essas duas ideias, obtemos a formula√ß√£o da aproxima√ß√£o n-gram [7]:

$$p(w_1, w_2, \ldots, w_M) \approx \prod_{m=1}^M p(w_m \mid w_{m-n+1}, \ldots, w_{m-1})$$

==Esta aproxima√ß√£o reduz drasticamente o n√∫mero de par√¢metros a serem estimados, de $V^M$ para $V^n$, onde $V$ √© o tamanho do vocabul√°rio [8].==

### Exemplo: Modelo Bigram (n=2)

Para ilustrar, considere a aproxima√ß√£o bigram para a frase "I like black coffee" [9]:

$$p(\text{I like black coffee}) = p(\text{I} \mid \langle s \rangle) \times p(\text{like} \mid \text{I}) \times p(\text{black} \mid \text{like}) \times p(\text{coffee} \mid \text{black}) \times p(\langle /s \rangle \mid \text{coffee})$$

Onde $\langle s \rangle$ e $\langle /s \rangle$ s√£o s√≠mbolos especiais que representam o in√≠cio e o fim da frase, respectivamente.

### Estima√ß√£o de Probabilidades

As probabilidades dos n-gramas s√£o tipicamente estimadas usando contagens relativas [10]:

$$p(w_m \mid w_{m-1}, \ldots, w_{m-n+1}) = \frac{\text{count}(w_{m-n+1}, \ldots, w_{m-1}, w_m)}{\sum_{w'} \text{count}(w_{m-n+1}, \ldots, w_{m-1}, w')}$$

Essa abordagem assume que a frequ√™ncia de ocorr√™ncia no corpus √© uma estimativa confi√°vel da probabilidade real.

## Discuss√£o Te√≥rica

### Deriva√ß√£o da Aproxima√ß√£o N-Gram

A aproxima√ß√£o n-gram √© derivada da necessidade de simplificar o c√°lculo da probabilidade conjunta de uma sequ√™ncia de palavras. ==Pela regra da cadeia de probabilidade, a probabilidade conjunta pode ser decomposta em probabilidades condicionais [6]:==

$$p(w_1, w_2, \ldots, w_M) = \prod_{m=1}^M p(w_m \mid w_1, w_2, \ldots, w_{m-1})$$

No entanto, ==calcular essas probabilidades condicionais √© impratic√°vel devido ao n√∫mero exponencial de poss√≠veis hist√≥ricos== Para tornar o problema trat√°vel, ==assumimos que cada palavra depende apenas das n-1 palavras anteriores:==

$$p(w_m \mid w_1, w_2, \ldots, w_{m-1}) \approx p(w_m \mid w_{m-n+1}, \ldots, w_{m-1})$$

Essa suposi√ß√£o simplifica o modelo, reduzindo o contexto considerado e permitindo a estima√ß√£o das probabilidades com um conjunto finito de dados.

### Trade-off entre Vi√©s e Vari√¢ncia

A escolha do valor de $n$ afeta o equil√≠brio entre vi√©s e vari√¢ncia no modelo n-gram:

- **Vi√©s**: Com $n$ pequeno, o modelo √© mais simples e pode n√£o capturar todas as depend√™ncias lingu√≠sticas, resultando em alto vi√©s.
- **Vari√¢ncia**: Com $n$ grande, o modelo √© mais complexo, podendo ajustar-se muito bem aos dados de treinamento e generalizar mal para novos dados, resultando em alta vari√¢ncia.

Matematicamente, conforme $n$ aumenta, o n√∫mero de par√¢metros $V^n$ cresce exponencialmente, exigindo mais dados para estimar as probabilidades com confian√ßa [16].

### Redu√ß√£o da Complexidade Computacional

Sem a aproxima√ß√£o n-gram, a complexidade computacional seria $O(V^M)$, onde $M$ √© o comprimento da sequ√™ncia. Com a aproxima√ß√£o n-gram, a complexidade √© reduzida para $O(V^n)$, tornando o problema computacionalmente vi√°vel mesmo para grandes vocabul√°rios [8].

## Vantagens e Desvantagens

| üëç **Vantagens**                                              | üëé **Desvantagens**                                           |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Redu√ß√£o de Complexidade**: Diminui drasticamente o n√∫mero de par√¢metros a serem estimados [11]. | **Perda de Depend√™ncias de Longo Alcance**: Incapaz de capturar rela√ß√µes que ultrapassam o contexto limitado [12]. |
| **Efici√™ncia Computacional**: Permite estima√ß√£o eficiente com dados finitos [13]. | **Probabilidade Zero para Sequ√™ncias V√°lidas**: Pode atribuir probabilidade zero a sequ√™ncias n√£o observadas [14]. |
| **Captura Padr√µes Locais**: Eficaz na modelagem de depend√™ncias locais [15]. | **Sensibilidade √† Escolha de n**: Valor inadequado de $n$ pode levar a overfitting ou underfitting [16]. |

## T√©cnicas de Suaviza√ß√£o

Para lidar com n-gramas n√£o observados e evitar probabilidades zero, v√°rias t√©cnicas de suaviza√ß√£o s√£o empregadas [17]:

1. **Suaviza√ß√£o de Lidstone**: Adiciona um pseudo-contagem $\alpha$ a todas as contagens [18]:

   $$p_{\text{smooth}}(w_m \mid w_{m-1}) = \frac{\text{count}(w_{m-1}, w_m) + \alpha}{\sum_{w' \in V} \text{count}(w_{m-1}, w') + V\alpha}$$

2. **Descontagem Absoluta**: Subtrai um valor fixo $d$ de todas as contagens n√£o nulas, redistribuindo a probabilidade [19].

3. **Backoff de Katz**: Utiliza modelos de ordem inferior quando as contagens de ordem superior s√£o insuficientes [20].

4. **Suaviza√ß√£o de Kneser-Ney**: Ajusta a distribui√ß√£o considerando a "diversidade" de contextos em que uma palavra aparece, sendo eficaz para n-gramas raros [21].

> üí° **Destaque**: A suaviza√ß√£o de Kneser-Ney √© considerada a t√©cnica de ponta para modelos n-gram, com embasamento te√≥rico robusto em estat√≠stica n√£o-param√©trica [22].

## Implementa√ß√£o em Python

Exemplo de implementa√ß√£o de um modelo bigram com suaviza√ß√£o de Lidstone usando Python e NLTK:

```python
import nltk
from collections import defaultdict

class BigramModel:
    def __init__(self, alpha=0.1):
        self.bigram_counts = defaultdict(lambda: defaultdict(int))
        self.unigram_counts = defaultdict(int)
        self.vocab = set()
        self.alpha = alpha
    
    def train(self, corpus):
        for sentence in corpus:
            tokens = ['<s>'] + nltk.word_tokenize(sentence.lower()) + ['</s>']
            self.vocab.update(tokens)
            for i in range(len(tokens)-1):
                self.bigram_counts[tokens[i]][tokens[i+1]] += 1
                self.unigram_counts[tokens[i]] += 1
        self.vocab_size = len(self.vocab)
    
    def probability(self, word, context):
        numerator = self.bigram_counts[context][word] + self.alpha
        denominator = self.unigram_counts[context] + self.alpha * self.vocab_size
        return numerator / denominator

# Uso
corpus = ["I like black coffee", "I love dark coffee"]
model = BigramModel()
model.train(corpus)
print(model.probability("coffee", "black"))
```

Este c√≥digo demonstra os conceitos b√°sicos de um modelo bigram com suaviza√ß√£o de Lidstone, conforme descrito [23].

## Conclus√£o

A aproxima√ß√£o n-gram √© uma t√©cnica essencial em modelagem de linguagem que permite lidar com a complexidade de estimar probabilidades de sequ√™ncias de palavras [24]. Embora tenha limita√ß√µes na captura de depend√™ncias de longo alcance, sua simplicidade e efic√°cia a tornam uma ferramenta valiosa, servindo de base para t√©cnicas mais avan√ßadas, como modelos neurais de linguagem [25].

## Perguntas Te√≥ricas Avan√ßadas

1. **Perplexidade e Entropia Cruzada**: Derive a f√≥rmula para a perplexidade de um modelo n-gram e explique como ela se relaciona com a entropia cruzada e a verossimilhan√ßa dos dados.

2. **Compara√ß√£o com Modelos Neurais**: Compare teoricamente a capacidade de modelar depend√™ncias de longo alcance entre modelos n-gram e modelos de linguagem baseados em redes neurais recorrentes (RNNs). Como isso se reflete nas respectivas fun√ß√µes objetivo?

3. **Efetividade da Suaviza√ß√£o de Kneser-Ney**: Demonstre matematicamente por que a suaviza√ß√£o de Kneser-Ney √© particularmente eficaz para n-gramas de baixa frequ√™ncia. Como isso se relaciona com o conceito de "diversidade" de contextos das palavras?

4. **Converg√™ncia da Aproxima√ß√£o N-Gram**: Desenvolva uma prova formal de que a aproxima√ß√£o n-gram converge para a distribui√ß√£o verdadeira √† medida que $n$ aumenta, assumindo um processo estacion√°rio subjacente.

5. **Complexidade de Kolmogorov**: Analise o impacto te√≥rico da escolha do valor de $n$ na complexidade de Kolmogorov da distribui√ß√£o modelada. Como isso se relaciona com o princ√≠pio da descri√ß√£o m√≠nima (MDL)?

## Refer√™ncias

[1] "In probabilistic classification, the problem is to compute the probability of a label, conditioned on the text. Let's now consider the inverse problem: computing the probability of text itself." *(Trecho de Language Models_143-162.pdf.md)*

[2] "n-gram language models, which compute the probability of a sequence as the product of probabilities of subsequences." *(Trecho de Language Models_143-162.pdf.md)*

[3] "n-gram models make a crucial simplifying approximation: they condition on only the past n - 1 words." *(Trecho de Language Models_143-162.pdf.md)*

[4] "This means that the probability of a sentence w can be approximated as..." *(Trecho de Language Models_143-162.pdf.md)*

[5] "We therefore need to introduce bias to have a chance of making reliable estimates from finite training data." *(Trecho de Language Models_143-162.pdf.md)*

[6] "p(w) = p(w_1, w_2, ..., w_M) = p(w_1) √ó p(w_2 | w_1) √ó p(w_3 | w_1, w_2) √ó ... √ó p(w_M | w_1, w_2, ..., w_{M-1})" *(Trecho de Language Models_143-162.pdf.md)*

[7] "p(w_m | w_1, ..., w_{m-1}) ‚âà p(w_m | w_{m-n+1}, ..., w_{m-1})" *(Trecho de Language Models_143-162.pdf.md)*

[8] "This model requires estimating and storing the probability of only V^n events, which is exponential in the order of the n-gram, and not V^M, which is exponential in the length of the sentence." *(Trecho de Language Models_143-162.pdf.md)*

[9] "p(I like black coffee) = p(I | ‚ü®s‚ü©) √ó p(like | I) √ó p(black | like) √ó p(coffee | black) √ó p(‚ü®/s‚ü© | coffee)." *(Trecho de Language Models_143-162.pdf.md)*

[10] "p(w_m | w_{m-1}, w_{m-2}) = count(w_{m-2}, w_{m-1}, w_m) / sum_{w'} count(w_{m-2}, w_{m-1}, w')" *(Trecho de Language Models_143-162.pdf.md)*

[11] "This means that we also haven't really made any progress: to compute the conditional probability p(w_M | w_{M-1}, w_{M-2}, ..., w_1), we would need to model V^{M-1} contexts. Such a distribution cannot be estimated from any realistic sample of text." *(Trecho de Language Models_143-162.pdf.md)*

[12] "Language is full of long-range dependencies that we cannot capture because n is too small." *(Trecho de Language Models_143-162.pdf.md)*

[13] "To solve this problem, n-gram models make a crucial simplifying approximation: they condition on only the past n - 1 words." *(Trecho de Language Models_143-162.pdf.md)*

[14] "Even grammatical sentences will have probability zero if they have not occurred in the training data." *(Trecho de Language Models_143-162.pdf.md)*

[15] "The hyperparameter n controls the size of the context used in each conditional probability." *(Trecho de Language Models_143-162.pdf.md)*

[16] "If this is misspecified, the language model will perform poorly." *(Trecho de Language Models_143-162.pdf.md)*

[17] "It is therefore necessary to add additional inductive biases to n-gram language models." *(Trecho de Language Models_143-162.pdf.md)*

[18] "p_{\text{smooth}}(w_m | w_{m-1}) = (count(w_{m-1}, w_m) + Œ±) / (sum_{w' ‚àà V} count(w_{m-1}, w') + VŒ±)" *(Trecho de Language Models_143-162.pdf.md)*

[19] "Another approach would be to borrow the same amount of probability mass from all observed n-grams, and redistribute it among only the unobserved n-grams. This is called absolute discounting." *(Trecho de Language Models_143-162.pdf.md)*

[20] "Instead, we can backoff to a lower-order language model: if you have trigrams, use trigrams; if you don't have trigrams, use bigrams; if you don't even have bigrams, use unigrams. This is called Katz backoff." *(Trecho de Language Models_143-162.pdf.md)*

[21] "Kneser-Ney smoothing is based on absolute discounting, but it redistributes the resulting probability mass in a different way from Katz backoff." *(Trecho de Language Models_143-162.pdf.md)*

[22] "Empirical evidence points to Kneser-Ney smoothing as the state-of-art for n-gram language modeling (Goodman, 2001)." *(Trecho de Language Models_143-162.pdf.md)*

[23] "Using the Pytorch library, train an LSTM language model from the Wikitext training corpus. After each epoch of training, compute its perplexity on the Wikitext validation corpus. Stop training when the perplexity stops improving." *(Trecho de Language Models_143-162.pdf.md)*

[24] "N-gram language models have been largely supplanted by neural networks. These models do not make the n-gram assumption of restricted context; indeed, they can incorporate arbitrarily distant contextual information, while remaining computationally and statistically tractable." *(Trecho de Language Models_143-162.pdf.md)*

[25] "The first insight behind neural language models is to treat word prediction as a discriminative learning task." *(Trecho de Language Models_143-162.pdf.md)*