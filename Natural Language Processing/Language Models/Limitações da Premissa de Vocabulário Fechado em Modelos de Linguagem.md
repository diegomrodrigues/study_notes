Aqui est√° um resumo detalhado e avan√ßado sobre o t√≥pico "Closed-Vocabulary Assumption" em Modelos de Linguagem:

## Limita√ß√µes da Premissa de Vocabul√°rio Fechado em Modelos de Linguagem

<imagem: Um diagrama mostrando um conjunto finito de palavras (vocabul√°rio fechado) com setas apontando para palavras desconhecidas fora do conjunto, ilustrando as limita√ß√µes desta abordagem>

### Introdu√ß√£o

A premissa de vocabul√°rio fechado √© uma suposi√ß√£o fundamental em muitos modelos de linguagem, onde se assume que o vocabul√°rio $V$ √© um conjunto finito [1]. Esta abordagem, embora comum, apresenta limita√ß√µes significativas em cen√°rios de aplica√ß√£o realistas, especialmente quando se lida com textos din√¢micos e em constante evolu√ß√£o [2]. Este resumo explorar√° as implica√ß√µes desta premissa, suas limita√ß√µes e as abordagens alternativas para lidar com palavras fora do vocabul√°rio (out-of-vocabulary words - OOV).

### Conceitos Fundamentais

| Conceito                | Explica√ß√£o                                                   |
| ----------------------- | ------------------------------------------------------------ |
| **Vocabul√°rio Fechado** | Um conjunto finito $V$ de palavras conhecidas pelo modelo [3]. |
| **Palavras OOV**        | Termos que n√£o fazem parte do vocabul√°rio predefinido $V$ [4]. |
| **Token \<UNK\>**       | S√≠mbolo especial usado para representar palavras desconhecidas [5]. |

> ‚ö†Ô∏è **Nota Importante**: A premissa de vocabul√°rio fechado pode levar a uma representa√ß√£o inadequada de textos em dom√≠nios din√¢micos ou multil√≠ngues [6].

### Limita√ß√µes da Premissa de Vocabul√°rio Fechado

<imagem: Gr√°fico mostrando a frequ√™ncia de palavras em um corpus, com uma longa cauda de palavras raras ou novas que seriam ignoradas em um vocabul√°rio fechado>

A premissa de vocabul√°rio fechado enfrenta desafios significativos em aplica√ß√µes do mundo real:

1. **Neologismos e Termos Emergentes**: Incapacidade de lidar com novas palavras que surgem constantemente, especialmente em dom√≠nios como tecnologia e m√≠dias sociais [7].

2. **Nomes Pr√≥prios**: Dificuldade em representar adequadamente nomes de pessoas, lugares ou organiza√ß√µes que n√£o foram vistos durante o treinamento [8].

3. **Varia√ß√µes Morfol√≥gicas**: Em l√≠nguas com sistemas morfol√≥gicos ricos, muitas formas inflexionadas de palavras podem n√£o ser capturadas [9].

4. **Multilinguismo**: Desafios ao lidar com m√∫ltiplos idiomas ou code-switching, onde o vocabul√°rio pode expandir drasticamente [10].

5. **Erros Ortogr√°ficos e Variantes**: Incapacidade de lidar com erros de digita√ß√£o ou variantes ortogr√°ficas n√£o inclu√≠das no vocabul√°rio original [11].

#### üëç Vantagens do Vocabul√°rio Fechado

- Simplicidade computacional e efici√™ncia de mem√≥ria [12].
- Maior controle sobre o espa√ßo de representa√ß√£o do modelo [13].

#### üëé Desvantagens do Vocabul√°rio Fechado

- Perda de informa√ß√£o sem√¢ntica para palavras OOV [14].
- Redu√ß√£o da adaptabilidade do modelo a novos dom√≠nios ou dados [15].

### Abordagens para Lidar com Palavras OOV

1. **Uso do Token \<UNK\>**
   - Todas as palavras desconhecidas s√£o mapeadas para um √∫nico token especial \<UNK\> [16].
   - F√≥rmula de probabilidade:
     $$P(w_i | w_{1:i-1}) = P(\text{\<UNK\>} | w_{1:i-1}) \text{ se } w_i \notin V$$

2. **Modelagem em N√≠vel de Caracteres**
   - Utiliza modelos de linguagem baseados em caracteres para lidar com palavras OOV [17].
   - RNNs ou CNNs s√£o aplicadas sobre sequ√™ncias de caracteres [18].

3. **Segmenta√ß√£o em Subpalavras**
   - Decomp√µe palavras em unidades menores (morfemas ou subpalavras) [19].
   - M√©todos como Byte-Pair Encoding (BPE) ou WordPiece s√£o comumente utilizados [20].

4. **Modelos H√≠bridos**
   - Combinam abordagens em n√≠vel de palavra e caractere [21].
   - Exemplo: LSTM com componente de caracteres para palavras OOV [22].

### Implementa√ß√£o Avan√ßada: Modelo H√≠brido Palavra-Caractere

```python
import torch
import torch.nn as nn

class HybridWordCharModel(nn.Module):
    def __init__(self, word_vocab_size, char_vocab_size, word_embed_dim, char_embed_dim, hidden_dim):
        super().__init__()
        self.word_embedding = nn.Embedding(word_vocab_size, word_embed_dim)
        self.char_embedding = nn.Embedding(char_vocab_size, char_embed_dim)
        self.char_lstm = nn.LSTM(char_embed_dim, hidden_dim // 2, bidirectional=True)
        self.word_lstm = nn.LSTM(word_embed_dim + hidden_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, word_vocab_size)
        
    def forward(self, word_ids, char_ids):
        # Word-level processing
        word_embeds = self.word_embedding(word_ids)
        
        # Character-level processing for OOV words
        char_embeds = self.char_embedding(char_ids)
        char_hidden, _ = self.char_lstm(char_embeds)
        char_hidden = char_hidden[:, -1, :]  # Use last hidden state
        
        # Combine word and character representations
        combined = torch.cat([word_embeds, char_hidden], dim=-1)
        
        # Main LSTM
        output, _ = self.word_lstm(combined)
        
        # Prediction
        logits = self.fc(output)
        return logits

# Uso do modelo
model = HybridWordCharModel(word_vocab_size=10000, char_vocab_size=128, 
                            word_embed_dim=300, char_embed_dim=50, hidden_dim=512)
```

Este modelo h√≠brido combina representa√ß√µes em n√≠vel de palavra e caractere, permitindo lidar com palavras OOV de forma mais eficaz [23].

### Conclus√£o

A premissa de vocabul√°rio fechado, embora computacionalmente conveniente, apresenta limita√ß√µes significativas em cen√°rios do mundo real. Abordagens como modelagem em n√≠vel de caracteres, segmenta√ß√£o em subpalavras e modelos h√≠bridos oferecem solu√ß√µes mais robustas para lidar com a natureza din√¢mica e diversa da linguagem natural [24]. A escolha da abordagem deve considerar o equil√≠brio entre complexidade computacional, adaptabilidade e desempenho espec√≠fico da tarefa [25].

### Perguntas Te√≥ricas Avan√ßadas

1. Derive a express√£o para a perplexidade de um modelo de linguagem com vocabul√°rio fechado quando uma fra√ß√£o $f$ das palavras em um corpus de teste s√£o OOV. Como isso se compara com um modelo que pode gerar novas palavras?

2. Analise teoricamente o impacto da Lei de Zipf na efic√°cia de modelos de vocabul√°rio fechado vs. aberto. Como a distribui√ß√£o de cauda longa das palavras afeta o desempenho desses modelos?

3. Desenvolva uma prova matem√°tica que demonstre as condi√ß√µes sob as quais um modelo h√≠brido palavra-caractere superaria consistentemente um modelo puramente baseado em palavras em termos de perplexidade.

4. Formule uma express√£o para a complexidade computacional de um modelo de linguagem baseado em caracteres vs. um modelo de vocabul√°rio fechado tradicional. Em que condi√ß√µes o modelo baseado em caracteres se torna computacionalmente mais eficiente?

5. Proponha e analise teoricamente um novo m√©todo de tokeniza√ß√£o que otimize o trade-off entre cobertura de vocabul√°rio e efici√™ncia computacional, considerando a distribui√ß√£o de frequ√™ncia de palavras em corpora de grande escala.

### Refer√™ncias

[1] "We have assumed a closed-vocabulary setting ‚Äî the vocabulary $V$ is assumed to be a finite set." *(Trecho de Language Models_143-162.pdf.md)*

[2] "In realistic application scenarios, this assumption may not hold." *(Trecho de Language Models_143-162.pdf.md)*

[3] "Consider, for example, the problem of translating newspaper articles." *(Trecho de Language Models_143-162.pdf.md)*

[4] "The bolded terms either did not exist at this date, or were not widely known; they are unlikely to be in the vocabulary." *(Trecho de Language Models_143-162.pdf.md)*

[5] "One solution is to simply mark all such terms with a special token, \<UNK\>." *(Trecho de Language Models_143-162.pdf.md)*

[6] "The same problem can occur for a variety of other terms: new technologies, previously unknown individuals, new words (e.g., hashtag), and numbers." *(Trecho de Language Models_143-162.pdf.md)*

[7] "This is particularly important in languages that have rich morphological systems, with many inflections for each word." *(Trecho de Language Models_143-162.pdf.md)*

[8] "For example, Portuguese is only moderately complex from a morphological perspective, yet each verb has dozens of inflected forms (see Figure 4.3b)." *(Trecho de Language Models_143-162.pdf.md)*

[9] "In such languages, there will be many word types that we do not encounter in a corpus, which are nonetheless predictable from the morphological rules of the language." *(Trecho de Language Models_143-162.pdf.md)*

[10] "To use a somewhat contrived English example, if transfenestrate is in the vocabulary, our language model should assign a non-zero probability to the past tense transfenestrated, even if it does not appear in the training data." *(Trecho de Language Models_143-162.pdf.md)*

[11] "One way to accomplish this is to supplement word-level language models with character-level language models." *(Trecho de Language Models_143-162.pdf.md)*

[12] "Such models can use $n$-grams or RNNs, but with a fixed vocabulary equal to the set of ASCII or Unicode characters." *(Trecho de Language Models_143-162.pdf.md)*

[13] "For example, Ling et al. (2015) propose an LSTM model over characters, and Kim (2014) employ a convolutional neural network." *(Trecho de Language Models_143-162.pdf.md)*

[14] "A more linguistically motivated approach is to segment words into meaningful subword units, known as morphemes (see chapter 9)." *(Trecho de Language Models_143-162.pdf.md)*

[15] "For example, Botha and Blunsom (2014) induce vector representations for morphemes, which they build into a log-bilinear language model; Bhatia et al. (2016) incorporate morpheme vectors into an LSTM." *(Trecho de Language Models_143-162.pdf.md)*

[16] "While training the language model, we decide in advance on the vocabulary (often the $K$ most common terms), and mark all other terms in the training data as \<UNK\>." *(Trecho de Language Models_143-162.pdf.md)*

[17] "One way to accomplish this is to supplement word-level language models with character-level language models." *(Trecho de Language Models_143-162.pdf.md)*

[18] "For example, Ling et al. (2015) propose an LSTM model over characters, and Kim (2014) employ a convolutional neural network." *(Trecho de Language Models_143-162.pdf.md)*

[19] "A more linguistically motivated approach is to segment words into meaningful subword units, known as morphemes (see chapter 9)." *(Trecho de Language Models_143-162.pdf.md)*

[20] "For example, Botha and Blunsom (2014) induce vector representations for morphemes, which they build into a log-bilinear language model; Bhatia et al. (2016) incorporate morpheme vectors into an LSTM." *(Trecho de Language Models_143-162.pdf.md)*

[21] "One way to accomplish this is to supplement word-level language models with character-level language models." *(Trecho de Language Models_143-162.pdf.md)*

[22] "For example, Ling et al. (2015) propose an LSTM model over characters, and Kim (2014) employ a convolutional neural network." *(Trecho de Language Models_143-162.pdf.md)*

[23] "Using the Pytorch library, train an LSTM language model from the Wikitext training corpus." *(Trecho de Language Models_143-162.pdf.md)*

[24] "A more linguistically motivated approach is to segment words into meaningful subword units, known as morphemes (see chapter 9)." *(Trecho de Language Models_143-162.pdf.md)*

[25] "For example, Botha and Blunsom (2014) induce vector representations for morphemes, which they build into a log-bilinear language model; Bhatia et al. (2016) incorporate morpheme vectors into an LSTM." *(Trecho de Language Models_143-162.pdf.md)*