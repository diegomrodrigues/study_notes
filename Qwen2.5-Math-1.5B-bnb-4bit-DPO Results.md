## Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-v1

https://wandb.ai/none-ai/Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-v1/runs/uxu7ukmg?nw=nwuserdiegomrodrigues11

| total_flos               | 0          |
| ------------------------ | ---------- |
| train/epoch              | 5          |
| train/global_step        | 920        |
| train/grad_norm          | 0.38727    |
| train/learning_rate      | 0          |
| train/logits/chosen      | -0.3021    |
| train/logits/rejected    | -0.013     |
| train/logps/chosen       | -171.38914 |
| train/logps/rejected     | -312.57379 |
| train/loss               | 0.0024     |
| train/rewards/accuracies | ==1==      |
| train/rewards/chosen     | -8.7233    |
| train/rewards/margins    | ==16.046== |
| train/rewards/rejected   | -24.7693   |
| train_loss               | 0.10352    |
| train_runtime            | 2942.2008  |
| train_samples_per_second | 4.991      |
| train_steps_per_second   | 0.313      |



## Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-v2

https://wandb.ai/none-ai/Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-v2/runs/cdik72uo?nw=nwuserdiegomrodrigues11

| total_flos               | 0            |
| ------------------------ | ------------ |
| train/epoch              | 1.99456      |
| train/global_step        | 366          |
| train/grad_norm          | 0.12693      |
| train/learning_rate      | 0.0          |
| train/logits/chosen      | -0.66373     |
| train/logits/rejected    | -0.49809     |
| train/logps/chosen       | -124.75818   |
| train/logps/rejected     | -225.38985   |
| train/loss               | 0.1057       |
| train/rewards/accuracies | ==0.9875==   |
| train/rewards/chosen     | -4.56508     |
| train/rewards/margins    | ==11.69354== |
| train/rewards/rejected   | -16.25862    |
| train_loss               | 0.30108      |
| train_runtime            | 1199.8603    |
| train_samples_per_second | 4.896        |
| train_steps_per_second   | 0.305        |



## Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-v3

https://wandb.ai/none-ai/Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-v3/runs/n7zcgri3?nw=nwuserdiegomrodrigues11

| total_flos               | 0            |
| ------------------------ | ------------ |
| train/epoch              | 1.99456      |
| train/global_step        | 366          |
| train/grad_norm          | 0.04121      |
| train/learning_rate      | 0.0          |
| train/logits/chosen      | -0.54659     |
| train/logits/rejected    | -0.34037     |
| train/logps/chosen       | -140.55208   |
| train/logps/rejected     | -281.76572   |
| train/loss               | 0.0753       |
| train/rewards/accuracies | ==0.9875==   |
| train/rewards/chosen     | -6.14447     |
| train/rewards/margins    | ==15.75174== |
| train/rewards/rejected   | -21.89621    |
| train_loss               | 0.23474      |
| train_runtime            | 1201.6232    |
| train_samples_per_second | 4.888        |
| train_steps_per_second   | 0.305        |

### 

## **Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-v4**

https://wandb.ai/none-ai/Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-v4/runs/al4amq5x?nw=nwuserdiegomrodrigues11

| total_flos               | 0           |
| ------------------------ | ----------- |
| train/epoch              | 15          |
| train/global_step        | 1380        |
| train/grad_norm          | 0.13868     |
| train/learning_rate      | 0.0         |
| train/logits/chosen      | 0.57585     |
| train/logits/rejected    | 1.0018      |
| train/logps/chosen       | -187.40787  |
| train/logps/rejected     | -407.20099  |
| train/loss               | 0.0012      |
| train/rewards/accuracies | ==1==       |
| train/rewards/chosen     | -10.41994   |
| train/rewards/margins    | ==23.7208== |
| train/rewards/rejected   | -34.14074   |
| train_loss               | 0.14557     |
| train_runtime            | 8812.1175   |
| train_samples_per_second | 4.999       |
| train_steps_per_second   | 0.157       |



## Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-all-linear-v1

https://wandb.ai/none-ai/Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-all-linear-v1/runs/ufdcelvm?nw=nwuserdiegomrodrigues11

| total_flos               | 0            |
| ------------------------ | ------------ |
| train/epoch              | 4.98503      |
| train/global_step        | 455          |
| train/grad_norm          | 0.8491       |
| train/learning_rate      | 0.0          |
| train/logits/chosen      | -0.60603     |
| train/logits/rejected    | -0.26497     |
| train/logps/chosen       | -159.24236   |
| train/logps/rejected     | -310.54254   |
| train/loss               | 0.0397       |
| train/rewards/accuracies | ==0.99375==  |
| train/rewards/chosen     | -7.55709     |
| train/rewards/margins    | ==16.36242== |
| train/rewards/rejected   | -23.91951    |
| train_loss               | 0.49398      |
| train_runtime            | 3338.5284    |
| train_samples_per_second | 4.399        |
| train_steps_per_second   | 0.136        |

## **Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-all-linear-v2**

| total_flos               | 0          |
| ------------------------ | ---------- |
| train/epoch              | 4.98503    |
| train/global_step        | 455        |
| train/grad_norm          | 0.24376    |
| train/learning_rate      | 0.0        |
| train/logits/chosen      | 0.03656    |
| train/logits/rejected    | 0.43283    |
| train/logps/chosen       | -168.59952 |
| train/logps/rejected     | -347.95016 |
| train/loss               | 0.0362     |
| train/rewards/accuracies | 0.99375    |
| train/rewards/chosen     | -8.49281   |
| train/rewards/margins    | 19.16747   |
| train/rewards/rejected   | -27.66027  |
| train_loss               | 0.39113    |
| train_runtime            | 3316.6656  |
| train_samples_per_second | 4.428      |
| train_steps_per_second   | 0.137      |

## **Qwen2.5-Math-1.5B-bnb-4bit-DPO-finqa-all-linear-v3**

| total_flos               | 0          |
| ------------------------ | ---------- |
| train/epoch              | 4.98503    |
| train/global_step        | 455        |
| train/grad_norm          | 0.01414    |
| train/learning_rate      | 0.0        |
| train/logits/chosen      | 0.61483    |
| train/logits/rejected    | 1.04984    |
| train/logps/chosen       | -214.54773 |
| train/logps/rejected     | -459.34717 |
| train/loss               | 0.0352     |
| train/rewards/accuracies | 0.99375    |
| train/rewards/chosen     | -13.08763  |
| train/rewards/margins    | 25.71235   |
| train/rewards/rejected   | -38.79997  |
| train_loss               | 0.31689    |
| train_runtime            | 3325.2849  |
| train_samples_per_second | 4.416      |
| train_steps_per_second   | 0.137      |

## **Qwen2.5-Math-7B-bnb-4bit-DPO-finqa-all-linear-v1**

 https://wandb.ai/none-ai/Qwen2.5-Math-7B-bnb-4bit-DPO-finqa-all-linear-v1

| total_flos               | 0          |
| ------------------------ | ---------- |
| train/epoch              | 4.98503    |
| train/global_step        | 455        |
| train/grad_norm          | 0.10761    |
| train/learning_rate      | 0.0        |
| train/logits/chosen      | 2.38961    |
| train/logits/rejected    | 2.56664    |
| train/logps/chosen       | -243.46164 |
| train/logps/rejected     | -415.64606 |
| train/loss               | 0.036      |
| train/rewards/accuracies | 0.99375    |
| train/rewards/chosen     | -16.17857  |
| train/rewards/margins    | 17.55812   |
| train/rewards/rejected   | -33.73669  |
| train_loss               | 0.43409    |
| train_runtime            | 8883.5567  |
| train_samples_per_second | 1.653      |
| train_steps_per_second   | 0.051      |

## **Qwen2.5-Math-7B-bnb-4bit-DPO-finqa-all-linear-v2**

https://wandb.ai/none-ai/Qwen2.5-Math-7B-bnb-4bit-DPO-finqa-all-linear-v2

| otal_flos                | 0          |
| ------------------------ | ---------- |
| train/epoch              | 4.98503    |
| train/global_step        | 455        |
| train/grad_norm          | 0.06473    |
| train/learning_rate      | 0.0        |
| train/logits/chosen      | 2.91606    |
| train/logits/rejected    | 3.04036    |
| train/logps/chosen       | -221.83328 |
| train/logps/rejected     | -407.16782 |
| train/loss               | 0.0354     |
| train/rewards/accuracies | 0.99375    |
| train/rewards/chosen     | -14.01574  |
| train/rewards/margins    | 18.87314   |
| train/rewards/rejected   | -32.88887  |
| train_loss               | 0.36477    |
| train_runtime            | 8826.893   |
| train_samples_per_second | 1.664      |
| train_steps_per_second   | 0.052      |

## **Qwen2.5-Math-7B-bnb-4bit-DPO-finqa-all-linear-v3**

https://wandb.ai/none-ai/Qwen2.5-Math-7B-bnb-4bit-DPO-finqa-all-linear-v3

| total_flos               | 0          |
| ------------------------ | ---------- |
| train/epoch              | 4.98503    |
| train/global_step        | 455        |
| train/grad_norm          | 0.03311    |
| train/learning_rate      | 0.0        |
| train/logits/chosen      | 3.02594    |
| train/logits/rejected    | 3.14182    |
| train/logps/chosen       | -230.44626 |
| train/logps/rejected     | -434.76715 |
| train/loss               | 0.035      |
| train/rewards/accuracies | 0.99375    |
| train/rewards/chosen     | -14.87703  |
| train/rewards/margins    | 20.77177   |
| train/rewards/rejected   | -35.6488   |
| train_loss               | 0.29639    |
| train_runtime            | 8835.8283  |
| train_samples_per_second | 1.662      |
| train_steps_per_second   | 0.051      |



| Model                                                        | FinQA Train | FinQA Test | GSM8K |
| ------------------------------------------------------------ | ----------- | ---------- | ----- |
| Qwen 1.5b                                                    |             |            |       |
| Unsloth Qwen 1.5b                                            |             |            |       |
| Unsloth Qwen 1.5b DPO Up/down only<br />$r = 4, \alpha = 16, epochs = 5$ |             |            |       |
| Unsloth Qwen 1.5b DPO Up/down only<br />$r = 8, \alpha = 32, epochs = 5$ |             |            |       |
| Unsloth Qwen 1.5b DPO Up/down only<br />$r = 16, \alpha = 64, epochs = 5$ |             |            |       |
| Unsloth Qwen 1.5b DPO all linear layers<br />$r = 4, \alpha = 16, epochs = 5$ |             |            |       |
| Unsloth Qwen 1.5b DPO all linear layers<br />$r = 8, \alpha = 32, epochs = 5$ |             |            |       |
| Unsloth Qwen 1.5b DPO all linear layers<br />$r = 16, \alpha = 64, epochs = 5$ |             |            |       |
| Qwen 7b                                                      |             |            |       |
| Unsloth Qwen 7b                                              |             |            |       |
| Unsloth Qwen 7b DPO all linear layers<br />$r = 4, \alpha = 16, epochs = 5$ |             |            |       |
| Unsloth Qwen 7b DPO all linear layers<br />$r = 8, \alpha = 32, epochs = 5$ |             |            |       |
| Unsloth Qwen 7b DPO all linear layers<br />$r = 16, \alpha = 64, epochs = 5$ |             |            |       |

