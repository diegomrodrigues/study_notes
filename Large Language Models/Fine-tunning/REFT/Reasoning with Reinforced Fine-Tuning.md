## REFT: Reasoning with Reinforced Fine-Tuning

<imagem: Um diagrama mostrando o fluxo do processo REFT, com um modelo de linguagem sendo inicialmente treinado por SFT (Supervised Fine-Tuning), seguido por uma etapa de explora√ß√£o usando PPO (Proximal Policy Optimization) para gerar m√∫ltiplos caminhos de racioc√≠nio (CoT) e, finalmente, convergindo para uma pol√≠tica otimizada.>

### Introdu√ß√£o

O artigo apresenta uma nova abordagem chamada Reinforced Fine-Tuning (REFT) para aprimorar a capacidade de racioc√≠nio de Grandes Modelos de Linguagem (LLMs) na resolu√ß√£o de problemas matem√°ticos [1]. REFT surge como uma alternativa ao Supervised Fine-Tuning (SFT) tradicional, que utiliza anota√ß√µes de Chain-of-Thought (CoT) para treinar modelos. A principal motiva√ß√£o para o desenvolvimento do REFT √© superar as limita√ß√µes do SFT em termos de generaliza√ß√£o, especialmente em cen√°rios onde apenas um caminho de racioc√≠nio anotado est√° dispon√≠vel para cada quest√£o no conjunto de treinamento [2].

> üí° **Conceito-chave**: REFT combina SFT com aprendizado por refor√ßo online para explorar m√∫ltiplos caminhos de racioc√≠nio, melhorando a generaliza√ß√£o do modelo.

### Revis√£o da Literatura

O artigo se posiciona no contexto de pesquisas recentes sobre resolu√ß√£o de problemas matem√°ticos usando LLMs. Ele reconhece a import√¢ncia de abordagens anteriores que utilizam SFT com anota√ß√µes CoT [3], mas identifica uma lacuna crucial: a limita√ß√£o de usar apenas um caminho de racioc√≠nio anotado por quest√£o no conjunto de treinamento.

**Contribui√ß√µes principais da literatura anterior:**

1. Uso de CoT para melhorar o racioc√≠nio em LLMs [4]
2. Aplica√ß√£o de SFT para treinar modelos em tarefas de racioc√≠nio [5]
3. Desenvolvimento de t√©cnicas de prompting e engenharia de dados para melhorar o desempenho [6]

O REFT se distingue ao introduzir uma abordagem de aprendizado por refor√ßo que permite a explora√ß√£o de m√∫ltiplos caminhos de racioc√≠nio durante o treinamento, potencialmente levando a uma melhor generaliza√ß√£o.

### Metodologia

A metodologia do REFT consiste em duas etapas principais:

1. **Warm-up com SFT**: 
   - Inicializa√ß√£o do modelo usando SFT por 1-2 √©pocas [7]
   - Objetivo: Equipar o modelo com capacidade b√°sica de gerar respostas corretas

2. **Refinamento com Aprendizado por Refor√ßo**:
   - Utiliza√ß√£o do algoritmo PPO (Proximal Policy Optimization) [8]
   - Explora√ß√£o de m√∫ltiplos caminhos de racioc√≠nio (CoT) para cada quest√£o
   - Recompensas derivadas naturalmente das respostas corretas

**Modelos Te√≥ricos e Conceituais:**

| Conceito                               | Explica√ß√£o                                                   |
| -------------------------------------- | ------------------------------------------------------------ |
| **Chain-of-Thought (CoT)**             | Sequ√™ncia de etapas de racioc√≠nio intermedi√°rias para resolver um problema [9] |
| **Proximal Policy Optimization (PPO)** | Algoritmo de aprendizado por refor√ßo que otimiza a pol√≠tica de gera√ß√£o de respostas [10] |

**Procedimentos Experimentais:**

1. Warm-up do modelo usando SFT por 1-2 √©pocas
2. Amostragem de m√∫ltiplos caminhos CoT usando a pol√≠tica atual
3. C√°lculo de recompensas baseadas na corre√ß√£o das respostas
4. Atualiza√ß√£o da pol√≠tica usando PPO
5. Repeti√ß√£o dos passos 2-4 at√© converg√™ncia

> ‚ö†Ô∏è **Detalhe Importante**: O REFT n√£o requer um modelo de recompensa separado, diferentemente do RLHF (Reinforcement Learning from Human Feedback) [11].

**Equa√ß√µes e F√≥rmulas Principais:**

1. Fun√ß√£o de perda para SFT:

   $$
   \mathcal{L}_{SFT}(\theta) = -\mathbb{E}_{e\sim\mathcal{D}}\left[\sum_{t=1}^L \log(\pi_\theta(a_t|s_t))\right]
   $$

   Onde:
   - $\theta$: par√¢metros do modelo
   - $e$: sequ√™ncia de tokens da CoT
   - $\mathcal{D}$: conjunto de dados de treinamento
   - $L$: comprimento m√°ximo da sequ√™ncia
   - $\pi_\theta(a_t|s_t)$: probabilidade de a√ß√£o $a_t$ dado o estado $s_t$ [12]

2. Fun√ß√£o de recompensa para PPO:

   $$
   r(s_t, a_t, s_{t+1}) = \begin{cases}
   1, & \text{EXTRACT}(s_{t+1}) = y \\
   0.1, & \text{EXTRACT}(s_{t+1}) \neq \text{null}, \neq y \\
   0, & \text{EXTRACT}(s_{t+1}) = \text{null}
   \end{cases}
   $$

   Onde:
   - $s_t$: estado atual
   - $a_t$: a√ß√£o tomada
   - $s_{t+1}$: pr√≥ximo estado
   - $y$: resposta correta
   - EXTRACT(): fun√ß√£o para extrair a resposta do estado [13]

3. Objetivo do PPO:

   $$
   \mathcal{L}_{RL}(\theta, \phi) = \mathcal{L}_{policy} + \alpha\mathcal{L}_{value}
   $$

   Onde:
   - $\mathcal{L}_{policy}$: fun√ß√£o de perda da pol√≠tica
   - $\mathcal{L}_{value}$: fun√ß√£o de perda do valor
   - $\alpha$: coeficiente de balanceamento [14]

### Resultados

Os experimentos foram conduzidos em tr√™s conjuntos de dados: GSM8K, SVAMP e MathQA, utilizando dois modelos base: Galactica-6.7B e CodeLLAMA-7B [15].

**Tabela de Resultados Principais:**

| M√©todo           | GSM8K (N-CoT / P-CoT) | SVAMP (N-CoT / P-CoT) | MathQAMCQ (N-CoT / P-CoT) |
| ---------------- | --------------------- | --------------------- | ------------------------- |
| Galactica + SFT  | 42.68 / 58.83         | 54.50 / 70.09         | 58.07 / 64.61             |
| Galactica + REFT | 48.14 / 68.91         | 61.40 / 74.09         | 58.13 / 70.47             |
| CodeLLAMA + SFT  | 43.59 / 63.68         | 58.09 / 75.40         | 56.01 / 64.79             |
| CodeLLAMA + REFT | 53.30 / 75.28         | 64.50 / 79.19         | 60.13 / 71.83             |

[16]

> ‚úîÔ∏è **Achado Significativo**: REFT consistentemente supera o SFT em quase todas as configura√ß√µes, com melhorias particularmente not√°veis no GSM8K.

**An√°lises e Interpreta√ß√µes:**

1. REFT demonstra ganhos significativos em rela√ß√£o ao SFT, especialmente em tarefas mais complexas como GSM8K [17].
2. A abordagem baseada em programa√ß√£o (P-CoT) geralmente supera a abordagem baseada em linguagem natural (N-CoT) [18].
3. O desempenho do REFT √© ainda mais aprimorado quando combinado com t√©cnicas de infer√™ncia como vota√ß√£o por maioria e reordena√ß√£o por modelo de recompensa [19].

### Proposi√ß√µes, Teoremas e Provas

Embora o artigo n√£o apresente teoremas formais, ele prop√µe v√°rias hip√≥teses e princ√≠pios importantes:

**Proposi√ß√£o 1: Generaliza√ß√£o Aprimorada**

*Enunciado*: O REFT melhora a capacidade de generaliza√ß√£o dos modelos de linguagem em tarefas de racioc√≠nio matem√°tico em compara√ß√£o com o SFT [20].

*Evid√™ncia*:
1. Desempenho consistentemente superior em m√∫ltiplos conjuntos de dados [21].
2. Capacidade de explorar m√∫ltiplos caminhos de racioc√≠nio durante o treinamento [22].

**Proposi√ß√£o 2: Efici√™ncia de Dados**

*Enunciado*: O REFT obt√©m melhorias de desempenho utilizando o mesmo conjunto de quest√µes de treinamento que o SFT, sem necessidade de dados adicionais ou aumentados [23].

*Evid√™ncia*:
1. Resultados compar√°veis ou superiores a m√©todos que utilizam dados aumentados ou gerados por modelos como GPT-3.5 [24].
2. Capacidade de extrair mais informa√ß√µes √∫teis do mesmo conjunto de dados atrav√©s da explora√ß√£o de m√∫ltiplos caminhos de racioc√≠nio [25].

> ‚ùó **Ponto de Aten√ß√£o**: A efici√™ncia do REFT em termos de dados sugere que h√° potencial inexplorado nos conjuntos de dados existentes que pode ser aproveitado atrav√©s de t√©cnicas de aprendizado por refor√ßo.

### Discuss√£o

O REFT apresenta uma abordagem inovadora para melhorar o racioc√≠nio em LLMs, com v√°rias vantagens e algumas limita√ß√µes:

**Compara√ß√µes com Trabalhos Anteriores:**

| Aspecto                    | REFT [26] | SFT Tradicional [27] |
| -------------------------- | --------- | -------------------- |
| Explora√ß√£o de Caminhos     | M√∫ltiplos | √önico                |
| Generaliza√ß√£o              | Superior  | Limitada             |
| Efici√™ncia de Dados        | Alta      | Moderada             |
| Complexidade Computacional | Maior     | Menor                |

**Limita√ß√µes e Perspectivas Futuras:**

1. **Efici√™ncia de Treinamento**: O REFT requer mais √©pocas de treinamento para convergir em compara√ß√£o com o SFT [28].
   - *Perspectiva*: Investigar t√©cnicas de otimiza√ß√£o para acelerar a converg√™ncia.

2. **Reward Hacking**: Potencial para explora√ß√£o de recompensas em cen√°rios com espa√ßo de respostas limitado (e.g., m√∫ltipla escolha) [29].
   - *Perspectiva*: Desenvolver fun√ß√µes de recompensa mais sofisticadas ou baseadas em processos.

3. **Escalabilidade**: O artigo foca em modelos de tamanho moderado (6.7B-7B par√¢metros) [30].
   - *Perspectiva*: Explorar a aplicabilidade do REFT em modelos maiores e mais complexos.

### Conclus√£o

O REFT representa um avan√ßo significativo na melhoria da capacidade de racioc√≠nio de LLMs para resolu√ß√£o de problemas matem√°ticos [31]. Ao combinar SFT com aprendizado por refor√ßo online, o REFT demonstra uma capacidade superior de generaliza√ß√£o e efici√™ncia de dados em compara√ß√£o com abordagens tradicionais [32].

As principais contribui√ß√µes do REFT incluem:
1. Explora√ß√£o de m√∫ltiplos caminhos de racioc√≠nio durante o treinamento [33].
2. Melhoria significativa no desempenho em benchmarks padr√£o de resolu√ß√£o de problemas matem√°ticos [34].
3. Demonstra√ß√£o do potencial do aprendizado por refor√ßo para aprimorar LLMs em tarefas de racioc√≠nio [35].

Futuros desenvolvimentos do REFT podem focar na otimiza√ß√£o da efici√™ncia de treinamento, no refinamento das fun√ß√µes de recompensa e na aplica√ß√£o da t√©cnica a uma gama mais ampla de tarefas de racioc√≠nio al√©m da matem√°tica [36].

### Perguntas Te√≥ricas

1. Como o REFT poderia ser modificado para incorporar feedback humano durante o processo de treinamento, semelhante ao RLHF, mantendo sua efici√™ncia em termos de dados?

2. Considerando a arquitetura do REFT, como voc√™ proporia uma extens√£o do m√©todo para lidar com problemas que requerem racioc√≠nio em m√∫ltiplas etapas, onde a recompensa intermedi√°ria n√£o √© facilmente definida?

3. Analise teoricamente como a escolha do coeficiente de diverg√™ncia KL ($\beta$) no REFT afeta o equil√≠brio entre explora√ß√£o e estabilidade do modelo. Como isso se compara com os trade-offs em outros algoritmos de aprendizado por refor√ßo?

### Perguntas Te√≥ricas Avan√ßadas

1. Desenvolva uma prova te√≥rica para demonstrar sob quais condi√ß√µes o REFT garantidamente converge para uma pol√≠tica √≥tima, considerando o espa√ßo de busca de caminhos de racioc√≠nio e a fun√ß√£o de recompensa proposta no artigo.

2. Proponha uma extens√£o te√≥rica do REFT que incorpore incerteza epist√™mica na gera√ß√£o de caminhos de racioc√≠nio. Como isso afetaria a converg√™ncia e a generaliza√ß√£o do modelo em problemas com m√∫ltiplas solu√ß√µes v√°lidas?

3. Analise matematicamente o impacto da fun√ß√£o de recompensa esparsa do REFT na efici√™ncia do aprendizado. Como voc√™ modificaria a fun√ß√£o de recompensa para criar um gradiente de aprendizado mais suave, mantendo a fidelidade ao objetivo original?

4. Formule um teorema que relacione a complexidade do espa√ßo de busca de caminhos de racioc√≠nio no REFT com a taxa de converg√™ncia do algoritmo. Como isso se compara com os limites te√≥ricos de outros algoritmos de aprendizado por refor√ßo em espa√ßos de a√ß√£o discretos e de alta dimensionalidade?

5. Desenvolva um framework te√≥rico para analisar a transfer√™ncia de conhecimento no REFT entre diferentes dom√≠nios de problemas matem√°ticos. Como voc√™ quantificaria e maximizaria a transfer√™ncia de habilidades de racioc√≠nio aprendidas?

### Refer√™ncias

[1] "We propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example." *(Abstract)*

[2] "Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question." *(Abstract)*

[3] "The state-of-the-art approaches to solving math problems (Luo et al., 2023; Wang et al., 2023a) employ Supervised Fine-Tuning (SFT) to train the models using Chain-of-Thought (CoT) annotations (Wei et al., 2022)." *(Introduction)*

[4] "As shown in Figure 1, a CoT annotation outlines the intermediate reasoning steps toward solving a math problem." *(Introduction)*

[5] "Usually there is only CoT annotation for each question in the training data, i.e., one correct reasoning path, which is utilized in SFT." *(Introduction)*

[6] "Recent research efforts focus on CoT prompt design and data engineering." *(Related Work)*

[7] "ReFT commences with a warm-up stage involving Supervised Fine-