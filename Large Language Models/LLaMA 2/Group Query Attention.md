# GQA: Treinamento de Modelos Transformer Multi-Query Generalizados a partir de Checkpoints Multi-Head

````mermaid
graph LR
    %% Diagrama mostrando a transi√ß√£o de MHA para GQA

    subgraph MHA [Aten√ß√£o Multi-Head]
        direction LR
        InputMHA[Entrada]
        InputMHA --> Q1[Consulta 1]
        InputMHA --> K1[Chave 1]
        InputMHA --> V1[Valor 1]
        InputMHA --> Q2[Consulta 2]
        InputMHA --> K2[Chave 2]
        InputMHA --> V2[Valor 2]
        InputMHA --> Q3[Consulta 3]
        InputMHA --> K3[Chave 3]
        InputMHA --> V3[Valor 3]
        InputMHA --> Q4[Consulta 4]
        InputMHA --> K4[Chave 4]
        InputMHA --> V4[Valor 4]

        Q1 --> Attn1[Atendimento 1]
        K1 --> Attn1
        V1 --> Attn1

        Q2 --> Attn2[Atendimento 2]
        K2 --> Attn2
        V2 --> Attn2

        Q3 --> Attn3[Atendimento 3]
        K3 --> Attn3
        V3 --> Attn3

        Q4 --> Attn4[Atendimento 4]
        K4 --> Attn4
        V4 --> Attn4

        Attn1 --> SaidaMHA[Sa√≠da MHA]
        Attn2 --> SaidaMHA
        Attn3 --> SaidaMHA
        Attn4 --> SaidaMHA
    end

    MHA -->|Transi√ß√£o| GQA

    subgraph GQA [Aten√ß√£o Grouped-Query]
        direction LR
        InputGQA[Entrada]
        InputGQA --> Q1g[Consulta 1]
        InputGQA --> Q2g[Consulta 2]
        InputGQA --> Q3g[Consulta 3]
        InputGQA --> Q4g[Consulta 4]
        InputGQA --> Kg[Chave Agrupada]
        InputGQA --> Vg[Valor Agrupado]

        Q1g --> AttnG1[Atendimento 1]
        Kg --> AttnG1
        Vg --> AttnG1

        Q2g --> AttnG2[Atendimento 2]
        Kg --> AttnG2
        Vg --> AttnG2

        Q3g --> AttnG3[Atendimento 3]
        Kg --> AttnG3
        Vg --> AttnG3

        Q4g --> AttnG4[Atendimento 4]
        Kg --> AttnG4
        Vg --> AttnG4

        AttnG1 --> SaidaGQA[Sa√≠da GQA]
        AttnG2 --> SaidaGQA
        AttnG3 --> SaidaGQA
        AttnG4 --> SaidaGQA
    end
````



### Introdu√ß√£o

O artigo "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints" apresenta uma inova√ß√£o significativa no campo do processamento de linguagem natural, focando na otimiza√ß√£o de modelos Transformer para infer√™ncia r√°pida [1]. Os autores abordam o ==desafio de equilibrar a qualidade do modelo com a velocidade de infer√™ncia==, propondo uma solu√ß√£o que combina as vantagens da ==aten√ß√£o multi-query (MQA) com a robustez da aten√ß√£o multi-head (MHA) [2].==

A relev√¢ncia deste trabalho √© destacada pelo crescente uso de modelos de linguagem em larga escala e a necessidade de torn√°-los mais eficientes para aplica√ß√µes pr√°ticas. Os objetivos principais do artigo s√£o:

1. ==Propor um m√©todo para converter modelos MHA existentes em modelos MQA com apenas 5% do custo computacional original de pr√©-treinamento [3].==
2. ==Introduzir a aten√ß√£o grouped-query (GQA), uma generaliza√ß√£o da MQA que oferece um equil√≠brio entre qualidade e velocidade [4].==

> üí° **Contribui√ß√£o Chave**: O artigo apresenta uma abordagem inovadora para melhorar a efici√™ncia de infer√™ncia dos modelos Transformer sem comprometer significativamente a qualidade.

### Revis√£o da Literatura

O artigo se posiciona no contexto de pesquisas anteriores sobre otimiza√ß√£o de modelos Transformer, particularmente no que diz respeito √† redu√ß√£o do overhead de largura de banda de mem√≥ria durante a infer√™ncia. ==Os autores reconhecem o trabalho seminal de Shazeer (2019) na proposi√ß√£o da aten√ß√£o multi-query [5], que reduziu significativamente o overhead de mem√≥ria ao usar apenas uma cabe√ßa de chave e valor.==

Trabalhos subsequentes, como Pope et al. (2022) e de Jong et al. (2022), demonstraram a efic√°cia da MQA, especialmente para entradas longas [6]. O artigo tamb√©m menciona outras abordagens para reduzir o overhead de largura de banda de mem√≥ria, incluindo:

- ==Flash attention (Dao et al., 2022)==
- Quantiza√ß√£o (Dettmers et al., 2022; Frantar et al., 2022)
- Destila√ß√£o de modelo (Hinton et al., 2015; Gou et al., 2021)
- Aten√ß√£o cruzada esparsa em camadas (de Jong et al., 2022)
- Amostragem especulativa (Chen et al., 2023; Leviathan et al., 2022) [7]

A contribui√ß√£o √∫nica deste artigo est√° na proposta de uma abordagem intermedi√°ria entre MHA e MQA, bem como um m√©todo eficiente para converter modelos existentes.

### Metodologia

#### Modelos Te√≥ricos e Conceituais:

| Conceito                        | Explica√ß√£o                                                   |
| ------------------------------- | ------------------------------------------------------------ |
| **Aten√ß√£o Multi-Head (MHA)**    | M√©todo padr√£o em modelos Transformer, onde m√∫ltiplas cabe√ßas de aten√ß√£o processam consultas, chaves e valores independentemente [8]. |
| **Aten√ß√£o Multi-Query (MQA)**   | Variante que usa ==m√∫ltiplas cabe√ßas de consulta, mas apenas uma cabe√ßa de chave e valor, reduzindo o overhead de mem√≥ria [9].== |
| **Aten√ß√£o Grouped-Query (GQA)** | Generaliza√ß√£o proposta que ==divide as cabe√ßas de consulta em grupos, cada um compartilhando uma √∫nica cabe√ßa de chave e valor [10].== |

#### Procedimentos Experimentais:

1. **Convers√£o de Checkpoint:**

   - **Agrupamento das Proje√ß√µes de Chave e Valor:**

     ==As matrizes de proje√ß√£o de chave e valor das cabe√ßas s√£o agrupadas usando m√©dia (mean pooling) para criar matrizes de proje√ß√£o compartilhadas [16]:==
     $$
     W_g^K = \frac{1}{|C_g|} \sum_{i \in C_g} W_i^K
     $$

     $$
     W_g^V = \frac{1}{|C_g|} \sum_{i \in C_g} W_i^V
     $$

     Onde $C_g$ √© o conjunto de cabe√ßas no grupo $g$.

2. **Uptraining:**

   - **Treinamento Adicional:**

     ==Ap√≥s a convers√£o, o modelo √© treinado adicionalmente por uma fra√ß√£o $\alpha$ dos passos de pr√©-treinamento original [17].==

     Isso permite que o modelo ==ajuste os pesos para a nova estrutura de aten√ß√£o, recuperando o desempenho.==

3. **Configura√ß√µes Experimentais:**

   - **Arquitetura Base:**

     Todos os modelos s√£o baseados na arquitetura T5.1.1 [18].

   - **Hiperpar√¢metros:**

     O otimizador Adafactor √© utilizado com os mesmos hiperpar√¢metros e agendamento de taxa de aprendizado do T5 original [19].

   - **Aplica√ß√£o de GQA:**

     MQA e GQA s√£o aplicados apenas √† auto-aten√ß√£o do decodificador e √† aten√ß√£o cruzada, n√£o √† auto-aten√ß√£o do codificador [20].

> ‚ö†Ô∏è **Detalhe Importante**: MQA e GQA s√£o aplicados apenas √† auto-aten√ß√£o do decodificador e √† aten√ß√£o cruzada, n√£o √† auto-aten√ß√£o do codificador [18].

#### Equa√ß√µes e F√≥rmulas Principais:

![image-20240917133412325](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240917133412325.png)

A aten√ß√£o grouped-query pode ser representada matematicamente como:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Onde:
- $Q$ representa as matrizes de consulta (m√∫ltiplas cabe√ßas)
- $K$ e $V$ representam as matrizes de chave e valor (n√∫mero reduzido de cabe√ßas)
- $d_k$ √© a dimens√£o das chaves

Para GQA com $G$ grupos, temos:

$$
Q \in \mathbb{R}^{n \times (G \times h_q) \times d_k}, K, V \in \mathbb{R}^{n \times G \times d_k}
$$

Onde:
- $n$ √© o tamanho da sequ√™ncia
- $h_q$ √© o n√∫mero de cabe√ßas de consulta por grupo
- $G$ √© o n√∫mero de grupos [19]

##### Aten√ß√£o Grouped-Query:

Para $G$ grupos, cada com $h_q$ cabe√ßas de consulta, temos:

- N√∫mero total de cabe√ßas de consulta: $H = G \times h_q$

- Proje√ß√µes:

  $$
  Q_{g,i} = QW_{g,i}^Q, \quad K_g = KW_g^K, \quad V_g = VW_g^V
  $$

  Onde $i = 1, \dots, h_q$, e $g = 1, \dots, G$.

- Aten√ß√£o para cada cabe√ßa no grupo $g$:

  $$
  \text{head}_{g,i} = \text{Attention}(Q_{g,i}, K_g, V_g)
  $$

- Sa√≠da GQA:

  $$
  \text{GQA}(Q, K, V) = \text{Concat}(\text{head}_{1,1}, \dots, \text{head}_{G,h_q})W^O
  $$

### Resultados

Os autores apresentam resultados comparativos entre modelos MHA, MQA e GQA em v√°rias tarefas de processamento de linguagem natural:

#### Tabela de Desempenho:

| Modelo    | T<sub>infer</sub> (s) | M√©dia | CNN/DM R<sub>1</sub> | arXiv R<sub>1</sub> | PubMed R<sub>1</sub> | MediaSum R<sub>1</sub> | MultiNews R<sub>1</sub> | WMT BLEU | TriviaQA F1 |
| --------- | --------------------- | ----- | -------------------- | ------------------- | -------------------- | ---------------------- | ----------------------- | -------- | ----------- |
| MHA-Large | 0.37                  | 46.0  | 42.9                 | 44.6                | 46.2                 | 35.5                   | 46.6                    | 27.7     | 78.2        |
| MHA-XXL   | 1.51                  | 47.2  | 43.8                 | 45.6                | 47.5                 | 36.4                   | 46.9                    | 28.4     | 81.9        |
| MQA-XXL   | 0.24                  | 46.6  | 43.0                 | 45.0                | 46.9                 | 36.1                   | 46.5                    | 28.5     | 81.3        |
| GQA-8-XXL | 0.28                  | 47.1  | 43.5                 | 45.4                | 47.7                 | 36.3                   | 47.2                    | 28.4     | 81.6        |

[20]

#### An√°lises e Interpreta√ß√µes:

> ‚úîÔ∏è **Achado Significativo**: GQA-8-XXL alcan√ßa desempenho pr√≥ximo ao MHA-XXL com tempo de infer√™ncia significativamente menor, oferecendo um equil√≠brio √≥timo entre qualidade e velocidade [21].

1. MQA-XXL apresenta uma redu√ß√£o substancial no tempo de infer√™ncia (0.24s) em compara√ß√£o com MHA-XXL (1.51s), mantendo um desempenho competitivo [22].
2. GQA-8-XXL oferece um compromisso intermedi√°rio, com tempo de infer√™ncia ligeiramente superior ao MQA-XXL (0.28s), mas com desempenho mais pr√≥ximo ao MHA-XXL [23].
3. Em tarefas espec√≠ficas como PubMed e MultiNews, GQA-8-XXL supera at√© mesmo o MHA-XXL, demonstrando sua efic√°cia em certos dom√≠nios [24].

### Proposi√ß√µes, Teoremas e Provas

Embora o artigo n√£o apresente teoremas formais, ele prop√µe conceitos importantes que podem ser formulados como proposi√ß√µes:

**Proposi√ß√£o 1: Efic√°cia do Uptraining**

*Enunciado:* O uptraining de modelos MHA para MQA ou GQA com apenas 5% dos passos originais de pr√©-treinamento √© suficiente para recuperar a maior parte do desempenho do modelo original [25].

*Prova (Emp√≠rica):*
1. Os autores converteram checkpoints MHA para MQA e GQA.
2. Realizaram uptraining por Œ±=0.05 (5%) dos passos originais de pr√©-treinamento.
3. Avaliaram o desempenho em v√°rias tarefas de NLP.
4. Os resultados mostram que o desempenho dos modelos uptrainados se aproxima significativamente dos modelos MHA originais [26].

> ‚ùó **Ponto de Aten√ß√£o:** A efic√°cia do uptraining pode variar dependendo da tarefa e do tamanho do modelo, sendo necess√°ria uma an√°lise cuidadosa para cada aplica√ß√£o espec√≠fica.

**Proposi√ß√£o 2: Vantagem da Aten√ß√£o Grouped-Query**

*Enunciado:* A aten√ß√£o grouped-query (GQA) oferece um compromisso superior entre qualidade do modelo e velocidade de infer√™ncia em compara√ß√£o com MHA e MQA puras [27].

*Prova (Emp√≠rica e Te√≥rica):*
1. Teoricamente, GQA reduz o overhead de mem√≥ria em compara√ß√£o com MHA, mas mant√©m mais capacidade que MQA.
2. Empiricamente, GQA-8-XXL alcan√ßa desempenho m√©dio de 47.1, pr√≥ximo ao MHA-XXL (47.2), com tempo de infer√™ncia de 0.28s, significativamente menor que MHA-XXL (1.51s).
3. GQA supera MQA em qualidade (47.1 vs 46.6) com um aumento m√≠nimo no tempo de infer√™ncia (0.28s vs 0.24s) [28].

### Discuss√£o

#### Compara√ß√µes com Trabalhos Anteriores:

| Aspecto       | Este Artigo (GQA) [29]                           | MQA (Shazeer, 2019) [30]               |
| ------------- | ------------------------------------------------ | -------------------------------------- |
| M√©todo        | Grupos intermedi√°rios de cabe√ßas de chave/valor  | Uma √∫nica cabe√ßa de chave/valor        |
| Qualidade     | Pr√≥xima √† MHA                                    | Degrada√ß√£o em rela√ß√£o √† MHA            |
| Velocidade    | Ligeiramente menor que MQA pura                  | M√°xima redu√ß√£o de overhead             |
| Flexibilidade | Permite ajuste fino entre qualidade e velocidade | Fixo na configura√ß√£o de menor overhead |

#### Limita√ß√µes e Perspectivas Futuras:

1. **Limita√ß√£o de Avalia√ß√£o:** O artigo reconhece que as m√©tricas utilizadas (como ROUGE) podem n√£o capturar completamente a qualidade das sa√≠das, especialmente para sequ√™ncias longas [31].

2. **Generaliza√ß√£o:** Os experimentos focam principalmente em modelos encoder-decoder. √â necess√°rio investigar o impacto de GQA em arquiteturas decoder-only, que s√£o cada vez mais populares [32].

3. **Otimiza√ß√£o de Hiperpar√¢metros:** O n√∫mero √≥timo de grupos GQA pode variar dependendo do tamanho do modelo e da tarefa. Pesquisas futuras poderiam explorar m√©todos para determinar automaticamente a configura√ß√£o ideal [33].

### Conclus√£o

O artigo apresenta uma contribui√ß√£o significativa para a otimiza√ß√£o de modelos Transformer, introduzindo a aten√ß√£o grouped-query (GQA) e um m√©todo eficiente de uptraining. As principais conclus√µes s√£o:

1. GQA oferece um equil√≠brio superior entre qualidade e velocidade de infer√™ncia em compara√ß√£o com MHA e MQA puras [34].
2. O m√©todo de uptraining proposto permite a convers√£o eficiente de modelos MHA existentes para GQA com apenas 5% do custo computacional original [35].
3. A abordagem proposta √© particularmente promissora para modelos de grande escala, onde o overhead de mem√≥ria √© um gargalo significativo [36].

Futuros caminhos de pesquisa incluem a aplica√ß√£o de GQA a arquiteturas decoder-only, otimiza√ß√£o autom√°tica do n√∫mero de grupos, e investiga√ß√£o de seus benef√≠cios em tarefas al√©m do processamento de linguagem natural [37].

### Perguntas Te√≥ricas

1. Derive a complexidade computacional e de mem√≥ria da aten√ß√£o grouped-query em fun√ß√£o do n√∫mero de grupos G, n√∫mero de cabe√ßas de consulta H, e tamanho da sequ√™ncia N. Compare com MHA e MQA.

2. Analise teoricamente o impacto da redu√ß√£o no n√∫mero de cabe√ßas de chave e valor na capacidade representacional do modelo. Como isso afeta a habilidade do modelo de capturar diferentes tipos de depend√™ncias nos dados?

3. Proponha um m√©todo te√≥rico para determinar o n√∫mero √≥timo de grupos GQA dado um tamanho de modelo e restri√ß√µes de lat√™ncia espec√≠ficas.

### Perguntas Te√≥ricas Avan√ßadas

1. Desenvolva uma prova formal demonstrando que, sob certas condi√ß√µes, GQA com um n√∫mero apropriado de grupos pode aproximar arbitrariamente bem o desempenho de MHA. Quais seriam essas condi√ß√µes?

2. Analise o impacto te√≥rico de GQA na estabilidade num√©rica e na converg√™ncia durante o treinamento. Como a redu√ß√£o no n√∫mero de cabe√ßas de chave e valor afeta a propaga√ß√£o do gradiente?

3. Proponha uma extens√£o te√≥rica de GQA que permita um n√∫mero vari√°vel de grupos por camada. Derive as equa√ß√µes para o c√°lculo da aten√ß√£o neste cen√°rio e discuta os potenciais benef√≠cios e desafios desta abordagem.

4. Desenvolva um modelo te√≥rico para prever o desempenho de GQA em fun√ß√£o do tamanho do modelo, n√∫mero de grupos, e caracter√≠sticas da tarefa. Como este modelo poderia ser usado para otimizar a arquitetura do Transformer para uma dada aplica√ß√£o?

5. Analise as implica√ß√µes te√≥ricas de GQA para a interpretabilidade do modelo. Como a estrutura de grupos afeta nossa capacidade de compreender e visualizar os padr√µes de aten√ß√£o aprendidos pelo modelo?

### Refer√™ncias

[1] "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints" (T√≠tulo do Artigo)

[2] "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation" (Resumo)

[3] "We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute"