## Implementa√ß√£o Detalhada do Modelo LLaMA

### 1. LlamaRMSNorm

<image: Um diagrama mostrando o fluxo de dados atrav√©s da camada de normaliza√ß√£o RMS, com entradas, c√°lculos intermedi√°rios e sa√≠das>

A classe `LlamaRMSNorm` implementa a normaliza√ß√£o RMS (Root Mean Square), que √© crucial para a estabilidade do treinamento [1].

```python
class LlamaRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)
```

Explica√ß√£o detalhada:
1. O construtor inicializa um par√¢metro `weight` com valores 1 para cada dimens√£o do `hidden_size`.
2. O m√©todo `forward` realiza a normaliza√ß√£o:
   - Converte os `hidden_states` para float32 para maior precis√£o nos c√°lculos.
   - Calcula a vari√¢ncia ao longo da √∫ltima dimens√£o.
   - Normaliza os `hidden_states` dividindo pela raiz quadrada da vari√¢ncia (com epsilon para estabilidade).
   - Multiplica pelo peso aprend√≠vel e converte de volta para o dtype original.

> ‚ö†Ô∏è **Nota Importante**: A convers√£o para float32 durante os c√°lculos √© crucial para manter a precis√£o num√©rica, especialmente em modelos de grande escala.

#### Quest√µes T√©cnicas:
1. Por que √© importante manter o dtype original dos `hidden_states` na sa√≠da da camada de normaliza√ß√£o?
2. Como o par√¢metro `weight` afeta o comportamento da normaliza√ß√£o RMS e por que √© implementado como um `nn.Parameter`?

### 2. LlamaRotaryEmbedding

<image: Uma visualiza√ß√£o das matrizes de seno e cosseno geradas pelo RoPE para diferentes posi√ß√µes e dimens√µes>

A classe `LlamaRotaryEmbedding` implementa os Embeddings Posicionais Rotativos (RoPE) [1].

```python
class LlamaRotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))
        self.register_buffer("inv_freq", inv_freq)
        self.max_seq_len_cached = max_position_embeddings
        t = torch.arange(self.max_seq_len_cached, device=device).type_as(self.inv_freq)
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :], persistent=False)
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :], persistent=False)

    def forward(self, x, seq_len=None):
        # x: [bs, num_attention_heads, seq_len, head_size]
        if seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache(seq_len)

        return (
            self.cos_cached[:, :, :seq_len, ...],
            self.sin_cached[:, :, :seq_len, ...],
        )
```

Explica√ß√£o detalhada:
1. O construtor calcula as frequ√™ncias inversas e cria caches para os valores de seno e cosseno.
2. O m√©todo `forward` retorna os valores de cosseno e seno pr√©-calculados para as posi√ß√µes requisitadas.
3. Se a sequ√™ncia for maior que o cache, ele √© recalculado usando `_set_cos_sin_cache`.

> üí° **Insight**: O caching dos valores de seno e cosseno melhora significativamente a efici√™ncia computacional durante o forward pass.

#### Quest√µes T√©cnicas:
1. Como o par√¢metro `base` afeta a resolu√ß√£o dos embeddings posicionais e qual √© o trade-off ao escolher seu valor?
2. Explique como o RoPE permite ao modelo extrapolar para sequ√™ncias mais longas do que as vistas durante o treinamento.

### 3. LlamaMLP

<image: Um diagrama do fluxo de dados atrav√©s da camada MLP, mostrando as transforma√ß√µes lineares e a ativa√ß√£o SwiGLU>

A classe `LlamaMLP` implementa o Perceptron de M√∫ltiplas Camadas com a fun√ß√£o de ativa√ß√£o SwiGLU [1].

```python
class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        if self.config.pretraining_tp > 1:
            slice = self.intermediate_size // self.config.pretraining_tp
            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)
            up_proj_slices = self.up_proj.weight.split(slice, dim=0)
            down_proj_slices = self.down_proj.weight.split(slice, dim=1)

            gate_proj = torch.cat(
                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1
            )
            up_proj = torch.cat(
                [F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1
            )

            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)
            down_proj = [
                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)
            ]
            down_proj = sum(down_proj)
        else:
            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj
```

Explica√ß√£o detalhada:
1. A classe define tr√™s proje√ß√µes lineares: `gate_proj`, `up_proj`, e `down_proj`.
2. O m√©todo `forward` implementa a fun√ß√£o SwiGLU: $\text{SwiGLU}(x) = \text{act_fn}(\text{gate_proj}(x)) * \text{up_proj}(x)$
3. O resultado √© ent√£o projetado de volta para o espa√ßo original usando `down_proj`.
4. H√° suporte para tensor parallelism (`pretraining_tp > 1`), onde as opera√ß√µes s√£o divididas em fatias para processamento paralelo.

> ‚úîÔ∏è **Destaque**: O uso de SwiGLU permite um fluxo de gradiente mais eficiente e pode levar a um melhor desempenho em compara√ß√£o com ativa√ß√µes tradicionais como ReLU.

#### Quest√µes T√©cnicas:
1. Como o tensor parallelism implementado no `LlamaMLP` contribui para a efici√™ncia do treinamento em hardware distribu√≠do?
2. Compare a complexidade computacional e os benef√≠cios da SwiGLU com outras fun√ß√µes de ativa√ß√£o comuns em redes neurais profundas.

### 4. LlamaAttention

<image: Um diagrama detalhado do mecanismo de aten√ß√£o, mostrando as transforma√ß√µes de query, key, value, e o produto de aten√ß√£o>

A classe `LlamaAttention` implementa o mecanismo de aten√ß√£o multi-cabe√ßa do LLaMA [1].

```python
class LlamaAttention(nn.Module):
    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                             f" and `num_heads`: {self.num_heads}).")
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)

    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        query_states = self._shape(self.q_proj(hidden_states), q_len, bsz)
        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)

        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            kv_seq_len += past_key_value[0].shape[-2]
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            # reuse k, v, self_attention
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)

        past_key_value = (key_states, value_states) if use_cache else None

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
                )
            attn_weights = attn_weights + attention_mask

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)

        attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value
```

Explica√ß√£o detalhada:
1. O construtor inicializa as proje√ß√µes lineares para query, key, value e output, al√©m do embedding rotacional.
2. O m√©todo `forward`:
   - Projeta os hidden states para query, key e value.
   - Aplica os embeddings posicionais rotativos.
   - Lida com o caching de past key/value states para gera√ß√£o eficiente.
   - Calcula os pesos de aten√ß√£o e aplica o mascaramento, se fornecido.
   - Realiza o produto de aten√ß√£o e projeta o resultado de volta para o espa√ßo original.

> ‚ùó **Ponto de Aten√ß√£o**: O uso de `repeat_kv` permite economia de par√¢metros ao usar menos cabe√ßas de key/value do que cabe√ßas de query.

#### Quest√µes T√©cnicas:
1. Como o mecanismo de group-query attention (onde `num_key_value_heads < num_heads`) afeta a efici√™ncia e a capacidade do modelo?
2. Discuta as implica√ß√µes de performance e precis√£o ao realizar o softmax em fp32 e depois converter de volta para o dtype original.

### 5. LlamaDecoderLayer

<image: Um diagrama de fluxo mostrando a sequ√™ncia de opera√ß√µes em uma camada do decoder LLaMA, incluindo normaliza√ß√£o, aten√ß√£o e MLP>

A classe `LlamaDecoderLayer` representa uma √∫nica camada do decoder do LLaMA, combinando aten√ß√£o e MLP [1].

```python
class LlamaDecoderLayer(nn.Module):
    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.self_attn = LlamaAttention(config=config)
        self.mlp = LlamaMLP(config)
        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs
```

Explica√ß√£o detalhada:
1. O construtor inicializa os componentes principais: aten√ß√£o, MLP e camadas de normaliza√ß√£o.
2. O m√©todo `forward`:
   - Aplica normaliza√ß√£o de entrada.
   - Processa os hidden states atrav√©s da camada de aten√ß√£o.
   - Adiciona uma conex√£o residual.
   - Aplica normaliza√ß√£o p√≥s-aten√ß√£o.
   - Processa atrav√©s do MLP.
   - Adiciona outra conex√£o residual.

> ‚úîÔ∏è **Destaque**: O uso de conex√µes residuais e normaliza√ß√£o pr√©-camada (pre-layer normalization) √© crucial para a estabilidade do treinamento em redes profundas.

#### Quest√µes T√©cnicas:
1. Explique a import√¢ncia das conex√µes residuais na arquitetura do LLaMA e como elas facilitam o treinamento de redes muito profundas.
2. Compare a abordagem de normaliza√ß√£o pr√©-camada utilizada no LLaMA com a normaliza√ß√£o p√≥s-camada. Quais s√£o as vantagens em termos de estabilidade e converg√™ncia do treinamento?

### 6. LlamaModel

<image: Uma representa√ß√£o visual da arquitetura completa do modelo LLaMA, mostrando a pilha de camadas do decoder>

A classe `LlamaModel` implementa o modelo LLaMA completo, combinando todas as camadas do decoder [1].

```python
class LlamaModel(LlamaPreTrainedModel):
    def __init__(self, config: LlamaConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])
        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.gradient_checkpointing = False
        # Initialize weights and apply final processing
        self.post_init()

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        # ... (c√≥digo omitido para brevidade)

        # Embedding
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        hidden_states = inputs_embeds

        # Decoder
        for idx, decoder_layer in enumerate(self.layers):
            # ... (c√≥digo de aplica√ß√£o de cada camada do decoder)

        hidden_states = self.norm(hidden_states)

        # ... (c√≥digo de prepara√ß√£o da sa√≠da)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=next_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )
```

Explica√ß√£o detalhada:
1. O construtor inicializa a camada de embedding, a lista de camadas do decoder e a normaliza√ß√£o final.
2. O m√©todo `forward`:
   - Converte os input_ids em embeddings ou usa inputs_embeds fornecidos.
   - Itera atrav√©s das camadas do decoder, aplicando cada uma sequencialmente.
   - Aplica a normaliza√ß√£o final aos hidden states.
   - Prepara e retorna a sa√≠da, incluindo opcionalmente estados ocultos intermedi√°rios e pesos de aten√ß√£o.

> üí° **Insight**: A arquitetura do LLaMA √© essencialmente uma pilha de camadas id√™nticas, permitindo um scaling eficiente do modelo atrav√©s do aumento do n√∫mero de camadas.

#### Quest√µes T√©cnicas:
1. Como o LLaMA lida com sequ√™ncias de diferentes comprimentos em um batch? Discuta as implica√ß√µes para efici√™ncia computacional e uso de mem√≥ria.
2. Explique o prop√≥sito e o funcionamento do gradient checkpointing no contexto do treinamento de modelos de linguagem de grande escala como o LLaMA.

### 7. LlamaForCausalLM

<image: Um diagrama mostrando a estrutura do modelo LlamaForCausalLM, destacando a adi√ß√£o da camada de language modeling head sobre o LlamaModel base>

A classe `LlamaForCausalLM` estende o `LlamaModel` para realizar modelagem de linguagem causal [1].

```python
class LlamaForCausalLM(LlamaPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        # ... (c√≥digo omitido para brevidade)

        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))

        # ... (c√≥digo de prepara√ß√£o da sa√≠da)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
```

Explica√ß√£o detalhada:
1. O construtor inicializa o `LlamaModel` base e adiciona uma camada linear (`lm_head`) para projetar os hidden states para o espa√ßo do vocabul√°rio.
2. O m√©todo `forward`:
   - Processa os inputs atrav√©s do modelo base.
   - Aplica a `lm_head` para obter logits sobre o vocabul√°rio.
   - Se fornecidos labels, calcula a loss de modelagem de linguagem causal.

> ‚ö†Ô∏è **Nota Importante**: O c√°lculo da loss envolve um deslocamento dos logits e labels para prever o pr√≥ximo token, alinhando-se com o objetivo da modelagem de linguagem causal.

#### Quest√µes T√©cnicas:
1. Como o LLaMA lida com o trade-off entre precis√£o e efici√™ncia computacional ao calcular logits para todo o vocabul√°rio em cada passo?
2. Discuta as implica√ß√µes de usar uma camada linear sem bias como `lm_head`. Quais s√£o as vantagens e potenciais limita√ß√µes desta abordagem?

### Conclus√£o

A implementa√ß√£o do LLaMA demonstra uma arquitetura de transformer avan√ßada e otimizada, incorporando t√©cnicas como RoPE, normaliza√ß√£o RMS e aten√ß√£o eficiente. Cada componente √© cuidadosamente projetado para maximizar a performance e a escalabilidade do modelo.

A compreens√£o profunda desta implementa√ß√£o √© crucial para cientistas de dados e engenheiros de ML trabalhando com modelos de linguagem de grande escala, permitindo-lhes otimizar, adaptar e estender esses modelos para diversas aplica√ß√µes de PLN.

### Quest√µes Avan√ßadas Adicionais

1. Proponha e discuta poss√≠veis modifica√ß√µes na arquitetura do LLaMA que poderiam melhorar sua efici√™ncia computacional sem sacrificar significativamente o desempenho.

2. Compare a implementa√ß√£o do LLaMA com outros modelos de linguagem de grande escala, como GPT-3 ou T5. Quais s√£o as principais diferen√ßas arquiteturais e como elas impactam o desempenho e a efici√™ncia?

3. Discuta as estrat√©gias de paralelismo (modelo, dados e pipeline) que poderiam ser aplicadas ao LLaMA para treinamento e infer√™ncia em clusters de GPUs ou TPUs.

4. Analise os desafios potenciais na fine-tuning do LLaMA para tarefas espec√≠ficas, considerando sua arquitetura e escala. Que t√©cnicas poderiam ser empregadas para superar esses desafios?

5. Elabore sobre as implica√ß√µes √©ticas e de privacidade do uso de modelos como o LLaMA em aplica√ß√µes do mundo real, considerando aspectos como vi√©s, interpretabilidade e seguran√ßa dos dados.

### Refer√™ncias

[1] "Este c√≥digo implementa o modelo LLaMA, um modelo de linguagem baseado em transformers desenvolvido pela Meta. Ele √© constru√≠do sobre a biblioteca GPT-NeoX e as implementa√ß√µes GPT-NeoX e OPT dentro da biblioteca Hugging Face Transformers. O c√≥digo foi modificado para acomodar pequenas diferen√ßas arquitet√¥nicas em compara√ß√£o com o GPT-NeoX e OPT usados pela Meta AI." (Excerto do documento fornecido)