## Chapter 5: REFT: Reasoning with Reinforced Fine-Tuning

### 5. Introduction

- **Reasoning Enhancement with CoT:**  Introducing Chain-of-Thought (CoT) annotations as a technique to improve reasoning capabilities of Large Language Models (LLMs) through supervised fine-tuning (SFT).
- **Limitations of SFT:**  The need for a more powerful fine-tuning approach due to the limitations of SFT, which relies solely on the given CoT data and lacks generalization ability.
- **Reinforced Fine-Tuning (ReFT):**  Proposing ReFT as a novel approach that utilizes reinforcement learning (RL) to enhance the generalizability of LLMs for reasoning, particularly in math problem-solving.
- **Multi-Path Reasoning:**  ReFT explores multiple valid reasoning paths (CoTs) for a given question, providing richer supervision signals.
- **Online RL with PPO:**  Employing the PPO algorithm for online RL training, where rewards are derived directly from the ground-truth answers, eliminating the need for a separate reward model.
- **Performance and Generalization:**  Demonstrating that ReFT significantly outperforms SFT and other baselines on various math datasets, showcasing its superior generalization ability.
- **Benefits of Majority Voting and Reranking:**  Showing that ReFT benefits from inference-time techniques like majority voting and reward model reranking, further boosting performance.

### 5. Related Work

- **Math Problem Solving with CoT:**  Reviewing existing approaches to math problem-solving, focusing on CoT prompt design, data engineering, and methods for improving CoT quality and quantity.
- **Reinforcement Learning for Language Models:**  Highlighting the use of PPO and other RL algorithms for aligning LLMs with human preferences.
- **Reward Model Reranking:**  Discussing the use of reward models for reranking CoT solutions, improving performance over SFT and majority voting.

### 5. Method

- **Natural Language CoT (N-CoT) and Program-based CoT (P-CoT):**  Defining the two types of CoT used in the paper and their representation.
- **Reinforced Fine-Tuning (ReFT):**  Presenting the two-stage ReFT process:
  - **Warm-up Stage:**  Supervised fine-tuning to equip the model with basic problem-solving skills.
  - **Reinforcement Learning Stage:**  Online RL training using PPO, exploring multiple reasoning paths and learning from feedback based on answer correctness.
- **State Representation:**  Defining the state $s_t$ as the concatenation of the question and previously generated tokens.
- **Action Representation:**  Representing actions as tokens in the vocabulary.
- **Reward Function:**  Defining the reward function based on the correctness of the answer extracted from the generated CoT.
- **KL-Divergence Constraint:**  Using a KL-divergence term to encourage the policy to remain close to the initial model and prevent divergence.

### 5. Experiments

- **Datasets:**  Introducing the datasets used in the experiments: GSM8K, SVAMP, and MathQA.
- **Baselines:**  Defining the baseline methods for comparison: SFT, Offline Self-Training, and Online Self-Training.
- **Experimental Setup:**  Describing the foundation models (Galactica, CodeLLAMA), hardware, training hyperparameters, and evaluation metrics.
- **Reward Model Reranking:**  Explaining the process of training a reward model to predict the correctness of CoTs for reranking.
- **Experimental Results:**
  - **ReFT Outperforms SFT:**  Demonstrating significant performance improvements of ReFT over SFT on all datasets.
  - **Reward Hacking in MathQA:**  Analyzing the performance of ReFT on MathQAMCQ, highlighting the potential for reward hacking due to the limited answer space.
  - **Benefits of Majority Voting and Reranking:**  Showing that ReFT further benefits from majority voting and reranking techniques.
  - **Comparison with Existing Open-Source Models:**  Demonstrating that ReFT achieves comparable or superior performance to other open-source models, including WizardMath, MathCoder, and MAmmoTH-Coder.
  - **Experiments with Smaller Models:**  Evaluating ReFT with smaller models, indicating that even small models can benefit from exploration and reinforcement learning.

### 5. Analysis

- **Generalization Ability of ReFT:**  Explaining how ReFT's exploration of multiple reasoning paths contributes to its superior generalization ability.
- **Qualitative Evaluation:**  Conducting human evaluations to assess the quality of reasoning paths generated by SFT, Warmup checkpoint, and ReFT.
- **Warm-up Step Ablation Study:**  Investigating the impact of the number of warm-up epochs on ReFT's performance.

### 5. Conclusion

- **Contributions of REFT:**  Summarizing the key contributions of the paper, including:
  - Introducing ReFT as a novel reinforcement learning-based fine-tuning approach for math problem-solving.
  - Demonstrating ReFT's superior performance and generalization ability compared to SFT.
  - Showcasing the compatibility of ReFT with majority voting and reranking techniques.
  - Outperforming other open-source models on GSM8K.

### 5. Future Work

- **Offline RL and Warm-up-Free Methods:**  Exploring the use of offline RL and investigating the possibility of eliminating the warm-up stage for improved efficiency.
- **Process-based Reward Models:**  Investigating the potential of using process-based reward models for enhanced performance.
- **Generalization to Other Reasoning Tasks:**  Applying ReFT to a broader range of reasoning tasks where CoT can be formalized.

### 5. Limitations

- **Training Efficiency:**  Acknowledging the potential for increased training time compared to SFT due to the RL optimization process.
- **Reward Hacking:**  Identifying the vulnerability of the reward function to reward hacking in limited answer spaces.

This comprehensive summary provides a thorough overview of the REFT method, its theoretical basis, experimental results, and future research directions. It aims to help readers understand the significance of this approach and its potential impact on the field of LLM reasoning.