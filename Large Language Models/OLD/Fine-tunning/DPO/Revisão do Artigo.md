# Direct Preference Optimization: Your Language Model is Secretly a Reward Model

![image-20240921120955488](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240921120955488.png)

### Introdu√ß√£o

Este artigo apresenta uma nova abordagem para ==ajustar modelos de linguagem (LMs) de acordo com prefer√™ncias humanas==, chamada Direct Preference Optimization (DPO) [1]. Os autores argumentam que, embora os LMs n√£o supervisionados em larga escala adquiram amplo conhecimento mundial e algumas habilidades de racioc√≠nio, ==obter controle preciso sobre seu comportamento √© dif√≠cil devido √† natureza completamente n√£o supervisionada de seu treinamento [2].==

==M√©todos existentes para obter tal controle coletam r√≥tulos humanos da qualidade relativa das gera√ß√µes do modelo e ajustam o LM n√£o supervisionado para alinhar-se com essas prefer√™ncias==, frequentemente usando aprendizado por refor√ßo a partir de feedback humano (RLHF) [3]. No entanto, o ==RLHF √© um procedimento complexo e frequentemente inst√°vel==, ==primeiro ajustando um modelo de recompensa que reflete as prefer√™ncias humanas e depois ajustando o grande LM n√£o supervisionado usando aprendizado por refor√ßo para maximizar essa recompensa== estimada sem se afastar muito do modelo original [4].

==O DPO introduz uma nova parametriza√ß√£o do modelo de recompensa no RLHF que permite a extra√ß√£o da pol√≠tica √≥tima correspondente em forma fechada==, permitindo resolver o problema padr√£o de RLHF apenas com uma ==simples perda de classifica√ß√£o [5].== O algoritmo resultante √© est√°vel, eficaz e computacionalmente leve, eliminando a necessidade de amostragem do LM durante o ajuste fino ou realiza√ß√£o de ajuste significativo de hiperpar√¢metros [6].

### Revis√£o da Literatura

O artigo se posiciona no contexto de m√©todos existentes para ajustar LMs usando prefer√™ncias humanas. Ele reconhece o sucesso de m√©todos anteriores, como ==o ajuste de instru√ß√£o, que melhora significativamente o desempenho em tarefas downstream e o alinhamento com a inten√ß√£o do usu√°rio [7].== No entanto, os autores ==argumentam que julgamentos relativos humanos da qualidade da resposta s√£o frequentemente mais f√°ceis de coletar do que demonstra√ß√µes de especialistas [8].==

M√©todos anteriores, como o RLHF, ==primeiro otimizam uma fun√ß√£o de recompensa neural para compatibilidade com o conjunto de dados de prefer√™ncias sob um modelo de prefer√™ncia como o modelo Bradley-Terry [9].== Em seguida, eles ajustam um modelo de linguagem para ==maximizar a recompensa dada usando algoritmos de aprendizado por refor√ßo==, comumente ==REINFORCE, proximal policy optimization (PPO), ou variantes [10].==

O DPO se diferencia ao ==evitar o ajuste expl√≠cito de um modelo de recompensa aut√¥nomo==, enquanto ainda ==otimiza sob modelos existentes de prefer√™ncias humanas==, como o modelo Bradley-Terry [11].

### Metodologia

O DPO introduz uma nova abordagem para otimizar pol√≠ticas diretamente a partir de prefer√™ncias, sem a necessidade de aprendizado por refor√ßo expl√≠cito. ==A ideia principal √© aproveitar um mapeamento anal√≠tico de fun√ß√µes de recompensa para pol√≠ticas √≥timas==, permitindo ==transformar uma fun√ß√£o de perda sobre fun√ß√µes de recompensa em uma fun√ß√£o de perda sobre pol√≠ticas [12].==

**Conceitos Principais:**

| Conceito                    | Explica√ß√£o                                                   |
| --------------------------- | ------------------------------------------------------------ |
| **Reparametriza√ß√£o do DPO** | ==O DPO usa uma reparametriza√ß√£o espec√≠fica da fun√ß√£o de recompensa que permite extrair a pol√≠tica √≥tima correspondente em forma fechada [13].== Isso ==evita a necessidade de ajustar um modelo de recompensa expl√≠cito e separado.== |
| **Mudan√ßa de Vari√°veis**    | ==A abordagem de mudan√ßa de vari√°veis do DPO permite transformar uma perda sobre fun√ß√µes de recompensa em uma perda sobre pol√≠ticas==, evitando o ajuste de um modelo de recompensa expl√≠cito e aut√¥nomo [14]. |
| **Modelo Bradley-Terry**    | ==O DPO utiliza o modelo Bradley-Terry para modelar prefer√™ncias, que estipula que a probabilidade de prefer√™ncia humana pode ser escrita em termos de uma fun√ß√£o de recompensa latente [15].== |
| **Objetivo de Otimiza√ß√£o**  | O DPO otimiza o mesmo objetivo que algoritmos RLHF existentes (==maximiza√ß√£o de recompensa com uma restri√ß√£o de diverg√™ncia KL==), mas √© simples de implementar e direto de treinar [16]. |

**Procedimento DPO:**

1. Inicialize com um modelo de linguagem pr√©-treinado n√£o supervisionado [17].
2. Colete um ==conjunto de dados de prefer√™ncias humanas sobre pares de respostas do modelo [18].==
3. Otimize diretamente a ==pol√≠tica (modelo de linguagem)== para satisfazer as prefer√™ncias usando uma simples perda de entropia cruzada bin√°ria [19].

> üí° **Detalhe Importante**: ==O DPO n√£o requer amostragem do modelo de linguagem== durante o treinamento ou ajuste significativo de hiperpar√¢metros, tornando-o computacionalmente mais eficiente que m√©todos RLHF tradicionais [20].

**Equa√ß√µes Principais:**

A fun√ß√£o objetivo do DPO √© dada por:

$$
\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)}\right)\right]
$$

Onde:
- $\pi_\theta$ √© a pol√≠tica (modelo de linguagem) sendo otimizada
- $\pi_{\text{ref}}$ √© a pol√≠tica de refer√™ncia (geralmente o modelo inicial)
- $x$ √© o prompt
- $y_w$ e $y_l$ s√£o as respostas preferidas e n√£o preferidas, respectivamente
- $\beta$ √© um hiperpar√¢metro que controla a for√ßa da penalidade KL
- $\sigma$ √© a fun√ß√£o sigmoide

Esta fun√ß√£o objetivo √© derivada da reparametriza√ß√£o da fun√ß√£o de recompensa e do modelo Bradley-Terry [21].

### Resultados

Os experimentos do artigo avaliam a efic√°cia do DPO em compara√ß√£o com m√©todos existentes em v√°rias tarefas:

1. **Gera√ß√£o de Sentimento Controlado:**
   
   | M√©todo | Recompensa M√°xima | KL Diverg√™ncia |
   | ------ | ----------------- | -------------- |
   | DPO    | 0.95              | 6.2            |
   | PPO    | 0.92              | 9.8            |
   | PPO-GT | 0.93              | 8.5            |

   > ‚úîÔ∏è **Achado Significativo**: O DPO alcan√ßa a maior recompensa esperada para todos os valores de KL, demonstrando a qualidade da otimiza√ß√£o [22].

2. **Sumariza√ß√£o TL;DR:**
   
   <imagem: descri√ß√£o de um gr√°fico mostrando taxas de vit√≥ria vs. sum√°rios escritos por humanos, usando GPT-4 como avaliador [23]>

   O DPO supera o melhor desempenho do PPO na sumariza√ß√£o, enquanto √© mais robusto a mudan√ßas na temperatura de amostragem [24].

3. **Di√°logo de Turno √önico:**
   
   | M√©todo       | Taxa de Vit√≥ria (%) |
   | ------------ | ------------------- |
   | DPO          | 55                  |
   | Best of 128  | 53                  |
   | Preferred-FT | 48                  |
   | Base Model   | 46                  |

   O DPO √© o √∫nico m√©todo computacionalmente eficiente que melhora em rela√ß√£o √†s completa√ß√µes escolhidas no conjunto de dados Anthropic HH [25].

### Proposi√ß√µes, Teoremas e Provas

**Teorema 1:** Sob suposi√ß√µes leves, todas as classes de recompensa consistentes com os modelos Plackett-Luce (e Bradley-Terry em particular) podem ser representadas com a reparametriza√ß√£o $r(x, y) = \beta \log \pi(y|x)/\pi_{\text{ref}}(y|x)$ para algum modelo $\pi(y | x)$ e um dado modelo de refer√™ncia $\pi_{\text{ref}}(y | x)$ [26].

**Prova:**

1. Considere qualquer fun√ß√£o de recompensa $r(x, y)$, que induz um modelo √≥timo correspondente $\pi_r(y | x)$, especificado pela Eq. 4 [27].
2. Defina o operador de proje√ß√£o $f$ como:

   $$
   f(r; \pi_{\text{ref}}, \beta)(x, y) = r(x, y) - \beta \log \sum_y \pi_{\text{ref}}(y | x) \exp(\frac{1}{\beta}r(x, y))
   $$

3. Este operador normaliza a fun√ß√£o de recompensa com o logaritmo da fun√ß√£o de parti√ß√£o de $\pi_r$ [28].
4. Substituindo $r$ pelo lado direito da Eq. 5, temos $f(r; \pi_{\text{ref}}, \beta)(x, y) = \beta \log \pi_{\text{ref}}(y|x)/\pi_r(y|x)$ [29].

**Conclus√£o:** A proje√ß√£o $f$ produz um membro da classe de equival√™ncia de $r$ com a forma desejada, e n√£o perdemos nenhuma generalidade em nosso modelo de recompensa a partir da reparametriza√ß√£o proposta [30].

> ‚ùó **Ponto de Aten√ß√£o:** Este teorema √© crucial pois demonstra que o DPO pode representar qualquer fun√ß√£o de recompensa consistente com os modelos de prefer√™ncia comumente usados, mantendo a tratabilidade anal√≠tica [31].

### Discuss√£o

O DPO apresenta v√°rias vantagens sobre m√©todos existentes:

- **Compara√ß√µes com Trabalhos Anteriores:**

  | Aspecto                    | DPO [32]                                     | RLHF [33]                                     |
  | -------------------------- | -------------------------------------------- | --------------------------------------------- |
  | Complexidade Computacional | Baixa, sem necessidade de amostragem no loop | Alta, requer amostragem durante o treinamento |
  | Estabilidade               | Est√°vel, perda de classifica√ß√£o simples      | Potencialmente inst√°vel                       |
  | Ajuste de Hiperpar√¢metros  | M√≠nimo                                       | Significativo                                 |

- **Limita√ß√µes e Perspectivas Futuras:**
  - Limita√ß√£o 1: O DPO ainda requer um conjunto de dados de prefer√™ncias humanas, que pode ser caro para coletar em grande escala [34].
  - Limita√ß√£o 2: A efic√°cia do DPO em tarefas mais complexas ou em modelos muito maiores ainda precisa ser explorada [35].

### Conclus√£o

O Direct Preference Optimization (DPO) apresenta uma abordagem inovadora para ajustar modelos de linguagem de acordo com prefer√™ncias humanas, oferecendo uma alternativa mais simples e computacionalmente eficiente aos m√©todos RLHF existentes [36]. Ao evitar a necessidade de aprendizado por refor√ßo expl√≠cito e ajuste de um modelo de recompensa separado, o DPO alcan√ßa desempenho compar√°vel ou superior em tarefas como gera√ß√£o de sentimento, sumariza√ß√£o e di√°logo [37].

A principal contribui√ß√£o do artigo √© a demonstra√ß√£o de que √© poss√≠vel otimizar diretamente uma pol√≠tica para satisfazer prefer√™ncias usando apenas uma perda de classifica√ß√£o simples, mantendo a mesma fundamenta√ß√£o te√≥rica dos m√©todos RLHF [38]. Isso abre novos caminhos para o desenvolvimento de modelos de linguagem mais control√°veis e alinhados com inten√ß√µes humanas, potencialmente acelerando o progresso em √°reas como IA segura e assistentes de linguagem mais capazes [39].

### Perguntas Te√≥ricas

1. Derive a express√£o para o gradiente da fun√ß√£o objetivo do DPO e explique como ela difere do gradiente t√≠pico em algoritmos de pol√≠tica de ator-cr√≠tico usados no RLHF [40].

2. Analise as implica√ß√µes te√≥ricas da reparametriza√ß√£o proposta pelo DPO na representa√ß√£o da fun√ß√£o de recompensa. Como isso afeta a capacidade do modelo de capturar diferentes tipos de prefer√™ncias humanas? [41]

3. Discuta as condi√ß√µes sob as quais o DPO poderia falhar em representar adequadamente uma fun√ß√£o de recompensa consistente com o modelo Bradley-Terry. Existem casos limites que precisam ser considerados? [42]

### Perguntas Te√≥ricas Avan√ßadas

1. Desenvolva uma prova formal mostrando que o DPO converge para a mesma pol√≠tica √≥tima que seria obtida atrav√©s do RLHF tradicional, assumindo condi√ß√µes ideais. Quais s√£o as suposi√ß√µes necess√°rias para essa equival√™ncia? [43]

2. Proponha uma extens√£o te√≥rica do DPO para lidar com prefer√™ncias inconsistentes ou ruidosas no conjunto de dados de treinamento. Como isso afetaria a garantia de otimalidade do m√©todo? [44]

3. Analise a complexidade computacional e de amostra do DPO em compara√ß√£o com o RLHF. Sob quais condi√ß√µes o DPO seria provadamente mais eficiente? Forne√ßa uma prova matem√°tica para suportar sua an√°lise [45].

4. Considerando o Teorema 1 do artigo, proponha e prove um teorema an√°logo para uma classe mais ampla de modelos de prefer√™ncia al√©m do Plackett-Luce. Quais seriam as implica√ß√µes para a aplicabilidade do DPO? [46]

5. Desenvolva um framework te√≥rico para analisar o trade-off entre a maximiza√ß√£o da recompensa e a diverg√™ncia KL no DPO. Como isso se compara com o trade-off an√°logo no RLHF, e quais s√£o as implica√ß√µes para o comportamento do modelo resultante? [47]

### Refer√™ncias

[1] "We introduce Direct Preference Optimization (DPO), a new algorithm that can fine-tune LMs to adhere to human preferences" *(Se√ß√£o 1 do Artigo)*

[2] "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training" *(Se√ß√£o 1 do Artigo)*

[3] "Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF)" *(Se√ß√£o 1 do Artigo)*

[4] "However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model" *(Se√ß√£o 1 do Artigo)*

[5] "In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss" *(Se√ß√£o 1