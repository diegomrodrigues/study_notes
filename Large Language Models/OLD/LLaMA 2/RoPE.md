## RoFormer: Transformador Aprimorado com Rotary Position Embedding

![image-20240917120717832](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240917120717832.png)

### Introdu√ß√£o

O artigo "RoFormer: Enhanced Transformer with Rotary Position Embedding" apresenta uma inova√ß√£o significativa na arquitetura Transformer, focando na melhoria do encoding de informa√ß√µes posicionais [1]. Os autores abordam uma limita√ß√£o fundamental dos modelos Transformer: ==a falta de sensibilidade √† ordem sequencial das palavras, que √© crucial para a compreens√£o da linguagem natural [2].==

O objetivo principal do estudo √© introduzir um novo m√©todo de embedding posicional, denominado Rotary Position Embedding (RoPE), que ==codifica informa√ß√µes de posi√ß√£o absoluta usando uma matriz de rota√ß√£o e, simultaneamente, incorpora depend√™ncia de posi√ß√£o relativa expl√≠cita na formula√ß√£o de self-attention [3].== Este m√©todo visa superar as limita√ß√µes das abordagens existentes de encoding posicional, oferecendo ==propriedades valiosas como flexibilidade de comprimento de sequ√™ncia e decaimento da depend√™ncia inter-token com o aumento das dist√¢ncias relativas [4].==

> üí° **Contribui√ß√£o Chave**: O RoFormer prop√µe uma nova forma de incorporar informa√ß√µes posicionais em modelos Transformer, potencialmente melhorando o desempenho em tarefas de processamento de linguagem natural, especialmente para textos longos.

### Revis√£o da Literatura

O artigo posiciona o RoFormer no contexto de pesquisas existentes sobre encoding posicional em arquiteturas Transformer. Os autores identificam duas principais abordagens na literatura:

1. **Encoding de posi√ß√£o absoluta**:
   - Gerado por fun√ß√µes pr√©-definidas [5]
   - ==Encoding de posi√ß√£o absoluta trein√°vel [6]==

2. **Encoding de posi√ß√£o relativa**:
   - ==Incorpora√ß√£o de informa√ß√µes de posi√ß√£o relativa no mecanismo de aten√ß√£o [7]==

Os autores argumentam que, apesar da efic√°cia dessas abordagens, ==elas geralmente adicionam informa√ß√µes de posi√ß√£o √† representa√ß√£o do contexto, tornando-as inadequadas para arquiteturas de self-attention linear [8].==

| Abordagem                    | Vantagens                                      | Limita√ß√µes                                        |
| ---------------------------- | ---------------------------------------------- | ------------------------------------------------- |
| Encoding de posi√ß√£o absoluta | Simplicidade, f√°cil implementa√ß√£o              | Pode n√£o capturar rela√ß√µes de longo alcance       |
| Encoding de posi√ß√£o relativa | Captura melhor rela√ß√µes entre tokens distantes | Complexidade computacional, dif√≠cil implementa√ß√£o |
| RoPE (proposto)              | Flexibilidade, efic√°cia em textos longos       | A ser determinado pela pesquisa futura            |

==O RoFormer se distingue ao propor uma abordagem que codifica tanto a posi√ß√£o absoluta quanto a relativa, mantendo a compatibilidade com self-attention linear [9].==

### Metodologia

#### Modelos Te√≥ricos e Conceituais:

1. **Rotary Position Embedding (RoPE)**:

   O RoPE √© a inova√ß√£o central do RoFormer. ==Ele codifica a posi√ß√£o absoluta com uma matriz de rota√ß√£o e incorpora depend√™ncia de posi√ß√£o relativa expl√≠cita na formula√ß√£o de self-attention [10].==

   $$
   f_{q,k}(x_m, m) = R_{\Theta,m}^d W_{q,k}x_m
   $$

   Onde:
   - $f_{q,k}$ s√£o as fun√ß√µes de transforma√ß√£o para queries e keys
   - $x_m$ √© o embedding da palavra na posi√ß√£o $m$
   - $R_{\Theta,m}^d$ √© a matriz de rota√ß√£o
   - $W_{q,k}$ s√£o as matrizes de proje√ß√£o para queries e keys

2. **Matriz de Rota√ß√£o**:

   A matriz de rota√ß√£o $R_{\Theta,m}^d$ √© definida como [11]:

   $$
   R_{\Theta,m}^d = \begin{pmatrix}
   \cos m\theta_1 & -\sin m\theta_1 & 0 & 0 & \cdots \\
   \sin m\theta_1 & \cos m\theta_1 & 0 & 0 & \cdots \\
   0 & 0 & \cos m\theta_2 & -\sin m\theta_2 & \cdots \\
   0 & 0 & \sin m\theta_2 & \cos m\theta_2 & \cdots \\
   \vdots & \vdots & \vdots & \vdots & \ddots
   \end{pmatrix}
   $$

   Com $\Theta = \{\theta_i = 10000^{-2(i-1)/d}, i \in [1, 2, ..., d/2]\}$

> ‚ö†Ô∏è **Ponto Crucial**: ==A escolha dos valores de $\theta_i$ √© fundamental para garantir a propriedade de decaimento de longo prazo do RoPE.==

#### Procedimentos Experimentais:

Os autores conduziram experimentos em v√°rias tarefas para validar a efic√°cia do RoFormer:

1. **Tradu√ß√£o Autom√°tica**: Utilizando o conjunto de dados WMT 2014 English-German [12].
2. **Pr√©-treinamento de Modelo de Linguagem**: Usando BookCorpus e Wikipedia Corpus [13].
3. **Fine-tuning em tarefas GLUE**: Incluindo MRPC, SST-2, QNLI, STS-B, QQP e MNLI [14].
4. **Performer com RoPE**: Implementa√ß√£o do RoPE no modelo Performer [15].
5. **Avalia√ß√£o em Dados Chineses**: Usando o conjunto de dados CAIL2019-SCM [16].

### RoPE com Aten√ß√£o Linear

A incorpora√ß√£o do **Rotary Position Embedding (RoPE)** em arquiteturas de aten√ß√£o linear representa um avan√ßo significativo na evolu√ß√£o dos modelos Transformer. ==O RoPE oferece uma maneira eficiente de incorporar informa√ß√µes posicionais atrav√©s de rota√ß√µes no espa√ßo vetorial, permitindo que o modelo capture rela√ß√µes posicionales de forma cont√≠nua e sem depender de embeddings posicionais expl√≠citos.== Nesta se√ß√£o, exploraremos em profundidade como o RoPE pode ser integrado aos mecanismos de aten√ß√£o linear, mantendo suas propriedades ben√©ficas enquanto reduz a complexidade computacional associada √† aten√ß√£o tradicional.

#### Formula√ß√£o Geral da Aten√ß√£o

A aten√ß√£o em modelos Transformer pode ser generalizada pela seguinte equa√ß√£o:

$$
\text{Aten√ß√£o}(\mathbf{Q}, \mathbf{K}, \mathbf{V})_m = \frac{\sum_{n=1}^N \text{sim}(\mathbf{q}_m, \mathbf{k}_n) \mathbf{v}_n}{\sum_{n=1}^N \text{sim}(\mathbf{q}_m, \mathbf{k}_n)}
$$

Onde:

- $\mathbf{q}_m \in \mathbb{R}^d$ √© a **query** na posi√ß√£o $m$.
- $\mathbf{k}_n \in \mathbb{R}^d$ √© a **key** na posi√ß√£o $n$.
- $\mathbf{v}_n \in \mathbb{R}^d$ √© o **value** na posi√ß√£o $n$.
- $\text{sim}(\mathbf{q}_m, \mathbf{k}_n)$ √© uma fun√ß√£o de **similaridade** que quantifica a compatibilidade entre $\mathbf{q}_m$ e $\mathbf{k}_n$.

==Na aten√ß√£o tradicional, a fun√ß√£o de similaridade √© definida como:==
$$
\text{sim}(\mathbf{q}_m, \mathbf{k}_n) = \exp\left(\frac{\mathbf{q}_m^\top \mathbf{k}_n}{\sqrt{d}}\right)
$$

Esta formula√ß√£o resulta em uma complexidade computacional de **$\mathcal{O}(N^2)$**, pois calcula-se a intera√ß√£o entre todas as pares poss√≠veis de queries e keys em uma sequ√™ncia de comprimento $N$.

#### Aten√ß√£o Linear

Para superar a limita√ß√£o de complexidade quadr√°tica, a aten√ß√£o linear reformula o mecanismo de aten√ß√£o de modo a permitir computa√ß√µes mais eficientes. ==A ideia central √© decompor a fun√ß√£o de similaridade em produtos internos de fun√ß√µes n√£o-negativas, permitindo a reordena√ß√£o das opera√ß√µes:==

$$
\text{Aten√ß√£o}(\mathbf{Q}, \mathbf{K}, \mathbf{V})_m = \frac{\phi(\mathbf{q}_m)^\top \left( \sum_{n=1}^N \varphi(\mathbf{k}_n) \odot \mathbf{v}_n \right)}{\phi(\mathbf{q}_m)^\top \left( \sum_{n=1}^N \varphi(\mathbf{k}_n) \right)}
$$

Onde:

- $\phi(\cdot)$ e $\varphi(\cdot)$ s√£o ==fun√ß√µes de mapeamento n√£o-negativas== aplicadas √†s queries e keys, respectivamente.
- ==$\odot$ representa o **produto de Hadamard** (produto elemento a elemento).==

**Abordagens Comuns:**

| Abordagem            | $\phi(\cdot)$                                       | $\varphi(\cdot)$                             | Caracter√≠sticas                                              |
| -------------------- | --------------------------------------------------- | -------------------------------------------- | ------------------------------------------------------------ |
| Katharopoulos et al. | $\phi(x) = \varphi(x) = \text{elu}(x) + 1$          | $\phi(x) = \varphi(x) = \text{elu}(x) + 1$   | Aproveita a propriedade associativa para efici√™ncia computacional |
| Shen et al.          | $\phi(\mathbf{q}_i) = \text{softmax}(\mathbf{q}_i)$ | $\varphi(\mathbf{k}_j) = \exp(\mathbf{k}_j)$ | Normaliza queries e keys separadamente para estabilidade num√©rica |

==Essas fun√ß√µes permitem que o c√°lculo da aten√ß√£o seja reescrito de forma que a complexidade computacional seja reduzida para **$\mathcal{O}(N)$**.==

#### Integra√ß√£o do RoPE com Aten√ß√£o Linear

A integra√ß√£o do RoPE em aten√ß√£o linear √© realizada aplicando as matrizes de rota√ß√£o √†s sa√≠das das fun√ß√µes n√£o-negativas $\phi(\cdot)$ e $\varphi(\cdot)$. A f√≥rmula modificada da aten√ß√£o linear com RoPE torna-se:

$$
\text{Aten√ß√£o}(\mathbf{Q}, \mathbf{K}, \mathbf{V})_m = \frac{\left( R_{\Theta, m} \phi(\mathbf{q}_m) \right)^\top \left( \sum_{n=1}^N R_{\Theta, n} \varphi(\mathbf{k}_n) \odot \mathbf{v}_n \right)}{\left( R_{\Theta, m} \phi(\mathbf{q}_m) \right)^\top \left( \sum_{n=1}^N R_{\Theta, n} \varphi(\mathbf{k}_n) \right)}
$$

Onde:

- $R_{\Theta, m}$ √© a **matriz de rota√ß√£o** gerada pelo RoPE para a posi√ß√£o $m$.
- As rota√ß√µes incorporam informa√ß√µes posicionais diretamente nas representa√ß√µes de queries e keys.

**Detalhes Importantes:**

- **Aplica√ß√£o das Rota√ß√µes:** As matrizes de rota√ß√£o s√£o aplicadas **ap√≥s** as fun√ß√µes $\phi(\cdot)$ e $\varphi(\cdot)$, garantindo que as propriedades n√£o-negativas dessas fun√ß√µes sejam mantidas.
- **Estabilidade Num√©rica:** O denominador √© cuidadosamente tratado para evitar divis√£o por zero e manter a estabilidade durante o treinamento.

#### An√°lise Te√≥rica

1. **Preserva√ß√£o da Norma:**
   - ==As matrizes de rota√ß√£o $R_{\Theta, m}$ s√£o **ortogonais**, o que significa que elas preservam a norma dos vetores.==
   - **Import√¢ncia:** A preserva√ß√£o da norma √© crucial para evitar mudan√ßas abruptas na escala dos vetores, o que poderia levar a instabilidades no treinamento.

2. **Compatibilidade com Fun√ß√µes N√£o-Negativas:**
   - Ao aplicar as rota√ß√µes ap√≥s $\phi(\cdot)$ e $\varphi(\cdot)$, asseguramos que as sa√≠das permane√ßam compat√≠veis com os requisitos da aten√ß√£o linear.
   - **Resultado:** Mant√©m-se a interpretabilidade dos pesos de aten√ß√£o e a efici√™ncia computacional.

3. **Complexidade Computacional:**
   - A integra√ß√£o do RoPE n√£o altera a complexidade linear da aten√ß√£o.
   - **Justificativa:** As opera√ß√µes de rota√ß√£o t√™m complexidade $\mathcal{O}(N)$ e podem ser implementadas de forma altamente eficiente.

#### Implica√ß√µes Pr√°ticas

1. **Flexibilidade de Implementa√ß√£o:**
   - O RoPE pode ser facilmente integrado em modelos existentes com modifica√ß√µes m√≠nimas.
   - **Benef√≠cio:** Facilita a ado√ß√£o em diversos cen√°rios sem a necessidade de reestruturar significativamente o modelo.

2. **Efici√™ncia em Processamento de Sequ√™ncias Longas:**
   - A combina√ß√£o de RoPE com aten√ß√£o linear √© especialmente √∫til para tarefas que envolvem sequ√™ncias longas, como processamento de texto longo ou s√©ries temporais.
   - **Vantagem:** Permite treinamento e infer√™ncia eficientes onde a aten√ß√£o tradicional seria computacionalmente invi√°vel.

3. **Interpreta√ß√£o dos Pesos de Aten√ß√£o:**
   - Embora os pesos resultantes n√£o sejam probabil√≠sticos no sentido estrito (n√£o somam exatamente 1), eles ainda fornecem uma medida relativa da import√¢ncia de cada valor $\mathbf{v}_n$ para a query $\mathbf{q}_m$.
   - **Considera√ß√£o:** Pode ser necess√°rio aplicar t√©cnicas adicionais de normaliza√ß√£o para certas aplica√ß√µes que requerem pesos probabil√≠sticos.

#### Considera√ß√µes Adicionais

- **Propriedades do RoPE:**
  - O RoPE permite que o modelo capture rela√ß√µes posicionales al√©m das adjac√™ncias imediatas, gra√ßas √† natureza cont√≠nua das rota√ß√µes.
  - **Efeito:** Melhora a capacidade do modelo de generalizar padr√µes posicionales em diferentes contextos.

- **Compara√ß√£o com Embeddings Posicionais Absolutos:**
  - Enquanto embeddings posicionais absolutos atribuem um vetor fixo a cada posi√ß√£o, o RoPE incorpora a posi√ß√£o atrav√©s de transforma√ß√µes aplicadas aos embeddings existentes.
  - **Benef√≠cio:** Reduz a necessidade de aprender ou armazenar embeddings adicionais, economizando recursos.

- **Desafios Potenciais:**
  - **Satura√ß√£o das Fun√ß√µes N√£o-Negativas:** Em alguns casos, as fun√ß√µes $\phi(\cdot)$ e $\varphi(\cdot)$ podem saturar, levando a gradientes pequenos.
    - **Solu√ß√£o:** Ajustes nas fun√ß√µes de ativa√ß√£o ou nos hiperpar√¢metros podem mitigar esse problema.
  - **Inicializa√ß√£o dos Par√¢metros:** A inicializa√ß√£o adequada das matrizes de rota√ß√£o e dos pesos do modelo √© essencial para um treinamento est√°vel.

#### Exemplifica√ß√£o Matem√°tica

Para ilustrar a efic√°cia da integra√ß√£o do RoPE com aten√ß√£o linear, considere um exemplo simplificado onde:

- $\phi(\mathbf{q}_m) = \mathbf{q}_m$ (identidade).
- $\varphi(\mathbf{k}_n) = \mathbf{k}_n$ (identidade).
- As matrizes de rota√ß√£o $R_{\Theta, m}$ s√£o definidas por uma fun√ß√£o senoidal baseada na posi√ß√£o.

Neste caso, a aten√ß√£o pode ser computada de forma eficiente, e as rota√ß√µes introduzem informa√ß√µes posicionales que permitem ao modelo diferenciar entre elementos em diferentes posi√ß√µes, mesmo quando os conte√∫dos s√£o semelhantes.

### Resultados

Os resultados dos experimentos demonstram a efic√°cia do RoFormer em v√°rias tarefas:

1. **Tradu√ß√£o Autom√°tica**:
   
   | Modelo      | BLEU Score |
   | ----------- | ---------- |
   | Transformer | 27.3       |
   | RoFormer    | 27.5       |

   O RoFormer superou o Transformer base na tarefa de tradu√ß√£o English-to-German [17].

2. **Pr√©-treinamento de Modelo de Linguagem**:

   <imagem: Um gr√°fico mostrando a perda de treinamento do BERT e do RoFormer ao longo do tempo, com o RoFormer convergindo mais rapidamente.>

   O RoFormer demonstrou converg√™ncia mais r√°pida durante o pr√©-treinamento em compara√ß√£o com o BERT [18].

3. **Fine-tuning em tarefas GLUE**:

   | Modelo   | MRPC | SST-2 | QNLI | STS-B | QQP  | MNLI(m/mm) |
   | -------- | ---- | ----- | ---- | ----- | ---- | ---------- |
   | BERT     | 88.9 | 93.5  | 90.5 | 85.8  | 71.2 | 84.6/83.4  |
   | RoFormer | 89.5 | 90.7  | 88.0 | 87.0  | 86.4 | 80.2/79.8  |

   O RoFormer superou o BERT em tr√™s das seis tarefas avaliadas [19].

4. **Performer com RoPE**:

   A incorpora√ß√£o do RoPE no Performer levou a uma converg√™ncia mais r√°pida e menor perda durante o pr√©-treinamento [20].

5. **Avalia√ß√£o em Dados Chineses**:

   | Modelo        | Valida√ß√£o | Teste  |
   | ------------- | --------- | ------ |
   | BERT-512      | 64.13%    | 67.77% |
   | WoBERT-512    | 64.07%    | 68.10% |
   | RoFormer-512  | 64.13%    | 68.29% |
   | RoFormer-1024 | 66.07%    | 69.79% |

   O RoFormer demonstrou desempenho superior em textos longos, superando modelos como BERT e WoBERT [21].

> ‚úîÔ∏è **Resultado Significativo**: O RoFormer mostrou melhorias consistentes em tarefas que envolvem textos longos, demonstrando a efic√°cia do RoPE em capturar depend√™ncias de longo alcance.

### Proposi√ß√µes, Teoremas e Provas

#### Proposi√ß√£o 1: Formula√ß√£o do RoPE em 2D

**Enunciado**: Em um espa√ßo bidimensional, o RoPE pode ser formulado como [22]:

$$
f_q(x_m, m) = (W_q x_m)e^{im\theta}
$$
$$
f_k(x_n, n) = (W_k x_n)e^{in\theta}
$$

**Prova**:

1. Considere a decomposi√ß√£o de fun√ß√µes em componentes radiais e angulares [23]:

   $$
   \begin{aligned}
   f_q(x_q, m) &= R_q(x_q, m)e^{i\Theta_q(x_q,m)} \\
   f_k(x_k, n) &= R_k(x_k, n)e^{i\Theta_k(x_k,n)} \\
   g(x_q, x_k, n - m) &= R_g(x_q, x_k, n - m)e^{i\Theta_g(x_q,x_k,n-m)}
   \end{aligned}
   $$

2. Aplicando as condi√ß√µes iniciais e as rela√ß√µes derivadas [24]:

   $$
   \begin{aligned}
   R_q(x_q, m) &= R_q(x_q, 0) = \|q\| \\
   R_k(x_k, n) &= R_k(x_k, 0) = \|k\| \\
   \Theta_f(x_{q,k}, m) &= \phi(m) + \theta_{q,k}
   \end{aligned}
   $$

3. Considerando $\phi(m) = m\theta + \gamma$, onde $\theta, \gamma \in \mathbb{R}$ s√£o constantes [25].

4. Finalmente, definindo $q = W_q x_m$ e $k = W_k x_n$ [26], obtemos:

   $$
   \begin{aligned}
   f_q(x_m, m) &= (W_q x_m)e^{im\theta} \\
   f_k(x_n, n) &= (W_k x_n)e^{in\theta}
   \end{aligned}
   $$

> ‚ùó **Ponto de Aten√ß√£o**: Esta formula√ß√£o em 2D serve como base para a generaliza√ß√£o do RoPE em dimens√µes superiores.

#### Teorema 1: Decaimento de Longo Prazo do RoPE

![image-20240917123312506](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240917123312506.png)

**Enunciado**: O RoPE possui a propriedade de decaimento de longo prazo, ==onde a influ√™ncia entre tokens diminui com o aumento da dist√¢ncia relativa [27].==

**Prova**:

1. Considere o produto interno de RoPE em dimens√£o d [28]:

   $$
   (R_{\theta,m}^d W_q x_m)^T(R_{\theta,n}^d W_k x_n) = \text{Re}\left[\sum_{i=0}^{d/2-1} q_{[2i:2i+1]}k_{[2i:2i+1]}^*e^{i(m-n)\theta_i}\right]
   $$

2. Usando a transforma√ß√£o de Abel e definindo $h_i = q_{[2i:2i+1]}k_{[2i:2i+1]}^*$ e $S_j = \sum_{i=0}^{j-1} e^{i(m-n)\theta_i}$ [29]:

   $$
   \left|\sum_{i=0}^{d/2-1} q_{[2i:2i+1]}k_{[2i:2i+1]}^*e^{i(m-n)\theta_i}\right| \leq \left(\max_i |h_{i+1} - h_i|\right) \sum_{i=0}^{d/2-1} |S_{i+1}|
   $$

3. Com $\theta_i = 10000^{-2i/d}$, demonstra-se que $\frac{1}{d/2} \sum_{i=1}^{d/2} |S_i|$ decai com o aumento da dist√¢ncia relativa $m - n$ [30].

> üí° **Insight Crucial**: Esta propriedade de decaimento √© fundamental para a capacidade do RoFormer em modelar eficientemente depend√™ncias de longo alcance.

### Discuss√£o

O RoFormer apresenta avan√ßos significativos em rela√ß√£o a abordagens anteriores de encoding posicional:

1. **Flexibilidade de Comprimento de Sequ√™ncia**: Ao contr√°rio de m√©todos de posi√ß√£o absoluta, o RoPE n√£o √© limitado por um comprimento m√°ximo de sequ√™ncia predefinido [31].

2. **Compatibilidade com Self-Attention Linear**: O RoPE pode ser facilmente incorporado em arquiteturas de self-attention linear, como o Performer [32].

3. **Desempenho em Textos Longos**: Os resultados em tarefas de textos longos, como o CAIL2019-SCM, demonstram a efic√°cia do RoFormer em capturar depend√™ncias de longo alcance [33].

**Compara√ß√£o com Trabalhos Anteriores**:

| Aspecto                            | RoFormer [34]           | Transformers Tradicionais [35]   |
| ---------------------------------- | ----------------------- | -------------------------------- |
| Encoding Posicional                | Rota√ß√£o de embeddings   | Adi√ß√£o de embeddings posicionais |
| Captura de Posi√ß√£o Relativa        | Intr√≠nseca √† formula√ß√£o | Requer modifica√ß√µes adicionais   |
| Desempenho em Textos Longos        | Superior                | Limitado pelo comprimento m√°ximo |
| Compatibilidade com Aten√ß√£o Linear | Compat√≠vel              | Geralmente incompat√≠vel          |

**Limita√ß√µes e Perspectivas Futuras**:

1. **Complexidade Computacional**: Embora o RoPE seja eficiente, a implementa√ß√£o em larga escala pode requerer otimiza√ß√µes adicionais [36].
2. **Generaliza√ß√£o para Outras Modalidades**: A efic√°cia do RoPE em tarefas al√©m do processamento de texto natural ainda precisa ser investigada [37].
3. **Interpretabilidade**: Uma an√°lise mais aprofundada da representa√ß√£o interna aprendida pelo RoFormer pode fornecer insights valiosos sobre seu funcionamento [38].

### Conclus√£o

O RoFormer, com sua inovadora abordagem de Rotary Position Embedding, representa um avan√ßo significativo na arquitetura Transformer [39]. Ao codificar informa√ß√µes de posi√ß√£o atrav√©s de rota√ß√µes em vez de adi√ß√µes, o RoFormer demonstra melhorias consistentes em v√°rias tarefas de processamento