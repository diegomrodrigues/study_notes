## Cloze Task: Treinamento de Encoders Bidirecionais atrav√©s de Preenchimento de Lacunas

<image: Um diagrama mostrando um texto com palavras mascaradas, representando a tarefa de Cloze, com setas bidirecionais indicando o fluxo de informa√ß√£o em ambas as dire√ß√µes no modelo de linguagem>

### Introdu√ß√£o

A tarefa de Cloze, tamb√©m conhecida como tarefa de preenchimento de lacunas, emergiu como um objetivo de treinamento fundamental para encoders bidirecionais em modelos de linguagem de √∫ltima gera√ß√£o [1]. Esta abordagem contrasta significativamente com o m√©todo tradicional de predi√ß√£o da pr√≥xima palavra utilizado em modelos causais, oferecendo uma nova perspectiva sobre como treinar modelos de linguagem para compreender contextos mais amplos e bidirecionais [2].

> ‚úîÔ∏è **Ponto de Destaque**: A tarefa de Cloze permite que os modelos aprendam representa√ß√µes contextuais bidirecionais, superando as limita√ß√µes dos modelos causais que s√≥ consideram o contexto √† esquerda.

### Conceitos Fundamentais

| Conceito                  | Explica√ß√£o                                                   |
| ------------------------- | ------------------------------------------------------------ |
| **Cloze Task**            | Tarefa de aprendizado onde o modelo deve prever elementos faltantes em uma sequ√™ncia de entrada, dado o contexto circundante [1]. |
| **Encoder Bidirecional**  | Arquitetura que processa a entrada em ambas as dire√ß√µes, permitindo que cada token seja contextualizado por todo o seu entorno [2]. |
| **Masked Language Model** | Abordagem de treinamento onde tokens aleat√≥rios s√£o mascarados e o modelo √© treinado para prev√™-los, utilizando tanto o contexto esquerdo quanto o direito [3]. |

### Cloze Task vs. Predi√ß√£o da Pr√≥xima Palavra

<image: Um diagrama comparativo mostrando um modelo causal predizendo a pr√≥xima palavra √† esquerda, e um modelo bidirecional preenchendo lacunas no meio do texto √† direita>

A tarefa de Cloze representa uma evolu√ß√£o significativa em rela√ß√£o √† predi√ß√£o da pr√≥xima palavra, m√©todo tradicionalmente utilizado em modelos causais. Enquanto a predi√ß√£o da pr√≥xima palavra foca em antecipar o token seguinte baseando-se apenas no contexto anterior, a tarefa de Cloze desafia o modelo a compreender e utilizar o contexto completo da sequ√™ncia [4].

#### üëç Vantagens da Cloze Task

* Permite aprendizado de representa√ß√µes bidirecionais [2]
* Facilita a captura de depend√™ncias de longo alcance em ambas as dire√ß√µes [3]
* Melhora a compreens√£o contextual geral do modelo [4]

#### üëé Desvantagens da Predi√ß√£o da Pr√≥xima Palavra

* Limitada ao contexto esquerdo, ignorando informa√ß√µes futuras [2]
* Pode levar a vieses direcionais na representa√ß√£o de palavras [4]
* Menos eficaz para tarefas que requerem compreens√£o bidirecional do contexto [3]

> ‚ùó **Ponto de Aten√ß√£o**: A elimina√ß√£o da m√°scara causal em modelos bidirecionais torna a tarefa de predi√ß√£o da pr√≥xima palavra trivial, necessitando de uma nova abordagem de treinamento [5].

### Formula√ß√£o Matem√°tica da Cloze Task

A tarefa de Cloze pode ser formalizada matematicamente da seguinte forma [6]:

Dado um conjunto de tokens de entrada $X = \{x_1, x_2, ..., x_n\}$, selecionamos aleatoriamente um subconjunto $M \subset X$ para mascaramento. O objetivo √© maximizar a probabilidade logar√≠tmica dos tokens mascarados, dado o restante da sequ√™ncia:

$$
\mathcal{L} = \sum_{x_i \in M} \log P(x_i | X \setminus M)
$$

Onde:
- $\mathcal{L}$ √© a fun√ß√£o de perda (loss)
- $P(x_i | X \setminus M)$ √© a probabilidade do token mascarado $x_i$ dado o contexto n√£o mascarado

Esta formula√ß√£o incentiva o modelo a utilizar tanto o contexto esquerdo quanto o direito para prever os tokens mascarados, promovendo assim uma compreens√£o bidirecional do texto [7].

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a tarefa de Cloze difere fundamentalmente da predi√ß√£o da pr√≥xima palavra em termos de aprendizado de contexto?
2. Quais s√£o as implica√ß√µes da tarefa de Cloze para a captura de depend√™ncias de longo alcance em modelos de linguagem?

### Implementa√ß√£o da Cloze Task

A implementa√ß√£o da tarefa de Cloze em modelos como BERT envolve v√°rias etapas cr√≠ticas [8]:

1. **Tokeniza√ß√£o**: A entrada √© primeiro tokenizada usando um modelo de subpalavras.
2. **Sele√ß√£o de Tokens**: Aproximadamente 15% dos tokens de entrada s√£o selecionados aleatoriamente para mascaramento.
3. **Mascaramento**: Os tokens selecionados s√£o tratados da seguinte forma:
   - 80% s√£o substitu√≠dos pelo token especial [MASK]
   - 10% s√£o substitu√≠dos por um token aleat√≥rio
   - 10% s√£o mantidos inalterados

> ‚ö†Ô∏è **Nota Importante**: A varia√ß√£o no tratamento dos tokens selecionados (mascaramento, substitui√ß√£o, manuten√ß√£o) ajuda o modelo a manter uma representa√ß√£o robusta de cada token e reduz a discrep√¢ncia entre o treinamento e a infer√™ncia [9].

Aqui est√° um exemplo simplificado de como implementar o mascaramento para a tarefa de Cloze usando PyTorch:

```python
import torch
import random

def mask_tokens(inputs, tokenizer, mlm_probability=0.15):
    labels = inputs.clone()
    probability_matrix = torch.full(labels.shape, mlm_probability)
    special_tokens_mask = [
        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)
        for val in labels.tolist()
    ]
    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)
    masked_indices = torch.bernoulli(probability_matrix).bool()
    labels[~masked_indices] = -100  # We only compute loss on masked tokens

    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])
    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices
    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)

    # 10% of the time, we replace masked input tokens with random word
    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)
    inputs[indices_random] = random_words[indices_random]

    # The rest of the time (10% of the time) we keep the masked input tokens unchanged
    return inputs, labels
```

Este c√≥digo implementa a l√≥gica de mascaramento descrita anteriormente, preparando os dados para o treinamento com a tarefa de Cloze [10].

### Impacto da Cloze Task em Modelos de Linguagem

A introdu√ß√£o da tarefa de Cloze como objetivo de treinamento teve um impacto profundo no desenvolvimento de modelos de linguagem [11]:

1. **Representa√ß√µes Contextuais Melhoradas**: Os modelos treinados com Cloze Task capturam melhor o contexto bidirecional, resultando em representa√ß√µes mais ricas e informativas [12].

2. **Flexibilidade em Downstream Tasks**: A natureza bidirecional das representa√ß√µes aprendidas facilita a adapta√ß√£o dos modelos para uma variedade de tarefas downstream, como classifica√ß√£o de texto, resposta a perguntas e infer√™ncia de linguagem natural [13].

3. **Efici√™ncia de Treinamento**: Apesar de mascarar apenas uma por√ß√£o dos tokens de entrada, a tarefa de Cloze permite um treinamento eficiente, aproveitando toda a sequ√™ncia de entrada para aprendizado [14].

4. **Generaliza√ß√£o Melhorada**: A exposi√ß√£o a diversos contextos durante o treinamento com Cloze Task melhora a capacidade de generaliza√ß√£o do modelo para textos e tarefas n√£o vistos [15].

> ‚úîÔ∏è **Ponto de Destaque**: A tarefa de Cloze permitiu o desenvolvimento de modelos como BERT, que revolucionaram o processamento de linguagem natural ao fornecer representa√ß√µes contextuais poderosas e vers√°teis [16].

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a porcentagem de tokens mascarados (15% no BERT) afeta o desempenho e a efici√™ncia do treinamento do modelo?
2. Quais s√£o as considera√ß√µes ao escolher entre uma abordagem de Cloze Task e um modelo autoregressive para uma tarefa espec√≠fica de NLP?

### Conclus√£o

A tarefa de Cloze emergiu como um paradigma de treinamento crucial para modelos de linguagem bidirecionais, oferecendo uma alternativa poderosa √† predi√ß√£o da pr√≥xima palavra usada em modelos causais [17]. Ao permitir que os modelos aprendam representa√ß√µes contextuais ricas e bidirecionais, a Cloze Task abriu caminho para avan√ßos significativos em uma ampla gama de tarefas de processamento de linguagem natural [18].

A capacidade de utilizar tanto o contexto esquerdo quanto o direito para prever tokens mascarados permite que os modelos capturem depend√™ncias complexas e de longo alcance no texto, resultando em representa√ß√µes mais informativas e vers√°teis [19]. Esta abordagem n√£o apenas melhorou o desempenho em tarefas downstream, mas tamb√©m aumentou a flexibilidade e adaptabilidade dos modelos de linguagem pr√©-treinados [20].

√Ä medida que o campo do processamento de linguagem natural continua a evoluir, a tarefa de Cloze permanece uma t√©cnica fundamental, impulsionando o desenvolvimento de modelos cada vez mais sofisticados e capazes [21].

### Quest√µes Avan√ßadas

1. Como a tarefa de Cloze poderia ser adaptada para capturar melhor estruturas sint√°ticas ou sem√¢nticas espec√≠ficas em diferentes idiomas?

2. Considerando as limita√ß√µes da tarefa de Cloze, como voc√™ projetaria um objetivo de pr√©-treinamento que pudesse capturar ainda melhor as nuances contextuais e as rela√ß√µes de longo alcance em textos?

3. Discuta as implica√ß√µes te√≥ricas e pr√°ticas de combinar a tarefa de Cloze com outros objetivos de treinamento, como a modelagem de linguagem contrastiva ou a predi√ß√£o de span, para melhorar ainda mais as representa√ß√µes aprendidas.

### Refer√™ncias

[1] "We're asked to predict a missing item given the rest of the sentence." (Trecho de Fine-Tuning and Masked Language Models)

[2] "Instead of predicting the next word, the model learns to perform a fill-in-the-blank task, technically called the cloze task (Taylor, 1953)." (Trecho de Fine-Tuning and Masked Language Models)

[3] "To see this, let's return to the motivating example from Chapter 3. Instead of predicting which words are likely to come next in this example:

Please turn your homework ____ .

we're asked to predict a missing item given the rest of the sentence.

Please turn ____ homework in." (Trecho de Fine-Tuning and Masked Language Models)

[4] "That is, given an input sequence with one or more elements missing, the learning task is to predict the missing elements." (Trecho de Fine-Tuning and Masked Language Models)

[5] "eliminating the causal mask makes the guess-the-next-word language modeling task trivial since the answer is now directly available from the context" (Trecho de Fine-Tuning and Masked Language Models)

[6] "More precisely, during training the model is deprived of one or more elements of an input sequence and must generate a probability distribution over the vocabulary for each of the missing items. We then use the cross-entropy loss from each of the model's predictions to drive the learning process." (Trecho de Fine-Tuning and Masked Language Models)

[7] "This approach can be generalized to any of a variety of methods that corrupt the training input and then asks the model to recover the original input. Examples of the kinds of manipulations that have been used include masks, substitutions, reorderings, deletions, and extraneous insertions into the training text." (Trecho de Fine-Tuning and Masked Language Models)

[8] "The original approach to training bidirectional encoders is called Masked Language Modeling (MLM) (Devlin et al., 2019). As with the language model training methods we've already seen, MLM uses unannotated text from a large corpus. Here, the model is presented with a series of sentences from the training corpus where a random sample of tokens from each training sequence is selected for use in the learning task. Once chosen, a token is used in one of three ways:

‚Ä¢ It is replaced with the unique vocabulary token [MASK].
‚Ä¢ It is replaced with another token from the vocabulary, randomly sampled based on token unigram probabilities.

‚Ä¢ It is left unchanged." (Trecho de Fine-Tuning and Masked Language Models)

[9] "In BERT, 15% of the input tokens in a training sequence are sampled for learning. Of these, 80% are replaced with [MASK], 10% are replaced with randomly selected tokens, and the remaining 10% are left unchanged." (Trecho de Fine-Tuning and Masked Language Models)

[10] "The MLM training objective is to predict the original inputs for each of the masked tokens using a bidirectional encoder of the kind described in the last section. The cross-entropy loss from these predictions drives the training process for all the parameters in the model. Note that all of the input tokens play a role in the self-attention process, but only the sampled tokens are used for learning." (Trecho de Fine-Tuning and Masked Language Models)

[11] "More specifically, the original input sequence is first tokenized using a subword model. The sampled items which drive the learning process are chosen from among the set of tokenized inputs. Word embeddings for all of the tokens in the input are retrieved from the word embedding matrix and then combined with positional embeddings to form the input to the transformer." (Trecho de Fine-Tuning and Masked Language Models)

[12] "Fig. 11.4 illustrates this approach with a simple example. Here, long, thanks and the have been sampled from the training sequence, with the first two masked and the replaced with the randomly sampled token apricot. The resulting embeddings are passed through a stack of bidirectional transformer blocks." (Trecho de Fine-Tuning and Masked Language Models)

[13] "To produce a probability distribution over the vocabulary for each of the masked tokens, the output vector zi from the final transformer layer for each masked token i is multiplied by a learned set of classification weights WV ‚àà R|V|√ódh and then through a softmax to yield the required predictions over the vocabulary." (Trecho de Fine-Tuning and Masked Language Models)

[14] "yi = softmax(WVzi)" (Trecho de Fine-Tuning and Masked Language Models)

[15] "With a predicted probability distribution for each masked item, we can use cross-entropy to compute the loss for each masked item‚Äîthe negative log probability assigned to the actual masked word, as shown in Fig. 11.4." (Trecho de Fine-Tuning and Masked Language Models)

[16] "More formally, for a given vector of input tokens in a sentence or batch be x, let the set of tokens that are masked be M, the version of that sentence with some tokens replaced by masks be x^mask, and the sequence of output vectors be z. For a given input token x_i, such as the word long in Fig. 11.4, the loss is the probability of the correct word long, given x^mask (