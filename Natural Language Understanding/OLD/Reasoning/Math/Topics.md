This paper presents a novel data augmentation technique, Iterative Question Composing (IQC), and a new dataset, MMIQC, to improve the performance of large language models (LLMs) on mathematical reasoning tasks. Let's break down the key aspects:

**I. Core Contributions:**

1. 
2. **Iterative Question Composing (IQC):** This is the core innovation.  IQC iteratively generates new math word problems from existing ones by using an LLM to compose increasingly complex questions based on the original problem and its solution.  This differs from simple rephrasing or paraphrasing; it aims to create problems requiring multiple reasoning steps, organically expanding the complexity without introducing irrelevant elements. A rejection sampling step using a second LLM ensures the generated problems and solutions are reasonable.
3. **MMIQC Dataset:** A mixed dataset combining pre-processed real-world data from math.stackexchange.com (part of RedPajama) with synthetic data generated using IQC and other augmentation methods (question bootstrapping, generating similar problems). This mixed approach attempts to leverage both the quality of real-world data and the quantity and diversity offered by synthetic generation.
4. **Improved Performance on MATH Benchmark:**  LLMs fine-tuned on MMIQC significantly outperform those fine-tuned on other datasets (e.g., MetaMathQA) across different model sizes, achieving state-of-the-art results on the MATH benchmark.  Qwen-72B-MMIQC reached 45% accuracy, surpassing the previous open-source state-of-the-art and even outperforming the initial release of GPT-4.
5. **Generalization to Unseen Data:** The improvements observed on the MATH benchmark generalize to unseen data, as demonstrated by the evaluation on the 2023 Hungarian high school mathematics finals.

**II. Key Methods and Techniques:**

1. 
2. **Iterative Question Composing (IQC) Algorithm:** The algorithm systematically generates new questions, iteratively building on previous questions using an LLM (πq for question generation) and filtering those with incorrect or unreasonable answers using a second LLM (πr for rejection sampling).  The algorithm uses a template function (x) to convert question-answer pairs into a text format suitable for LLM processing.
3. **MMIQC Dataset Composition:** MMIQC combines several data sources:  a subset of MetaMathQA, data generated by augmenting answers and performing question bootstrapping (based on Yu et al., 2023), data generated by creating similar problems (based on Liu et al., 2023), and the IQC-generated data. Each component contributes to different aspects of diversity and complexity. The largest component (52.5%) comes from Math Stack Exchange questions and answers.
4. **Fine-tuning Strategy:** A standard supervised fine-tuning approach is used, with minor adjustments to the prompt template depending on the data source. The learning rate and other hyperparameters are chosen through experiments.

**III. Evaluation and Results:**

The paper provides a detailed comparative analysis of different LLMs fine-tuned on MMIQC and other datasets (MetaMathQA).  The results clearly show the superiority of MMIQC in improving the performance on the MATH benchmark and its generalization to an entirely different held-out dataset (Hungarian high school exams).  The ablation study further demonstrates the significant contribution of the IQC method to the performance gains.

**IV. Limitations and Future Work:**

The authors acknowledge that while IQC is effective, it still relies on LLMs (GPT-3.5 and GPT-4) for data generation.  Future work will explore methods to make the process more autonomous, potentially by empowering the LLM to perform question composition itself (self-evolution).  Furthermore, they suggest integrating verification systems into the IQC process to improve efficiency and accuracy.

**In summary:** This paper presents a strong contribution to the field of improving LLMs for mathematical reasoning by introducing a novel, iterative data augmentation method (IQC) and a well-designed mixed dataset (MMIQC). The empirical results are compelling, showcasing substantial performance gains and generalization to unseen data. The proposed methods offer a promising direction for future research in this area.



---



This paper introduces MARIO (MAth Reasoning with code Interpreter Output), a reproducible pipeline for training LLMs to perform mathematical reasoning.  The core contributions focus on creating a high-quality dataset and a robust fine-tuning protocol. Let's break down the key aspects:

**I. Core Contributions:**

1. 
2. **Data Generation Pipeline for Math Reasoning:** MARIO generates a dataset with solutions integrating both textual reasoning steps and Python code snippets for precise calculations. This addresses the limitations of purely text-based or purely code-based approaches by combining the strengths of both.  The pipeline uses GPT-4 for initial annotation, followed by human review (especially for GSM8K) and a self-training process using a strong teacher model (Llemma-34B) for the MATH dataset to correct errors and increase coverage.
3. **Reproducible Fine-tuning Protocol:**  The paper presents a clear, three-stage fine-tuning process:
   - 
   - **Continual Pre-training (CPT):**  Starts with a pre-trained LLM (Llama-2) and continues pre-training on a math and code-focused dataset.
   - **Supervised Fine-tuning (SFT):**  Fine-tunes the entire model on the generated dataset using full parameter training, optimizing for generating correct solutions.
   - **Multi-task Fine-tuning with LoRA:** Fine-tunes a lightweight binary classifier (using LoRA) to predict the correctness of solutions, combining generation and verification into a single model.  This enables efficient computation and reduces the need for separate verification models.
4. **Enhanced Performance on Math Benchmarks:** The fine-tuned MARIO models significantly outperform other open-source LLMs on GSM8K and MATH, approaching the performance of closed-source models like GPT-4 and even surpassing them on some more complex, out-of-domain datasets. The results highlight the benefits of the integrated text-code solution format and the proposed fine-tuning protocol.
5. **Publicly Available Resources:**  The authors make their code, data, and model checkpoints publicly available, promoting reproducibility and facilitating further research within the community.  This is a significant contribution to open-source research in mathematical reasoning.

**II. Key Methods and Techniques:**

1. 
2. **REACT-inspired Prompting:** The authors use prompts inspired by the REACT framework to guide GPT-4 in generating solutions that seamlessly integrate textual reasoning and code execution.  This ensures that the LLM understands when to use the code interpreter.
3. **Human Verification and Self-Training:** A crucial aspect of data generation is the combination of human verification (especially for easily correctable errors in GSM8K) and a self-training process for the MATH dataset, using a stronger LLM as a teacher model.  This strategy balances human effort with automation to create a high-quality dataset.
4. **Multi-task LoRA Fine-tuning:** This efficient fine-tuning technique trains both the generation and verification aspects of the model simultaneously, reducing computational costs and improving performance.
5. **Outlier-free OVM Selection:** During inference, multiple solutions are generated, and an outcome value model (OVM) is used to select the best solution. An outlier-free selection strategy is employed to avoid selecting solutions with abnormally high predicted probabilities due to random chance.

**III. Evaluation and Results:**

The paper presents a comprehensive evaluation across several benchmarks:  GSM8K, MATH, OCWCourses, and GaoKao2023-Math-En.  MARIO consistently outperforms other open-source models, particularly on the more challenging MATH and out-of-domain datasets. The results underscore the effectiveness of the integrated text-code solution format, multi-task training, and the outlier-free selection method.  The ablation study provides further evidence of the individual contributions of different data sources and techniques.

**IV. Limitations and Future Work:**

The authors acknowledge that the performance on the GSM-Hard dataset is lower than expected.  They attribute this to the inherent issues in GSM-Hard (unrealistic scenarios and potential data leakage), suggesting that this dataset is not suitable for evaluating more sophisticated reasoning models.  Future work will focus on addressing the limitations of GSM-Hard, as well as improving the OVM and exploring the potential of incorporating the OVM's output into the beam search during decoding.

**In summary:**  This paper offers a valuable contribution to the field of LLM-based mathematical reasoning. The proposed pipeline, with its focus on reproducibility and the integration of code execution, demonstrates a significant step forward in achieving high performance on challenging mathematical tasks. The public availability of the resources further enhances the value of this work for the research community.



---



This paper explores scaling self-training for language models on problem-solving tasks, moving beyond the limitations of relying solely on human-generated data.  The core contribution is ReSTEM, a self-training method achieving state-of-the-art results on challenging benchmarks.

**I. Core Contributions:**

1. 
2. **ReSTEM Algorithm:**  This self-training algorithm leverages expectation-maximization (EM) for reinforcement learning.  It iteratively generates model outputs, filters them based on a binary reward (correct/incorrect), and then fine-tunes the model on the filtered data.  This process is repeated for several iterations.  A key distinction from previous self-training methods (like ReST) is that ReSTEM fine-tunes the *base* pretrained model in each iteration, rather than the model from the previous iteration, resulting in better generalization.
3. **Scalability to Large Language Models:** Unlike prior work in self-training, ReSTEM demonstrates significant and consistent performance improvements when applied to large language models (PaLM-2), showcasing its scalability.  This is a crucial advance, as previous methods struggled to scale effectively to larger models.
4. **Superior Performance on Challenging Benchmarks:**  ReSTEM significantly surpasses methods relying solely on human-generated data on both MATH (mathematical reasoning) and APPS (code generation) benchmarks.  The improvements are substantial and consistent across different PaLM-2 model sizes.
5. **Robust Generalization:** The models trained with ReSTEM exhibit strong generalization capabilities, showing improved performance on related, held-out benchmarks like GSM8K, Hungarian high school math exams, and HumanEval.

**II. Key Methods and Techniques:**

1. 
2. **Expectation-Maximization (EM) Framework:**  The theoretical foundation of ReSTEM is the EM algorithm applied to reinforcement learning.  This framework provides a principled approach to iteratively improve the model by alternating between data generation (E-step) and model fine-tuning (M-step).
3. **Binary Reward Function:** ReSTEM simplifies the reward function to a binary signal (correct/incorrect), making it suitable for tasks where automatic evaluation is feasible (like MATH and APPS). This avoids the complexities of designing or learning more nuanced reward functions.
4. **Temperature Sampling:**  The model generates multiple outputs using top-K sampling with a temperature parameter, introducing controlled randomness in the exploration process. This helps generate diverse solutions, including those that might not be immediately obvious.
5. **Dataset Size Control:**  A maximum number of samples per problem is used during the E-step to create a balanced dataset and prevent overfitting to easier problems.

**III. Evaluation and Results:**

The paper presents a comprehensive evaluation of ReSTEM on MATH and APPS, comparing its performance against SFT using human data and other state-of-the-art models. The results show clear advantages of ReSTEM, particularly for larger models.  Ablation studies investigate the impact of various parameters (number of iterations, samples per problem, dataset size).  The paper also demonstrates strong transfer learning capabilities to held-out benchmarks.  The Hungarian high school exam results are particularly notable, showcasing generalization to a real-world, out-of-distribution dataset.

**IV. Limitations and Future Work:**

The authors acknowledge that ReSTEM still requires:

- 
- A moderately-sized initial dataset of problems (to bootstrap the process).
- An automatic reward function (correctness is easily verifiable in the selected tasks).
  While ReSTEM improves Pass@1, the gap to Pass@K (with large K) remains.  Future work will focus on automating the initial data collection process and improving the algorithm to close this gap.

**In summary:** This paper makes a substantial contribution to the field of LLM training by demonstrating the effectiveness and scalability of self-training with binary feedback. ReSTEM offers a promising approach to reduce reliance on human data for training LLMs on complex problem-solving tasks.  The rigorous evaluation and ablation studies provide valuable insights into the strengths and limitations of the method, guiding future research directions.



---



This paper introduces MATH-SHEPHERD, a process-oriented reward model for improving mathematical reasoning in LLMs.  The key innovation lies in its ability to automatically generate training data for the reward model, eliminating the need for expensive human annotation.

**I. Core Contributions:**

1. 
2. **Automatic Process Annotation Framework:**  This is the central contribution.  MATH-SHEPHERD avoids the costly human annotation required for existing process reward models (PRMs) by automatically generating training data.  It leverages a "completer" LLM to generate multiple solution continuations from an intermediate step in a given solution.  The correctness of the final answers from these continuations determines the reward assigned to the intermediate step.  This framework uses two estimation methods, Hard Estimation (HE) and Soft Estimation (SE), to label the steps.
3. **MATH-SHEPHERD as a Verifier:** The model effectively reranks multiple LLM outputs, significantly improving the accuracy of selecting the correct solution.  It consistently outperforms self-consistency and outcome reward models (ORMs) across various LLMs and datasets.
4. **MATH-SHEPHERD for Step-by-Step Reinforcement Learning:**  MATH-SHEPHERD is used to provide step-wise feedback during Proximal Policy Optimization (PPO) training.  This leads to substantial improvements in LLM performance on both GSM8K and MATH benchmarks, surpassing the results of other open-source methods.
5. **Data Efficiency:** The automatic annotation method shows high data efficiency, outperforming human-annotated PRMs even with a smaller dataset.

**II. Key Methods and Techniques:**

1. 
2. **Process-Oriented Reward Model (PRM):**  MATH-SHEPHERD is a PRM, assigning scores to individual steps in a solution rather than the entire solution.  This provides finer-grained feedback for both verification and reinforcement learning.
3. **Completer LLM:** A separate LLM is used to complete partial solutions, generating multiple possible continuations from an intermediate step. This is crucial for the automatic annotation process.
4. **Hard and Soft Estimation:** Two methods for labeling steps are used: HE (binary: correct/incorrect based on at least one correct final answer) and SE (probability: fraction of correct final answers).
5. **Step-by-Step PPO:** The PRM is used to provide rewards at each step during PPO training, unlike typical PPO methods that only reward the final answer.
6. **Combination with Self-Consistency:** The paper explores combining MATH-SHEPHERD with self-consistency, showing improvements on the MATH dataset but not on GSM8K. This suggests that the effectiveness of combining methods depends on the task complexity.

**III. Evaluation and Results:**

The paper evaluates MATH-SHEPHERD extensively on GSM8K and MATH, comparing it against self-consistency, ORM, and other state-of-the-art methods.  The results demonstrate substantial improvements in accuracy when using MATH-SHEPHERD for both verification and reinforcement learning.  The ablation studies show the impact of different parameters and the quality of automatic annotations. Out-of-distribution results on the Hungarian high school exam further highlight the generalization capabilities of the model.

**IV. Limitations and Future Work:**

The main limitations are the computational cost of the completion process (using the completer LLM to generate multiple continuations) and the inherent noise in the automatic annotation process.  Future work will focus on improving efficiency through advanced inference techniques and potentially refining the annotation method to reduce noise.

**In summary:**  This paper presents a significant advancement in training LLMs for mathematical reasoning by introducing an automatic process annotation framework for PRMs.  The results demonstrate the effectiveness of MATH-SHEPHERD in both verification and reinforcement learning scenarios, showcasing a promising path towards creating more robust and capable mathematical reasoning LLMs with reduced reliance on expensive human annotation.