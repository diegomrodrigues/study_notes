## Token Embeddings: Explorando M√©todos Avan√ßados de Representa√ß√£o Vetorial de Palavras

<image: Uma rede neural com camadas de entrada, ocultas e de sa√≠da, onde as palavras de entrada s√£o transformadas em vetores densos (embeddings) na camada de sa√≠da. Setas indicam o fluxo de informa√ß√£o e transforma√ß√£o das palavras em vetores multidimensionais.>

### Introdu√ß√£o

Token embeddings s√£o representa√ß√µes vetoriais densas de palavras ou tokens, fundamentais para o processamento de linguagem natural (NLP) moderno. Estas representa√ß√µes capturam rela√ß√µes sem√¢nticas e sint√°ticas entre palavras, permitindo que modelos de machine learning compreendam e processem texto de forma mais eficaz [1]. Este resumo explorar√° m√©todos avan√ßados de cria√ß√£o de embeddings, incluindo Word2Vec, GloVe e FastText, analisando seu impacto na qualidade das representa√ß√µes e suas aplica√ß√µes em tarefas de NLP.

### Conceitos Fundamentais

| Conceito                  | Explica√ß√£o                                                   |
| ------------------------- | ------------------------------------------------------------ |
| **Embedding**             | Representa√ß√£o vetorial densa de uma palavra ou token em um espa√ßo multidimensional, capturando caracter√≠sticas sem√¢nticas e sint√°ticas [1]. |
| **Dimensionalidade**      | N√∫mero de dimens√µes do vetor de embedding, influenciando a capacidade de representa√ß√£o e a efici√™ncia computacional [2]. |
| **Corpus de Treinamento** | Conjunto de textos utilizados para treinar os modelos de embedding, afetando diretamente a qualidade e abrang√™ncia das representa√ß√µes [3]. |

> ‚úîÔ∏è **Ponto de Destaque**: Embeddings transformam palavras em vetores num√©ricos, permitindo opera√ß√µes matem√°ticas que capturam rela√ß√µes sem√¢nticas entre palavras.

### Word2Vec: Modelo Pioneiro de Embeddings

<image: Duas arquiteturas de rede neural lado a lado - CBOW e Skip-gram - mostrando a entrada de palavras de contexto e a previs√£o da palavra-alvo (CBOW) ou vice-versa (Skip-gram).>

Word2Vec, introduzido por Mikolov et al., revolucionou a cria√ß√£o de embeddings com duas arquiteturas principais: Continuous Bag-of-Words (CBOW) e Skip-gram [4].

#### Continuous Bag-of-Words (CBOW)

O modelo CBOW prediz uma palavra-alvo dado seu contexto. A fun√ß√£o objetivo para CBOW pode ser expressa como:

$$
J_\theta = -\frac{1}{T} \sum_{t=1}^T \log p(w_t | w_{t-n}, ..., w_{t-1}, w_{t+1}, ..., w_{t+n})
$$

Onde:
- $T$ √© o n√∫mero total de palavras no corpus
- $w_t$ √© a palavra-alvo
- $w_{t-n}, ..., w_{t+n}$ s√£o as palavras de contexto
- $\theta$ s√£o os par√¢metros do modelo

#### Skip-gram

O modelo Skip-gram faz o inverso, prevendo as palavras de contexto dada uma palavra-alvo. A fun√ß√£o objetivo para Skip-gram √©:

$$
J_\theta = -\frac{1}{T} \sum_{t=1}^T \sum_{-n \leq j \leq n, j \neq 0} \log p(w_{t+j} | w_t)
$$

> ‚ùó **Ponto de Aten√ß√£o**: Skip-gram geralmente tem melhor performance para palavras raras e com conjuntos de dados menores [4].

#### Negative Sampling

Para otimizar o treinamento, Word2Vec utiliza Negative Sampling, reduzindo a complexidade computacional. A fun√ß√£o de loss para Skip-gram com Negative Sampling √©:

$$
J_\theta = \log \sigma(v_{w_O}^T v_{w_I}) + \sum_{i=1}^k \mathbb{E}_{w_i \sim P_n(w)}[\log \sigma(-v_{w_i}^T v_{w_I})]
$$

Onde:
- $v_{w_O}$ √© o vetor de output para a palavra de contexto
- $v_{w_I}$ √© o vetor de input para a palavra-alvo
- $\sigma$ √© a fun√ß√£o sigmoide
- $k$ √© o n√∫mero de amostras negativas
- $P_n(w)$ √© a distribui√ß√£o de ru√≠do

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a escolha entre CBOW e Skip-gram afeta a qualidade dos embeddings para palavras raras em um corpus de dom√≠nio espec√≠fico?
2. Explique como o Negative Sampling melhora a efici√™ncia computacional do treinamento do Word2Vec e por que isso √© crucial para grandes corpora.

### GloVe: Global Vectors for Word Representation

<image: Uma matriz de co-ocorr√™ncia de palavras sendo fatorada em duas matrizes menores, representando os vetores de palavras e contextos do GloVe.>

GloVe, desenvolvido por Pennington et al., combina as vantagens de m√©todos baseados em contagem (como LSA) e m√©todos preditivos (como Word2Vec) [5].

A fun√ß√£o objetivo do GloVe √©:

$$
J = \sum_{i,j=1}^V f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2
$$

Onde:
- $X_{ij}$ √© a matriz de co-ocorr√™ncia de palavras
- $w_i$ e $\tilde{w}_j$ s√£o vetores de palavras e contextos
- $b_i$ e $\tilde{b}_j$ s√£o termos de bias
- $f(X_{ij})$ √© uma fun√ß√£o de peso que d√° menos import√¢ncia a co-ocorr√™ncias muito frequentes

> ‚úîÔ∏è **Ponto de Destaque**: GloVe captura informa√ß√µes globais de co-ocorr√™ncia, complementando a abordagem local do Word2Vec.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a matriz de co-ocorr√™ncia no GloVe difere da abordagem de janela deslizante do Word2Vec, e quais s√£o as implica√ß√µes para a captura de rela√ß√µes sem√¢nticas?
2. Discuta as vantagens e desvantagens de usar a fun√ß√£o de peso $f(X_{ij})$ no treinamento do GloVe.

### FastText: Embeddings Subpalavras

<image: Representa√ß√£o de uma palavra sendo composta por n-gramas de caracteres, com vetores correspondentes sendo somados para formar o embedding final.>

FastText, proposto por Bojanowski et al., estende o modelo Skip-gram do Word2Vec para incorporar informa√ß√µes de subpalavras [6].

A representa√ß√£o de uma palavra $w$ no FastText √© dada por:

$$
s(w) = \sum_{g \in G_w} z_g
$$

Onde:
- $G_w$ √© o conjunto de n-gramas da palavra $w$
- $z_g$ √© o vetor de embedding para o n-grama $g$

A probabilidade de uma palavra de contexto $w_c$ dado uma palavra-alvo $w_t$ √©:

$$
p(w_c|w_t) = \frac{\exp(s(w_t)^T v_{w_c})}{\sum_{w' \in V} \exp(s(w_t)^T v_{w'})}
$$

> ‚ö†Ô∏è **Nota Importante**: FastText √© particularmente eficaz para l√≠nguas morfologicamente ricas e para lidar com palavras fora do vocabul√°rio (OOV).

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como o FastText lida com palavras fora do vocabul√°rio (OOV) e por que isso √© uma vantagem significativa sobre Word2Vec e GloVe?
2. Discuta o trade-off entre a capacidade do FastText de capturar informa√ß√µes morfol√≥gicas e o aumento da complexidade computacional devido ao uso de n-gramas.

### Impacto da Dimensionalidade e Dados de Treinamento

A qualidade dos embeddings √© fortemente influenciada pela dimensionalidade dos vetores e pelo corpus de treinamento [7].

#### Dimensionalidade

| üëç Vantagens de Alta Dimensionalidade  | üëé Desvantagens de Alta Dimensionalidade |
| ------------------------------------- | --------------------------------------- |
| Maior capacidade de representa√ß√£o [7] | Aumento do custo computacional [7]      |
| Captura de rela√ß√µes mais sutis [7]    | Risco de overfitting [7]                |

A escolha da dimensionalidade √≥tima pode ser expressa como um problema de otimiza√ß√£o:

$$
\text{dim}_{\text{opt}} = \arg\max_d \frac{\text{Performance}(d)}{\text{Custo Computacional}(d)}
$$

#### Corpus de Treinamento

A qualidade e quantidade dos dados de treinamento afetam diretamente a qualidade dos embeddings [8].

> ‚ùó **Ponto de Aten√ß√£o**: Corpora maiores e mais diversos geralmente levam a embeddings mais robustos, mas podem introduzir ru√≠do e vi√©s.

A rela√ß√£o entre o tamanho do corpus e a qualidade dos embeddings pode ser modelada como:

$$
Q = f(S, D)
$$

Onde:
- $Q$ √© a qualidade dos embeddings
- $S$ √© o tamanho do corpus
- $D$ √© a diversidade do corpus
- $f$ √© uma fun√ß√£o n√£o-linear que captura a rela√ß√£o complexa entre estas vari√°veis

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como voc√™ determinaria empiricamente a dimensionalidade √≥tima para embeddings em uma tarefa espec√≠fica de NLP, considerando o trade-off entre performance e custo computacional?
2. Discuta estrat√©gias para mitigar o vi√©s introduzido por corpora de treinamento em embeddings e como isso pode afetar aplica√ß√µes downstream.

### Conclus√£o

Token embeddings representam um avan√ßo fundamental em NLP, permitindo a captura de rela√ß√µes sem√¢nticas e sint√°ticas complexas em representa√ß√µes vetoriais densas. Word2Vec, GloVe e FastText oferecem abordagens distintas, cada uma com suas vantagens e desafios espec√≠ficos. A escolha do m√©todo, dimensionalidade e corpus de treinamento deve ser cuidadosamente considerada com base nos requisitos espec√≠ficos da tarefa e recursos dispon√≠veis. √Ä medida que o campo evolui, a integra√ß√£o destes m√©todos com t√©cnicas de aprendizado profundo e a adapta√ß√£o a dom√≠nios espec√≠ficos continuam a impulsionar avan√ßos em aplica√ß√µes de NLP.

### Quest√µes Avan√ßadas

1. Compare e contraste as abordagens de Word2Vec, GloVe e FastText em termos de sua capacidade de capturar analogias sem√¢nticas complexas. Como voc√™ projetaria um experimento para avaliar quantitativamente o desempenho de cada m√©todo nesta tarefa?

2. Discuta as implica√ß√µes √©ticas e pr√°ticas do uso de embeddings pr√©-treinados em tarefas de NLP sens√≠veis, como an√°lise de sentimento em contextos multil√≠ngues ou detec√ß√£o de discurso de √≥dio. Como os vieses inerentes aos dados de treinamento podem ser mitigados?

3. Proponha uma arquitetura h√≠brida que combine elementos de Word2Vec, GloVe e FastText para criar embeddings mais robustos. Quais seriam os desafios t√©cnicos na implementa√ß√£o de tal modelo e como voc√™ avaliaria sua efic√°cia em compara√ß√£o com os m√©todos individuais?

### Refer√™ncias

[1] "Token embeddings s√£o representa√ß√µes vetoriais densas de palavras ou tokens, fundamentais para o processamento de linguagem natural (NLP) moderno." (Trecho de Transformers and Large Language Models - Chapter 10)

[2] "Recall that in a one-hot vector all the elements are 0 except one, the element whose dimension is the word's index in the vocabulary, which has value 1." (Trecho de Transformers and Large Language Models - Chapter 10)

[3] "Large language models are mainly trained on text scraped from the web, augmented by more carefully curated data." (Trecho de Transformers and Large Language Models - Chapter 10)

[4] "We can formalize this algorithm for generating a sequence of words W = w1, w2, . . . , wN until we hit the end-of-sequence token, using x ‚àº p(x) to mean 'choose x by sampling from the distribution p(x):" (Trecho de Transformers and Large Language Models - Chapter 10)

[5] "To train a transformer as a language model, we use the same self-supervision (or self-training) algorithm we saw in Section ??: we take a corpus of text as training material and at each time step t ask the model to predict the next word." (Trecho de Transformers and Large Language Models - Chapter 10)

[6] "Large models are generally trained by filling the full context window (for example 2048 or 4096 tokens for GPT3 or GPT4) with text." (Trecho de Transformers and Large Language Models - Chapter 10)

[7] "The performance of large language models has shown to be mainly determined by 3 factors: model size (the number of parameters not counting embeddings), dataset size (the amount of training data), and the amount of computer used for training." (Trecho de Transformers and Large Language Models - Chapter 10)

[8] "Web text is usually taken from corpora of automatically-crawled web pages like the common crawl, a series of snapshots of the entire web produced by the non-profit Common Crawl (https://commoncrawl.org/) that each have billions of webpages." (Trecho de Transformers and Large Language Models - Chapter 10)