## Absolute vs. Relative Positional Embeddings em Transformers

```mermaid
flowchart TD
    %% Defini√ß√£o de estilos
    classDef transformer fill:#f0f0ff,stroke:#333,stroke-width:1px;
    classDef embedding fill:#e0ffe0,stroke:#333,stroke-width:1px;
    classDef attention fill:#ffe0e0,stroke:#333,stroke-width:1px;
    classDef output fill:#ffffe0,stroke:#333,stroke-width:1px;

    %% Transformer com Embeddings Posicionais Absolutos
    subgraph A[Transformer com Embeddings Posicionais Absolutos]
    direction TB
        A1[Tokens de Entrada\nx‚ÇÅ, x‚ÇÇ, ..., x‚Çô]:::transformer
        A2["Embeddings de Token E(x‚ÇÅ), E(x‚ÇÇ), ..., E(x‚Çô)"]:::embedding
        A3["Embeddings Posicionais Absolutos\nPE(1), PE(2), ..., PE(n)"]:::embedding
        A4["Soma dos Embeddings\nH·µ¢ = E(x·µ¢) + PE(i)"]:::embedding
        A5[Camadas de Aten√ß√£o]:::attention
        A6[Sa√≠da do Modelo]:::output
        A1 --> A2
        A2 --> A4
        A3 --> A4
        A4 --> A5
        A5 --> A6
    end

    %% Transformer com Embeddings Posicionais Relativos
    subgraph B[Transformer com Embeddings Posicionais Relativos]
    direction TB
        B1[Tokens de Entrada\nx‚ÇÅ, x‚ÇÇ, ..., x‚Çô]:::transformer
        B2["Embeddings de Token\nE(x‚ÇÅ), E(x‚ÇÇ), ..., E(x‚Çô)"]:::embedding
        B3[Proje√ß√µes Q, K, V\nQ·µ¢, K·µ¢, V·µ¢]:::embedding
        B4[Embeddings Posicionais Relativos\na·µ¢‚±º, b·µ¢‚±º]:::embedding
        B5["C√°lculo da Aten√ß√£o Modificado\ne·µ¢‚±º = (Q·µ¢ ¬∑ (K‚±º + a·µ¢‚±º)·µó) / ‚àöd‚Çñ"]:::attention
        B6["Pesos de Aten√ß√£o\nŒ±·µ¢‚±º = softmax(e·µ¢‚±º)"]:::attention
        B7["Agrega√ß√£o dos Valores\ny·µ¢ = Œ£‚±º Œ±·µ¢‚±º ¬∑ (V‚±º + b·µ¢‚±º)"]:::attention
        B8[Sa√≠da do Modelo]:::output
        B1 --> B2
        B2 --> B3
        B3 --> B5
        B4 --> B5
        B5 --> B6
        B6 --> B7
        B7 --> B8
    end
```

### Introdu√ß√£o

Os **embeddings posicionais** s√£o um componente crucial na arquitetura Transformer, permitindo que o modelo capture informa√ß√µes sobre a ordem sequencial dos tokens de entrada [1]. Essa funcionalidade √© essencial, uma vez que o mecanismo de aten√ß√£o em si √© invariante √† ordem dos tokens. Neste resumo, exploraremos em profundidade duas abordagens principais para embeddings posicionais: **absolutos** e **relativos**, analisando suas caracter√≠sticas, vantagens, desvantagens e aplicabilidades em diferentes cen√°rios e tarefas de processamento de linguagem natural.

### Conceitos Fundamentais

| Conceito                             | Explica√ß√£o                                                   |
| ------------------------------------ | ------------------------------------------------------------ |
| **Embeddings Posicionais Absolutos** | Vetores √∫nicos que codificam a posi√ß√£o absoluta de cada token na sequ√™ncia. S√£o adicionados diretamente aos embeddings dos tokens. [1] |
| **Embeddings Posicionais Relativos** | ==Codificam a dist√¢ncia relativa entre pares de tokens, permitindo que o modelo capture rela√ß√µes posicionais de forma mais flex√≠vel. [2]== |
| **Invari√¢ncia √† Transla√ß√£o**         | ==Propriedade onde o significado de uma subsequ√™ncia n√£o muda com sua posi√ß√£o absoluta na sequ√™ncia==. Importante para certos tipos de tarefas de NLP. [3] |

> ‚ö†Ô∏è **Nota Importante**: A escolha entre embeddings posicionais absolutos e relativos pode impactar significativamente o desempenho do modelo em diferentes tarefas e comprimentos de sequ√™ncia.

### Embeddings Posicionais Absolutos

Os embeddings posicionais absolutos, ==introduzidos no artigo original do Transformer [1]==, s√£o ==vetores √∫nicos associados a cada posi√ß√£o na sequ√™ncia de entrada==. Eles s√£o somados diretamente aos embeddings dos tokens antes de serem processados pelas camadas de aten√ß√£o.

A formula√ß√£o matem√°tica para os embeddings posicionais absolutos, conforme proposta originalmente, √©:

$$
PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

$$
PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

Onde:

- $pos$ √© a posi√ß√£o absoluta do token na sequ√™ncia.
- $i$ √© o √≠ndice da dimens√£o do embedding.
- $d_{\text{model}}$ √© a dimens√£o do modelo.

Esta formula√ß√£o permite que o modelo aprenda a atender a posi√ß√µes absolutas na sequ√™ncia. Os embeddings posicionais absolutos t√™m algumas propriedades interessantes:

1. **Determin√≠sticos**: ==N√£o s√£o par√¢metros aprendidos, mas calculados de forma determin√≠stica.==
2. **Periodicidade**: As fun√ß√µes seno e cosseno fornecem uma periodicidade que permite ao modelo generalizar para sequ√™ncias mais longas do que as vistas durante o treinamento.
3. **Unicidade**: ==Cada posi√ß√£o tem um vetor √∫nico, permitindo que o modelo diferencie tokens em diferentes posi√ß√µes==.

#### Vantagens e Desvantagens dos Embeddings Posicionais Absolutos

üëç **Vantagens**:

- **Simplicidade de implementa√ß√£o** [4]: F√°cil de incorporar em modelos existentes.
- **Efici√™ncia computacional** [4]: Podem ser pr√©-computados e n√£o adicionam complexidade ao modelo.
- **Generaliza√ß√£o limitada para sequ√™ncias mais longas** [1]: A periodicidade ajuda, mas com limita√ß√µes.

üëé **Desvantagens**:

- **Limita√ß√£o em capturar rela√ß√µes relativas entre tokens distantes** [5]: N√£o modelam diretamente a dist√¢ncia entre tokens.
- **Potencial perda de desempenho em tarefas que requerem invari√¢ncia √† transla√ß√£o** [3]: Dependem da posi√ß√£o absoluta.
- ==**Dificuldade em lidar com sequ√™ncias muito longas al√©m do comprimento m√°ximo visto durante o treinamento** [6]:== O modelo pode n√£o generalizar bem.

#### Quest√µes T√©cnicas/Te√≥ricas

1. **Como a periodicidade dos embeddings posicionais absolutos contribui para a generaliza√ß√£o do modelo para sequ√™ncias mais longas? Explique matematicamente.**

   *Resposta*: A periodicidade das fun√ß√µes seno e cosseno permite que os embeddings posicionais tenham valores similares para posi√ß√µes que s√£o m√∫ltiplos do per√≠odo da fun√ß√£o. Isso significa que, para posi√ß√µes al√©m do que o modelo viu durante o treinamento, os embeddings ter√£o padr√µes que o modelo j√° aprendeu a interpretar, permitindo alguma generaliza√ß√£o.

2. **Em uma tarefa de classifica√ß√£o de documentos longos, como os embeddings posicionais absolutos podem impactar o desempenho do modelo?**

   *Resposta*: Se os documentos forem significativamente mais longos do que as sequ√™ncias vistas durante o treinamento, os embeddings posicionais podem atribuir valores a posi√ß√µes que o modelo n√£o sabe interpretar, levando a um desempenho inferior. O modelo pode n√£o capturar corretamente as rela√ß√µes entre tokens em posi√ß√µes distantes.

### Embeddings Posicionais Relativos

Os **embeddings posicionais relativos** foram introduzidos como uma alternativa aos embeddings absolutos para superar algumas de suas limita√ß√µes, especialmente na captura de rela√ß√µes entre tokens independentemente de sua posi√ß√£o absoluta na sequ√™ncia [7]. ==Em vez de atribuir um vetor fixo a cada posi√ß√£o absoluta, os embeddings relativos consideram a dist√¢ncia entre tokens, permitindo que o modelo compreenda como os tokens est√£o relacionados entre si em termos de posi√ß√£o.==

#### Como Funcionam os Embeddings Posicionais Relativos

No Transformer padr√£o, o c√°lculo da aten√ß√£o entre tokens √© realizado utilizando as proje√ß√µes de *query* (Q) e *key*:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Onde:

- $Q$, $K$, $V$ s√£o as proje√ß√µes dos embeddings dos tokens.
- $d_k$ √© a dimens√£o das *keys*.

Com embeddings posicionais absolutos, as informa√ß√µes de posi√ß√£o s√£o adicionadas aos embeddings dos tokens antes das proje√ß√µes. No entanto, isso limita o modelo a considerar apenas as posi√ß√µes absolutas, n√£o capturando eficientemente as rela√ß√µes relativas entre tokens.

==**Com embeddings posicionais relativos**, modificamos o c√°lculo da aten√ß√£o para incluir embeddings que representam a dist√¢ncia relativa entre tokens.== Uma formula√ß√£o proposta por Shaw et al. [7] √©:
$$
e_{ij} = \frac{(x_iW^Q)(x_jW^K)^T + (x_iW^Q)(a_{i - j})^T}{\sqrt{d_k}}
$$

Onde:

- $x_i$ √© o embedding do token na posi√ß√£o $i$.
- $W^Q$, $W^K$ s√£o matrizes de proje√ß√£o para *query* e *key*.
- $a_{i - j}$ √© o embedding da posi√ß√£o relativa entre os tokens nas posi√ß√µes $i$ e $j$.
- $e_{ij}$ √© a pontua√ß√£o de aten√ß√£o n√£o normalizada entre os tokens $i$ e $j$.

==O termo **$(x_iW^Q)(a_{i - j})^T$** adiciona ao c√°lculo de aten√ß√£o um componente que depende da dist√¢ncia relativa entre os tokens, permitindo que o modelo capture rela√ß√µes posicionais de forma mais flex√≠vel.==

Ap√≥s calcular $e_{ij}$ para todos os pares de tokens, aplicamos a fun√ß√£o softmax para obter os pesos de aten√ß√£o $\alpha_{ij}$:

$$
\alpha_{ij} = \text{softmax}(e_{ij})
$$

Finalmente, calculamos a sa√≠da $y_i$ para cada posi√ß√£o $i$:

$$
y_i = \sum_{j=1}^n \alpha_{ij}(x_jW^V)
$$

==Em algumas implementa√ß√µes, pode-se adicionar um termo adicional com embeddings de posi√ß√£o relativa aos valores:==
$$
y_i = \sum_{j=1}^n \alpha_{ij}(x_jW^V + b_{i - j})
$$

Onde $b_{i - j}$ s√£o embeddings de posi√ß√£o relativa aplicados aos valores.

#### Intui√ß√£o por Tr√°s dos Embeddings Relativos

==A ideia central √© que a rela√ß√£o entre dois tokens depende n√£o apenas de suas caracter√≠sticas sem√¢nticas, mas tamb√©m de qu√£o distantes eles est√£o na sequ√™ncia.== Por exemplo, em linguagem natural, ==palavras pr√≥ximas tendem a ter intera√ß√µes mais fortes do que palavras distantes.==

Ao modelar explicitamente a dist√¢ncia relativa, os embeddings posicionais relativos permitem que o modelo:

- **Capture rela√ß√µes locais de forma mais eficaz**: Tokens pr√≥ximos podem influenciar-se mutuamente de maneira mais significativa.
- **Generalize melhor para sequ√™ncias de diferentes comprimentos**: ==Como o modelo foca nas dist√¢ncias entre tokens, ele pode aplicar o mesmo conhecimento a sequ√™ncias mais longas ou mais curtas do que aquelas vistas durante o treinamento.==
- **Seja invariante √† posi√ß√£o absoluta**: A import√¢ncia da rela√ß√£o entre dois tokens √© determinada pela dist√¢ncia entre eles, n√£o por suas posi√ß√µes absolutas.

#### Exemplos Pr√°ticos

- **An√°lise Sint√°tica**: Em tarefas de an√°lise sint√°tica, a rela√ß√£o entre palavras (como sujeito e verbo) depende de sua proximidade na senten√ßa. ==Embeddings relativos ajudam o modelo a identificar essas rela√ß√µes independentemente da posi√ß√£o na frase.==

- **Tradu√ß√£o Autom√°tica**: ==Em tradu√ß√£o entre idiomas com ordens de palavras diferentes, √© importante capturar a rela√ß√£o entre palavras que podem estar em posi√ß√µes distintas nas duas l√≠nguas.== Embeddings relativos permitem que o modelo alinhe palavras com base em sua dist√¢ncia relativa, melhorando a qualidade da tradu√ß√£o.

#### Vantagens e Desvantagens dos Embeddings Posicionais Relativos

üëç **Vantagens**:

- **Melhor captura de rela√ß√µes locais e distantes entre tokens** [7]: Modelam diretamente a dist√¢ncia entre tokens.
- **Maior invari√¢ncia √† transla√ß√£o** [3]: O desempenho n√£o depende da posi√ß√£o absoluta dos tokens.
- **Potencial para melhor generaliza√ß√£o em sequ√™ncias longas** [8]: Adaptam-se melhor a sequ√™ncias de comprimento vari√°vel.

üëé **Desvantagens**:

- **Maior complexidade computacional** [9]: Adicionam opera√ß√µes extras no c√°lculo da aten√ß√£o.
- **Potencial aumento no n√∫mero de par√¢metros do modelo** [7]: ==Necess√°rio armazenar embeddings para m√∫ltiplas dist√¢ncias relativas.==
- **Implementa√ß√£o mais complexa comparada aos embeddings absolutos** [9]: Requer modifica√ß√µes no c√°lculo da aten√ß√£o.

#### Quest√µes T√©cnicas/Te√≥ricas

1. **Como a incorpora√ß√£o de embeddings posicionais relativos no c√°lculo da aten√ß√£o afeta a complexidade computacional do modelo? Analise em termos de opera√ß√µes de matriz.**

   *Resposta*: A incorpora√ß√£o de embeddings relativos adiciona termos extras no c√°lculo da aten√ß√£o, especificamente o produto entre as *queries* e os embeddings de posi√ß√£o relativa. Isso aumenta o n√∫mero de opera√ß√µes de multiplica√ß√£o e adi√ß√£o. ==Enquanto o c√°lculo padr√£o de aten√ß√£o tem complexidade $O(n^2d_k)$, onde $n$ √© o comprimento da sequ√™ncia e $d_k$ √© a dimens√£o das *keys*, a adi√ß√£o dos embeddings relativos mant√©m a mesma ordem de complexidade, mas com um coeficiente maior devido √†s opera√ß√µes adicionais.==

2. **Em uma tarefa de tradu√ß√£o autom√°tica, como os embeddings posicionais relativos podem melhorar o alinhamento entre palavras de idiomas com estruturas sint√°ticas diferentes? D√™ um exemplo concreto.**

   *Resposta*: Em idiomas como o ingl√™s e o japon√™s, a ordem das palavras pode ser diferente (Sujeito-Verbo-Objeto vs. Sujeito-Objeto-Verbo). Embeddings relativos permitem que o modelo reconhe√ßa que certas palavras est√£o relacionadas, independentemente de sua posi√ß√£o absoluta. Por exemplo, o verbo pode aparecer em posi√ß√µes diferentes nas duas l√≠nguas, mas a dist√¢ncia relativa entre o sujeito e o verbo pode ser consistente, ajudando o modelo a alinhar corretamente essas palavras durante a tradu√ß√£o.

#### Considera√ß√µes Pr√°ticas

- **Limita√ß√£o do Alcance das Dist√¢ncias**:

  - Na pr√°tica, limitamos o alcance das dist√¢ncias relativas para gerenciar o n√∫mero de embeddings e a mem√≥ria utilizada.
  - Dist√¢ncias maiores que um certo limite s√£o agrupadas em um √∫nico embedding.

- **Compartilhamento de Embeddings**:

  - Os embeddings de posi√ß√£o relativa podem ser compartilhados entre camadas ou cabe√ßas de aten√ß√£o para reduzir o n√∫mero de par√¢metros.

- **Efici√™ncia**:

  - T√©cnicas como *windowing* (janelamento) ou mascaramento podem ser usadas para reduzir o n√∫mero de c√°lculos, especialmente em sequ√™ncias muito longas.

#### Implementa√ß√£o e Exemplo em PyTorch

Implementar embeddings posicionais relativos requer modificar o c√°lculo da aten√ß√£o. Aqui est√° um esbo√ßo simplificado:

```python
import torch
import torch.nn as nn

class RelativeMultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, max_len=5000):
        super().__init__()
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.max_len = max_len

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        self.rel_k = nn.Parameter(torch.randn(2 * max_len - 1, self.d_k))
        self.rel_v = nn.Parameter(torch.randn(2 * max_len - 1, self.d_k))

    def forward(self, x):
        batch_size, seq_len, _ = x.size()

        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k)

        # Calcula pontua√ß√µes de aten√ß√£o com embeddings relativos
        # ...

        # Implementa√ß√£o detalhada requer manipula√ß√£o cuidadosa das tensores
        # e √≠ndices para incorporar os embeddings de posi√ß√£o relativa.

        return output
```

> ‚ùó **Ponto de Aten√ß√£o**: A implementa√ß√£o de embeddings posicionais relativos pode variar significativamente dependendo da abordagem espec√≠fica escolhida. A vers√£o acima √© uma simplifica√ß√£o e pode requerer ajustes para casos de uso espec√≠ficos.

### Exemplo Num√©rico de Embeddings Posicionais Relativos

Para ilustrar como os **embeddings posicionais relativos** funcionam na pr√°tica, vamos considerar um exemplo simples com uma sequ√™ncia de quatro tokens:

**Sequ√™ncia de Tokens**:

1. **Token 0**: "Eu"
2. **Token 1**: "amo"
3. **Token 2**: "aprendizado"
4. **Token 3**: "profundo"

Vamos assumir que:

- A dimens√£o dos embeddings (d_model) √© **2** para simplificar.
- ==As matrizes de proje√ß√£o $W^Q$ e $W^K$ s√£o a **matriz identidade** (ou seja, n√£o alteram os embeddings originais).==
- Os embeddings dos tokens s√£o vetores de dimens√£o 2.
  
#### Embeddings dos Tokens ($x_i$)

| Posi√ß√£o (i) | Token         | Embedding ($x_i$) |
| ----------- | ------------- | ----------------- |
| 0           | "Eu"          | $[1, 0]$          |
| 1           | "amo"         | $[0, 1]$          |
| 2           | "aprendizado" | $[1, 1]$          |
| 3           | "profundo"    | $[0, 0]$          |

#### Embeddings de Posi√ß√£o Relativa ($r_{k}$)

Consideramos posi√ß√µes relativas de $-3$ a $+3$:

| Posi√ß√£o Relativa (k) | $r_k$     |
| -------------------- | --------- |
| $-3$                 | $[0, 0]$  |
| $-2$                 | $[0, 1]$  |
| $-1$                 | $[1, 0]$  |
| $0$                  | $[1, 1]$  |
| $+1$                 | $[0, -1]$ |
| $+2$                 | $[-1, 0]$ |
| $+3$                 | $[0, 0]$  |

#### C√°lculo das Pontua√ß√µes de Aten√ß√£o ($e_{ij}$)

A pontua√ß√£o de aten√ß√£o entre os tokens na posi√ß√£o $i$ e $j$ √© dada por:

$$
e_{ij} = q_i^T k_j + q_i^T r_{i - j}
$$

Onde:

- $q_i = W^Q x_i = x_i$ (j√° que $W^Q$ √© a matriz identidade)
- $k_j = W^K x_j = x_j$ (j√° que $W^K$ √© a matriz identidade)
- ==$r_{i - j}$ √© o embedding da posi√ß√£o relativa entre $i$ e $j$==

Vamos calcular $e_{ij}$ para todos os pares $(i, j)$:

#### Tabela de C√°lculos

| $i$  | $j$  | $x_i$  | $x_j$  | $x_i^T x_j$     | $i - j$ | $r_{i - j}$ | $x_i^T r_{i - j}$   | $e_{ij} = x_i^T x_j + x_i^T r_{i - j}$ |
| ---- | ---- | ------ | ------ | --------------- | ------- | ----------- | ------------------- | -------------------------------------- |
| 0    | 0    | [1, 0] | [1, 0] | $1*1 + 0*0 = 1$ | 0       | [1, 1]      | $1*1 + 0*1 = 1$     | $1 + 1 = 2$                            |
| 0    | 1    | [1, 0] | [0, 1] | $1*0 + 0*1 = 0$ | -1      | [1, 0]      | $1*1 + 0*0 = 1$     | $0 + 1 = 1$                            |
| 0    | 2    | [1, 0] | [1, 1] | $1*1 + 0*1 = 1$ | -2      | [0, 1]      | $1*0 + 0*1 = 0$     | $1 + 0 = 1$                            |
| 0    | 3    | [1, 0] | [0, 0] | $1*0 + 0*0 = 0$ | -3      | [0, 0]      | $1*0 + 0*0 = 0$     | $0 + 0 = 0$                            |
| 1    | 0    | [0, 1] | [1, 0] | $0*1 + 1*0 = 0$ | 1       | [0, -1]     | $0*0 + 1*(-1) = -1$ | $0 + (-1) = -1$                        |
| 1    | 1    | [0, 1] | [0, 1] | $0*0 + 1*1 = 1$ | 0       | [1, 1]      | $0*1 + 1*1 = 1$     | $1 + 1 = 2$                            |
| 1    | 2    | [0, 1] | [1, 1] | $0*1 + 1*1 = 1$ | -1      | [1, 0]      | $0*1 + 1*0 = 0$     | $1 + 0 = 1$                            |
| 1    | 3    | [0, 1] | [0, 0] | $0*0 + 1*0 = 0$ | -2      | [0, 1]      | $0*0 + 1*1 = 1$     | $0 + 1 = 1$                            |
| 2    | 0    | [1, 1] | [1, 0] | $1*1 + 1*0 = 1$ | 2       | [-1, 0]     | $1*(-1) + 1*0 = -1$ | $1 + (-1) = 0$                         |
| 2    | 1    | [1, 1] | [0, 1] | $1*0 + 1*1 = 1$ | 1       | [0, -1]     | $1*0 + 1*(-1) = -1$ | $1 + (-1) = 0$                         |
| 2    | 2    | [1, 1] | [1, 1] | $1*1 + 1*1 = 2$ | 0       | [1, 1]      | $1*1 + 1*1 = 2$     | $2 + 2 = 4$                            |
| 2    | 3    | [1, 1] | [0, 0] | $1*0 + 1*0 = 0$ | -1      | [1, 0]      | $1*1 + 1*0 = 1$     | $0 + 1 = 1$                            |
| 3    | 0    | [0, 0] | [1, 0] | $0*1 + 0*0 = 0$ | 3       | [0, 0]      | $0*0 + 0*0 = 0$     | $0 + 0 = 0$                            |
| 3    | 1    | [0, 0] | [0, 1] | $0*0 + 0*1 = 0$ | 2       | [-1, 0]     | $0*(-1) + 0*0 = 0$  | $0 + 0 = 0$                            |
| 3    | 2    | [0, 0] | [1, 1] | $0*1 + 0*1 = 0$ | 1       | [0, -1]     | $0*0 + 0*(-1) = 0$  | $0 + 0 = 0$                            |
| 3    | 3    | [0, 0] | [0, 0] | $0*0 + 0*0 = 0$ | 0       | [1, 1]      | $0*1 + 0*1 = 0$     | $0 + 0 = 0$                            |

#### Interpreta√ß√£o dos Resultados

- **Pontua√ß√µes Elevadas**: Os pares $(i, j)$ com valores altos de $e_{ij}$ indicam que o token na posi√ß√£o $i$ est√° dando mais aten√ß√£o ao token na posi√ß√£o $j$.

- **Impacto da Posi√ß√£o Relativa**:

  - Por exemplo, $e_{2,2} = 4$ √© alto porque tanto $x_2^T x_2 = 2$ quanto $x_2^T r_0 = 2$.
  
  - O termo $x_i^T r_{i - j}$ ajusta a pontua√ß√£o de aten√ß√£o com base na dist√¢ncia entre os tokens, permitindo que o modelo capture depend√™ncias posicionais de forma mais eficaz.

#### C√°lculo da Aten√ß√£o

Ap√≥s obter as pontua√ß√µes $e_{ij}$, aplicamos a fun√ß√£o *softmax* para obter os pesos de aten√ß√£o $\alpha_{ij}$:

$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=0}^{n-1} \exp(e_{ik})}
$$

**Exemplo para $i = 0$**:

Calculamos $\alpha_{0j}$ para $j = 0$ a $3$:

1. **Somat√≥rio das Exponenciais**:

   $$
   \sum_{k=0}^{3} \exp(e_{0k}) = \exp(2) + \exp(1) + \exp(1) + \exp(0) \approx 7.389 + 2.718 + 2.718 + 1 = 13.825
   $$

2. **Pesos de Aten√ß√£o**:

   - $\alpha_{00} = \frac{\exp(2)}{13.825} \approx \frac{7.389}{13.825} \approx 0.534$
   - $\alpha_{01} = \frac{\exp(1)}{13.825} \approx \frac{2.718}{13.825} \approx 0.197$
   - $\alpha_{02} = \frac{\exp(1)}{13.825} \approx 0.197$
   - $\alpha_{03} = \frac{\exp(0)}{13.825} \approx \frac{1}{13.825} \approx 0.072$

3. **Interpreta√ß√£o**:

   - O token na posi√ß√£o 0 ("Eu") presta mais aten√ß√£o a si mesmo ($\alpha_{00} \approx 53.4\%$).
   - A aten√ß√£o aos tokens nas posi√ß√µes 1 e 2 √© menor, mas significativa ($\approx 19.7\%$ cada).
   - A aten√ß√£o ao token na posi√ß√£o 3 √© a menor ($\approx 7.2\%$).

#### Resumo do Processo

1. **Proje√ß√µes**: Calculamos $q_i$ e $k_j$ projetando os embeddings dos tokens.

2. **Embeddings Relativos**: Obtemos $r_{i - j}$ para cada par $(i, j)$ com base na dist√¢ncia relativa.

3. **Pontua√ß√µes de Aten√ß√£o**: Computamos $e_{ij} = q_i^T k_j + q_i^T r_{i - j}$.

4. **Pesos de Aten√ß√£o**: Aplicamos a fun√ß√£o *softmax* √†s pontua√ß√µes $e_{ij}$ para obter $\alpha_{ij}$.

5. **Agrega√ß√£o**: Calculamos a sa√≠da $y_i$ para cada posi√ß√£o $i$:

   $$
   y_i = \sum_{j=0}^{n-1} \alpha_{ij} v_j
   $$

   Onde $v_j$ s√£o as proje√ß√µes dos valores (neste exemplo, assumimos $v_j = x_j$ para simplificar).

### Visualiza√ß√£o do Processo

```mermaid
flowchart TD
    
    %% N√≥s
    A["Embeddings dos Tokens<br/>$x_i$"]
    B["Proje√ß√µes Query e Key<br/>$q_i = x_i$, $k_j = x_j$"]
    C["Embeddings Relativos<br/>$r_{i - j}$"]
    D["Pontua√ß√µes de Aten√ß√£o<br/>$e_{ij} = q_i^T k_j + q_i^T r_{i - j}$"]
    E["Pesos de Aten√ß√£o<br/>$\alpha_{ij} = \text{softmax}(e_{ij})$"]
    F["Agrega√ß√£o dos Valores<br/>$y_i = \sum_j \alpha_{ij} v_j$"]
    
    %% Conex√µes
    A --> B
    B --> D
    C --> D
    D --> E
    E --> F
```

### Compara√ß√£o e An√°lise

Para uma compara√ß√£o mais detalhada entre embeddings posicionais absolutos e relativos, consideremos os seguintes aspectos:

| Aspecto                                  | Embeddings Absolutos | Embeddings Relativos |
| ---------------------------------------- | -------------------- | -------------------- |
| **Captura de Contexto Local**            | Limitada [5]         | Superior [7]         |
| **Invari√¢ncia √† Transla√ß√£o**             | Baixa [3]            | Alta [3]             |
| **Efici√™ncia Computacional**             | Alta [4]             | Moderada a Baixa [9] |
| **Generaliza√ß√£o para Sequ√™ncias Longas** | Moderada [1]         | Alta [8]             |
| **Complexidade de Implementa√ß√£o**        | Baixa [4]            | Alta [9]             |

> ‚úîÔ∏è **Ponto de Destaque**: A escolha entre embeddings absolutos e relativos deve ser baseada nas caracter√≠sticas espec√≠ficas da tarefa, no comprimento das sequ√™ncias e nos recursos computacionais dispon√≠veis.

### Implementa√ß√£o e Considera√ß√µes Pr√°ticas

Ao implementar embeddings posicionais em um modelo Transformer, √© crucial considerar o trade-off entre desempenho e efici√™ncia. Aqui est√° um exemplo simplificado de como implementar embeddings posicionais absolutos em PyTorch:

```python
import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)  # dimens√µes pares
        pe[:, 1::2] = torch.cos(position * div_term)  # dimens√µes √≠mpares

        pe = pe.unsqueeze(0)  # adiciona dimens√£o do batch
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]
```

Para embeddings posicionais relativos, a implementa√ß√£o √© mais complexa e geralmente envolve modificar o c√°lculo da aten√ß√£o. Aqui est√° um esbo√ßo simplificado:

```python
import torch
import torch.nn as nn

class RelativePositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        self.d_model = d_model
        self.max_len = max_len
        self.rel_embeddings = nn.Embedding(2 * max_len - 1, d_model)

    def forward(self, length):
        range_vec = torch.arange(length)
        distance_mat = range_vec[None, :] - range_vec[:, None] + self.max_len - 1
        return self.rel_embeddings(distance_mat)
```

> ‚ùó **Ponto de Aten√ß√£o**: A implementa√ß√£o completa requer integra√ß√£o desses embeddings no c√°lculo de aten√ß√£o, modificando as opera√ß√µes de matriz para incluir as informa√ß√µes de posi√ß√£o relativa.

### Conclus√£o

A escolha entre embeddings posicionais absolutos e relativos √© crucial para o desempenho de modelos Transformer em diversas tarefas de NLP. **Embeddings absolutos** oferecem simplicidade e efici√™ncia, sendo adequados para muitas aplica√ß√µes padr√£o. Por outro lado, **embeddings relativos** proporcionam maior flexibilidade e potencial de generaliza√ß√£o, especialmente em tarefas que envolvem sequ√™ncias longas ou requerem invari√¢ncia √† transla√ß√£o [3][7][8].

A decis√£o deve ser baseada nas caracter√≠sticas espec√≠ficas da tarefa, nos recursos computacionais dispon√≠veis e nos requisitos de desempenho do modelo. Em alguns casos, abordagens h√≠bridas ou varia√ß√µes mais recentes desses m√©todos podem oferecer o melhor equil√≠brio entre desempenho e efici√™ncia.

√Ä medida que a pesquisa em NLP avan√ßa, √© prov√°vel que surjam novas t√©cnicas de embedding posicional, possivelmente combinando os pontos fortes das abordagens absoluta e relativa ou introduzindo conceitos inteiramente novos para capturar informa√ß√µes posicionais em modelos de linguagem.

### Quest√µes Avan√ßadas

1. **Como voc√™ projetaria um experimento para comparar o desempenho de embeddings posicionais absolutos e relativos em uma tarefa de sumariza√ß√£o de documentos longos?**

   *Resposta*: Criaria um conjunto de dados com documentos de diferentes comprimentos, incluindo textos que excedem o comprimento m√°ximo visto durante o treinamento. Treinaria dois modelos id√™nticos, um com embeddings absolutos e outro com embeddings relativos, e avaliaria o desempenho em termos de m√©tricas como ROUGE ou BLEU. Analisaria como cada modelo lida com a captura de informa√ß√µes de contexto distante e a coer√™ncia geral do resumo.

2. **Analise o impacto potencial de embeddings posicionais relativos na interpretabilidade de modelos de linguagem. Como eles podem afetar nossa capacidade de entender as decis√µes do modelo em tarefas como an√°lise de sentimentos ou extra√ß√£o de entidades?**

   *Resposta*: Embeddings relativos podem tornar mais complexo o rastreamento de quais tokens influenciam as decis√µes do modelo, pois a aten√ß√£o depende de dist√¢ncias relativas. No entanto, eles tamb√©m podem oferecer insights sobre quais rela√ß√µes posicionais s√£o importantes, melhorando a interpretabilidade em termos de depend√™ncias sint√°ticas ou sem√¢nticas.

3. **Proponha uma abordagem h√≠brida que combine elementos de embeddings posicionais absolutos e relativos. Como essa abordagem poderia superar as limita√ß√µes de cada m√©todo individual?**

   *Resposta*: Uma abordagem h√≠brida poderia somar os embeddings absolutos aos embeddings dos tokens e, ao mesmo tempo, incorporar embeddings relativos no c√°lculo da aten√ß√£o. Isso permitiria ao modelo beneficiar-se da simplicidade dos embeddings absolutos e da flexibilidade dos embeddings relativos. O desafio seria equilibrar a complexidade computacional e garantir que as informa√ß√µes posicionais n√£o se sobreponham de forma redundante.

### Refer√™ncias

[1] "Embeddings posicionais absolutos, introduzidos no artigo original do Transformer, s√£o vetores √∫nicos associados a cada posi√ß√£o na sequ√™ncia de entrada. Eles s√£o somados diretamente aos embeddings dos tokens antes de serem processados pelas camadas de aten√ß√£o." (Trecho de Transformers and Large Language Models - Chapter 10)

[2] "Embeddings Posicionais Relativos: Codificam a dist√¢ncia relativa entre pares de tokens, permitindo que o modelo capture rela√ß√µes posicionais de forma mais flex√≠vel." (Trecho de Transformers and Large Language Models - Chapter 10)

[3] "Invari√¢ncia √† Transla√ß√£o: Propriedade onde o significado de uma subsequ√™ncia n√£o muda com sua posi√ß√£o absoluta na sequ√™ncia. Importante para certos tipos de tarefas de NLP." (Trecho de Transformers and Large Language Models - Chapter 10)

[4] "Simplicidade de implementa√ß√£o [...] Efici√™ncia computacional (podem ser pr√©-computados)" (Trecho de Transformers and Large Language Models - Chapter 10)

[5] "Limita√ß√£o em capturar rela√ß√µes relativas entre tokens distantes" (Trecho de Transformers and Large Language Models - Chapter 10)

[6] "Dificuldade em lidar com sequ√™ncias muito longas al√©m do comprimento m√°ximo visto durante o treinamento" (Trecho de Transformers and Large Language Models - Chapter 10)

[7] "Os embeddings posicionais relativos foram introduzidos como uma alternativa aos embeddings absolutos, visando superar algumas de suas limita√ß√µes [...] Uma implementa√ß√£o mais sofisticada, proposta por Shaw et al., introduz embeddings de posi√ß√£o relativa a_{ij} e b_{ij} no c√°lculo da aten√ß√£o" (Trecho de Transformers and Large Language Models - Chapter 10)

[8] "Potencial para melhor generaliza√ß√£o em sequ√™ncias longas" (Trecho de Transformers and Large Language Models - Chapter 10)

[9] "Maior complexidade computacional [...] Implementa√ß√£o mais complexa comparada aos embeddings absolutos" (Trecho de Transformers and Large Language Models - Chapter 10)