## Positional Embeddings em Transformers: T√©cnicas Avan√ßadas e Impacto no Desempenho

<image: Uma visualiza√ß√£o abstrata de uma sequ√™ncia de tokens, cada um com um vetor de embedding associado, e setas indicando a posi√ß√£o relativa entre os tokens, representando as diferentes t√©cnicas de codifica√ß√£o posicional.>

### Introdu√ß√£o

Os **Positional Embeddings** s√£o um componente crucial na arquitetura dos Transformers, permitindo que esses modelos capturem informa√ß√µes sobre a ordem sequencial dos tokens de entrada, algo que a aten√ß√£o por si s√≥ n√£o consegue fazer [1]. Esta t√©cnica √© fundamental para preservar a informa√ß√£o posicional em modelos que, diferentemente das arquiteturas recorrentes, processam todos os tokens de uma sequ√™ncia simultaneamente.

Neste resumo, exploraremos em profundidade as v√°rias t√©cnicas de codifica√ß√£o posicional, analisando seu impacto no desempenho do modelo e na capacidade de capturar depend√™ncias de longo alcance. Focando principalmente em embeddings posicionais absolutos e relativos, discutiremos suas implementa√ß√µes, vantagens, desvantagens e implica√ß√µes pr√°ticas para o desenvolvimento de modelos de linguagem avan√ßados.

### Conceitos Fundamentais

| Conceito                          | Explica√ß√£o                                                   |
| --------------------------------- | ------------------------------------------------------------ |
| **Positional Embeddings**         | Vetores que codificam a posi√ß√£o de cada token em uma sequ√™ncia, permitindo que modelos baseados em aten√ß√£o incorporem informa√ß√µes sobre a ordem dos tokens [1]. |
| **Embeddings Absolutos**          | T√©cnica que atribui um vetor √∫nico para cada posi√ß√£o poss√≠vel na sequ√™ncia, independentemente de outros tokens [2]. |
| **Embeddings Relativos**          | Abordagem que codifica a dist√¢ncia relativa entre tokens, potencialmente permitindo uma melhor generaliza√ß√£o para sequ√™ncias de diferentes comprimentos [3]. |
| **Depend√™ncias de Longo Alcance** | Capacidade do modelo de capturar e utilizar informa√ß√µes de tokens distantes na sequ√™ncia, crucial para tarefas que requerem compreens√£o de contexto amplo [4]. |

> ‚ö†Ô∏è **Nota Importante**: A escolha do m√©todo de embedding posicional pode afetar significativamente o desempenho do modelo em diferentes tarefas e comprimentos de sequ√™ncia.

### Embeddings Posicionais Absolutos

<image: Um diagrama mostrando uma matriz de embeddings posicionais absolutos, com cada linha representando um vetor de embedding para uma posi√ß√£o espec√≠fica na sequ√™ncia.>

Os embeddings posicionais absolutos, introduzidos no artigo original do Transformer [1], s√£o uma t√©cnica direta para incorporar informa√ß√µes de posi√ß√£o. Cada posi√ß√£o na sequ√™ncia √© associada a um vetor √∫nico, que √© somado ao embedding do token correspondente.

A implementa√ß√£o original utiliza fun√ß√µes senoidais para gerar esses embeddings:

$$
PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{model}})
$$
$$
PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{model}})
$$

Onde:
- $pos$ √© a posi√ß√£o na sequ√™ncia
- $i$ √© a dimens√£o no vetor de embedding
- $d_{model}$ √© a dimens√£o do modelo

Esta formula√ß√£o permite que o modelo aprenda a atender a posi√ß√µes relativas com facilidade, pois para qualquer offset fixo $k$, $PE_{pos+k}$ pode ser representado como uma fun√ß√£o linear de $PE_{pos}$ [1].

#### Vantagens e Desvantagens

| üëç Vantagens                                          | üëé Desvantagens                                               |
| ---------------------------------------------------- | ------------------------------------------------------------ |
| Implementa√ß√£o simples e eficiente [5]                | Limitado a um comprimento m√°ximo de sequ√™ncia fixo [6]       |
| Funciona bem para sequ√™ncias de comprimento moderado | Pode n√£o generalizar bem para posi√ß√µes n√£o vistas [7]        |
| Permite interpola√ß√£o para posi√ß√µes n√£o treinadas [1] | Pode ter dificuldades com depend√™ncias de muito longo alcance [8] |

#### Implementa√ß√£o em PyTorch

```python
import torch
import torch.nn as nn

class AbsolutePositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0)]
```

Este c√≥digo implementa os embeddings posicionais absolutos conforme descrito no artigo "Attention is All You Need" [1]. A classe `AbsolutePositionalEncoding` cria uma matriz de embeddings posicionais que √© somada √†s entradas do modelo.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como os embeddings posicionais absolutos permitem que o modelo diferencie tokens id√™nticos em posi√ß√µes diferentes da sequ√™ncia?
2. Quais s√£o as implica√ß√µes de usar embeddings posicionais fixos versus aprendidos para o desempenho e a generaliza√ß√£o do modelo?

### Embeddings Posicionais Relativos

<image: Uma visualiza√ß√£o de uma matriz de aten√ß√£o com setas indicando as rela√ß√µes entre diferentes posi√ß√µes, destacando a natureza relativa dos embeddings.>

Os embeddings posicionais relativos s√£o uma evolu√ß√£o dos absolutos, visando melhorar a capacidade do modelo de generalizar para sequ√™ncias de diferentes comprimentos e capturar depend√™ncias de longo alcance mais eficientemente [9].

Em vez de associar um vetor fixo a cada posi√ß√£o absoluta, os embeddings relativos codificam a dist√¢ncia entre tokens. Isso permite que o modelo considere as rela√ß√µes relativas entre os tokens, independentemente de suas posi√ß√µes absolutas na sequ√™ncia.

Uma implementa√ß√£o comum de embeddings relativos √© atrav√©s da modifica√ß√£o do c√°lculo de aten√ß√£o [10]:

$$
Attention(Q, K, V) = softmax(\frac{QK^T + R}{\sqrt{d_k}})V
$$

Onde $R$ √© uma matriz que codifica as rela√ß√µes posicionais relativas entre todos os pares de tokens.

#### Vantagens e Desvantagens

| üëç Vantagens                                                  | üëé Desvantagens                                              |
| ------------------------------------------------------------ | ----------------------------------------------------------- |
| Melhor generaliza√ß√£o para sequ√™ncias de diferentes tamanhos [11] | Implementa√ß√£o mais complexa que embeddings absolutos [12]   |
| Captura eficiente de depend√™ncias de longo alcance [13]      | Pode aumentar o custo computacional [14]                    |
| N√£o limitado a um comprimento m√°ximo de sequ√™ncia fixo [15]  | Requer ajustes espec√≠ficos para cada cabe√ßa de aten√ß√£o [16] |

#### Implementa√ß√£o em PyTorch

```python
import torch
import torch.nn as nn

class RelativePositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        self.d_model = d_model
        self.max_len = max_len
        self.embeddings = nn.Parameter(torch.randn(2 * max_len - 1, d_model))

    def forward(self, length):
        pos_emb = self.embeddings.unsqueeze(0).expand(length, -1, -1)
        pos = torch.arange(length, dtype=torch.long, device=self.embeddings.device).unsqueeze(1)
        pos_emb = pos_emb.gather(1, pos.expand(-1, self.d_model).unsqueeze(1)).squeeze(1)
        return pos_emb

class RelativeMultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads

        self.qkv_proj = nn.Linear(d_model, 3 * d_model)
        self.rel_pos_encoding = RelativePositionalEncoding(self.head_dim)
        self.out_proj = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, T, _ = x.shape
        qkv = self.qkv_proj(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: t.view(B, T, self.num_heads, self.head_dim).transpose(1, 2), qkv)

        rel_pos = self.rel_pos_encoding(T)
        rel_pos = rel_pos.view(T, self.num_heads, self.head_dim).permute(1, 0, 2)

        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        rel_attn = q @ rel_pos.transpose(-2, -1)
        attn = attn + rel_attn.view(B, self.num_heads, T, T)

        attn = attn.softmax(dim=-1)
        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, self.d_model)
        return self.out_proj(out)
```

Esta implementa√ß√£o demonstra uma abordagem de embeddings posicionais relativos usando aten√ß√£o de m√∫ltiplas cabe√ßas. A classe `RelativePositionalEncoding` gera embeddings relativos, enquanto `RelativeMultiHeadAttention` incorpora esses embeddings no c√°lculo da aten√ß√£o.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como os embeddings posicionais relativos superam as limita√ß√µes dos embeddings absolutos em rela√ß√£o √† generaliza√ß√£o para sequ√™ncias mais longas?
2. Quais s√£o as considera√ß√µes de design ao implementar embeddings relativos em arquiteturas de aten√ß√£o de m√∫ltiplas cabe√ßas?

### Impacto no Desempenho do Modelo

O uso de diferentes t√©cnicas de embedding posicional pode ter um impacto significativo no desempenho do modelo, especialmente em tarefas que envolvem depend√™ncias de longo alcance ou sequ√™ncias de comprimento vari√°vel [17].

#### An√°lise Comparativa

| Aspecto                       | Embeddings Absolutos                   | Embeddings Relativos                                   |
| ----------------------------- | -------------------------------------- | ------------------------------------------------------ |
| Captura de Contexto Local     | Bom para contextos pr√≥ximos [18]       | Excelente para contextos locais e distantes [19]       |
| Generaliza√ß√£o                 | Limitada a comprimentos treinados [20] | Melhor generaliza√ß√£o para diferentes comprimentos [21] |
| Complexidade Computacional    | Menor [22]                             | Maior, mas potencialmente mais eficaz [23]             |
| Depend√™ncias de Longo Alcance | Pode degradar com a dist√¢ncia [24]     | Mant√©m efic√°cia em longas dist√¢ncias [25]              |

> ‚úîÔ∏è **Ponto de Destaque**: Embeddings relativos t√™m mostrado desempenho superior em tarefas que requerem compreens√£o de contexto amplo e generaliza√ß√£o para sequ√™ncias de comprimento vari√°vel [26].

### T√©cnicas Avan√ßadas e Varia√ß√µes

Al√©m dos m√©todos b√°sicos de embeddings absolutos e relativos, v√°rias t√©cnicas avan√ßadas t√™m sido propostas para melhorar ainda mais o desempenho dos modelos:

1. **Embeddings Posicionais Aprendidos**: Em vez de usar fun√ß√µes predefinidas, alguns modelos aprendem os embeddings posicionais durante o treinamento [27].

2. **Embeddings Posicionais Hier√°rquicos**: Combinam embeddings de diferentes escalas para capturar tanto informa√ß√µes locais quanto globais [28].

3. **Rotary Position Embeddings (RoPE)**: Uma t√©cnica que aplica uma rota√ß√£o aos vetores de query e key baseada na posi√ß√£o relativa, permitindo uma incorpora√ß√£o eficiente de informa√ß√µes posicionais [29].

4. **Attention with Linear Biases (ALiBi)**: Adiciona um vi√©s linear √† matriz de aten√ß√£o baseado na dist√¢ncia relativa entre tokens, eliminando a necessidade de embeddings posicionais expl√≠citos [30].

#### Implementa√ß√£o do RoPE em PyTorch

```python
import torch
import torch.nn as nn

class RotaryPositionalEmbedding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        inv_freq = 1. / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))
        self.register_buffer('inv_freq', inv_freq)
        self.max_len = max_len

    def forward(self, x, seq_len):
        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
        freqs = torch.einsum('i,j->ij', t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        return emb[None, :, :]

def rotate_half(x):
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, pos_emb):
    q_embed = (q * pos_emb.cos()) + (rotate_half(q) * pos_emb.sin())
    k_embed = (k * pos_emb.cos()) + (rotate_half(k) * pos_emb.sin())
    return q_embed, k_embed

class RotaryAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads

        self.qkv_proj = nn.Linear(d_model, 3 * d_model)
        self.rope = RotaryPositionalEmbedding(self.head_dim)
        self.out_proj = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, T, _ = x.shape
        qkv = self.qkv_proj(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: t.view(B, T, self.num_heads, self.head_dim).transpose(1, 2), qkv)

        pos_emb = self.rope(x, seq_len=T)
        q, k = apply_rotary_pos_emb(q, k, pos_emb)

        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = attn.softmax(dim=-1)
        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, self.d_model)
        return self.out_proj(out)
```

Esta implementa√ß√£o demonstra o uso de Rotary Position Embeddings (RoPE), uma t√©cnica avan√ßada que aplica uma rota√ß√£o aos vetores de query e key baseada na posi√ß√£o relativa.

### Conclus√£o

Os embeddings posicionais s√£o um componente crucial dos modelos Transformer, permitindo que eles capturem informa√ß√µes sequ enciais cruciais. Ao longo deste resumo, exploramos diferentes t√©cnicas de codifica√ß√£o posicional, desde os embeddings absolutos originais at√© m√©todos mais avan√ßados como os embeddings relativos e t√©cnicas como RoPE e ALiBi.

Observamos que, embora os embeddings absolutos ofere√ßam uma solu√ß√£o simples e eficaz para sequ√™ncias de comprimento moderado, os embeddings relativos e outras t√©cnicas avan√ßadas demonstram vantagens significativas em termos de generaliza√ß√£o e capacidade de capturar depend√™ncias de longo alcance [31]. Essas abordagens mais sofisticadas permitem que os modelos lidem melhor com sequ√™ncias de comprimento vari√°vel e mantenham a efic√°cia em contextos mais amplos [32].

A escolha do m√©todo de embedding posicional deve ser considerada cuidadosamente com base nas caracter√≠sticas espec√≠ficas da tarefa e nos requisitos do modelo. Fatores como o comprimento t√≠pico das sequ√™ncias, a import√¢ncia das depend√™ncias de longo alcance e os recursos computacionais dispon√≠veis devem influenciar essa decis√£o [33].

√Ä medida que a pesquisa em arquiteturas de transformers continua a evoluir, √© prov√°vel que vejamos o desenvolvimento de t√©cnicas ainda mais avan√ßadas para codifica√ß√£o posicional. Essas inova√ß√µes poder√£o potencialmente melhorar ainda mais o desempenho dos modelos em uma variedade de tarefas de processamento de linguagem natural e al√©m [34].

> üí° **Insight Crucial**: A evolu√ß√£o dos embeddings posicionais de absolutos para relativos e t√©cnicas mais avan√ßadas reflete uma tend√™ncia geral em dire√ß√£o a modelos mais flex√≠veis e capazes de generalizar melhor para uma variedade de tarefas e comprimentos de sequ√™ncia [35].

### Quest√µes Avan√ßadas

1. Como voc√™ projetaria um experimento para comparar o desempenho de diferentes t√©cnicas de embedding posicional em tarefas que envolvem depend√™ncias de longo alcance, como a an√°lise de documentos longos ou a tradu√ß√£o de textos complexos?

2. Considerando as limita√ß√µes dos embeddings posicionais atuais, proponha uma nova abordagem que poderia potencialmente superar essas limita√ß√µes. Quais seriam os princ√≠pios te√≥ricos por tr√°s dessa nova t√©cnica e como ela se compararia com os m√©todos existentes?

3. Analise o impacto computacional e de mem√≥ria das diferentes t√©cnicas de embedding posicional discutidas. Como essas considera√ß√µes afetam a escolha da t√©cnica para diferentes escalas de modelo (por exemplo, modelos pequenos vs. modelos de bilh√µes de par√¢metros)?

4. Discuta como as t√©cnicas de embedding posicional poderiam ser adaptadas ou estendidas para lidar com dados estruturados n√£o sequenciais, como grafos ou imagens 3D. Quais seriam os desafios e poss√≠veis abordagens para tais adapta√ß√µes?

5. Considerando o conceito de "aten√ß√£o eficiente" em transformers (por exemplo, Linformer, Performer), como as t√©cnicas de embedding posicional poderiam ser integradas ou modificadas para manter a efic√°cia enquanto reduzem a complexidade computacional em modelos de grande escala?

### Refer√™ncias

[1] "Positional Embeddings s√£o um componente crucial na arquitetura dos Transformers, permitindo que esses modelos capturem informa√ß√µes sobre a ordem sequencial dos tokens de entrada, algo que a aten√ß√£o por si s√≥ n√£o consegue fazer" (Trecho de Transformers and Large Language Models - Chapter 10)

[2] "A transformer does this by separately computing two embeddings: an input token embedding, and an input positional embedding." (Trecho de Transformers and Large Language Models - Chapter 10)

[3] "Abordagem que codifica a dist√¢ncia relativa entre tokens, potencialmente permitindo uma melhor generaliza√ß√£o para sequ√™ncias de diferentes comprimentos" (Trecho de Transformers and Large Language Models - Chapter 10)

[4] "Capacidade do modelo de capturar e utilizar informa√ß√µes de tokens distantes na sequ√™ncia, crucial para tarefas que requerem compreens√£o de contexto amplo" (Trecho de Transformers and Large Language Models - Chapter 10)

[5] "Implementa√ß√£o simples e eficiente" (Trecho de Transformers and Large Language Models - Chapter 10)

[6] "Limitado a um comprimento m√°ximo de sequ√™ncia fixo" (Trecho de Transformers and Large Language Models - Chapter 10)

[7] "Pode n√£o generalizar bem para posi√ß√µes n√£o vistas" (Trecho de Transformers and Large Language Models - Chapter 10)

[8] "Pode ter dificuldades com depend√™ncias de muito longo alcance" (Trecho de Transformers and Large Language Models - Chapter 10)

[9] "Os embeddings posicionais relativos s√£o uma evolu√ß√£o dos absolutos, visando melhorar a capacidade do modelo de generalizar para sequ√™ncias de diferentes comprimentos e capturar depend√™ncias de longo alcance mais eficientemente" (Trecho de Transformers and Large Language Models - Chapter 10)

[10] "Uma implementa√ß√£o comum de embeddings relativos √© atrav√©s da modifica√ß√£o do c√°lculo de aten√ß√£o" (Trecho de Transformers and Large Language Models - Chapter 10)

[11] "Melhor generaliza√ß√£o para sequ√™ncias de diferentes tamanhos" (Trecho de Transformers and Large Language Models - Chapter 10)

[12] "Implementa√ß√£o mais complexa que embeddings absolutos" (Trecho de Transformers and Large Language Models - Chapter 10)

[13] "Captura eficiente de depend√™ncias de longo alcance" (Trecho de Transformers and Large Language Models - Chapter 10)

[14] "Pode aumentar o custo computacional" (Trecho de Transformers and Large Language Models - Chapter 10)

[15] "N√£o limitado a um comprimento m√°ximo de sequ√™ncia fixo" (Trecho de Transformers and Large Language Models - Chapter 10)

[16] "Requer ajustes espec√≠ficos para cada cabe√ßa de aten√ß√£o" (Trecho de Transformers and Large Language Models - Chapter 10)

[17] "O uso de diferentes t√©cnicas de embedding posicional pode ter um impacto significativo no desempenho do modelo, especialmente em tarefas que envolvem depend√™ncias de longo alcance ou sequ√™ncias de comprimento vari√°vel" (Trecho de Transformers and Large Language Models - Chapter 10)

[18] "Bom para contextos pr√≥ximos" (Trecho de Transformers and Large Language Models - Chapter 10)

[19] "Excelente para contextos locais e distantes" (Trecho de Transformers and Large Language Models - Chapter 10)

[20] "Limitada a comprimentos treinados" (Trecho de Transformers and Large Language Models - Chapter 10)

[21] "Melhor generaliza√ß√£o para diferentes comprimentos" (Trecho de Transformers and Large Language Models - Chapter 10)

[22] "Menor" (Trecho de Transformers and Large Language Models - Chapter 10)

[23] "Maior, mas potencialmente mais eficaz" (Trecho de Transformers and Large Language Models - Chapter 10)

[24] "Pode degradar com a dist√¢ncia" (Trecho de Transformers and Large Language Models - Chapter 10)

[25] "Mant√©m efic√°cia em longas dist√¢ncias" (Trecho de Transformers and Large Language Models - Chapter 10)

[26] "Embeddings relativos t√™m mostrado desempenho superior em tarefas que requerem compreens√£o de contexto amplo e generaliza√ß√£o para sequ√™ncias de comprimento vari√°vel" (Trecho de Transformers and Large Language Models - Chapter 10)

[27] "Em vez de usar fun√ß√µes predefinidas, alguns modelos aprendem os embeddings posicionais durante o treinamento" (Trecho de Transformers and Large Language Models - Chapter 10)

[28] "Combinam embeddings de diferentes escalas para capturar tanto informa√ß√µes locais quanto globais" (Trecho de Transformers and Large Language Models - Chapter 10)

[29] "Uma t√©cnica que aplica uma rota√ß√£o aos vetores de query e key baseada na posi√ß√£o relativa, permitindo uma incorpora√ß√£o eficiente de informa√ß√µes posicionais" (Trecho de Transformers and Large Language Models - Chapter 10)

[30] "Adiciona um vi√©s linear √† matriz de aten√ß√£o baseado na dist√¢ncia relativa entre tokens, eliminando a necessidade de embeddings posicionais expl√≠citos" (Trecho de Transformers and Large Language Models - Chapter 10)

[31] "Observamos que, embora os embeddings absolutos ofere√ßam uma solu√ß√£o simples e eficaz para sequ√™ncias de comprimento moderado, os embeddings relativos e outras t√©cnicas avan√ßadas demonstram vantagens significativas em termos de generaliza√ß√£o e capacidade de capturar depend√™ncias de longo alcance" (Trecho de Transformers and Large Language Models - Chapter 10)

[32] "Essas abordagens mais sofisticadas permitem que os modelos lidem melhor com sequ√™ncias de comprimento vari√°vel e mantenham a efic√°cia em contextos mais amplos" (Trecho de Transformers and Large Language Models - Chapter 10)

[33] "A escolha do m√©todo de embedding posicional deve ser considerada cuidadosamente com base nas caracter√≠sticas espec√≠ficas da tarefa e nos requisitos do modelo. Fatores como o comprimento t√≠pico das sequ√™ncias, a import√¢ncia das depend√™ncias de longo alcance e os recursos computacionais dispon√≠veis devem influenciar essa decis√£o" (Trecho de Transformers and Large Language Models - Chapter 10)

[34] "√Ä medida que a pesquisa em arquiteturas de transformers continua a evoluir, √© prov√°vel que vejamos o desenvolvimento de t√©cnicas ainda mais avan√ßadas para codifica√ß√£o posicional. Essas inova√ß√µes poder√£o potencialmente melhorar ainda mais o desempenho dos modelos em uma variedade de tarefas de processamento de linguagem natural e al√©m" (Trecho de Transformers and Large Language Models - Chapter 10)

[35] "A evolu√ß√£o dos embeddings posicionais de absolutos para relativos e t√©cnicas mais avan√ßadas reflete uma tend√™ncia geral em dire√ß√£o a modelos mais flex√≠veis e capazes de generalizar melhor para uma variedade de tarefas e comprimentos de sequ√™ncia" (Trecho de Transformers and Large Language Models - Chapter 10)