## Transformers: Investigando os Pap√©is de Query, Key e Value no Processo de Aten√ß√£o

<image: Um diagrama mostrando tr√™s vetores coloridos (query, key, value) interagindo em um espa√ßo vetorial multidimensional, com linhas pontilhadas representando a aten√ß√£o entre eles>

### Introdu√ß√£o

Os transformers revolucionaram o processamento de linguagem natural (NLP) ao introduzir um mecanismo de aten√ß√£o baseado em Query, Key e Value (QKV). Este resumo aprofunda-se na interpreta√ß√£o geom√©trica e funcional desses componentes, explorando como eles capturam diferentes aspectos das rela√ß√µes entre palavras e investigando mecanismos alternativos de aten√ß√£o al√©m da aten√ß√£o de produto escalar escalado.

### Conceitos Fundamentais

| Conceito    | Explica√ß√£o                                                   |
| ----------- | ------------------------------------------------------------ |
| **Query**   | Vetor que representa a palavra atual sendo processada, usado para consultar informa√ß√µes relevantes no contexto. [1] |
| **Key**     | Vetor que codifica informa√ß√µes sobre palavras no contexto, usado para compara√ß√£o com a query. [1] |
| **Value**   | Vetor que cont√©m o conte√∫do sem√¢ntico real da palavra, usado para computar a sa√≠da da camada de aten√ß√£o. [1] |
| **Aten√ß√£o** | Mecanismo que permite ao modelo focar em partes relevantes do input ao processar uma sequ√™ncia. [2] |

> ‚úîÔ∏è **Ponto de Destaque**: A decomposi√ß√£o em Query, Key e Value permite que o modelo aprenda diferentes aspectos das rela√ß√µes entre palavras de forma paralela e eficiente.

### Interpreta√ß√£o Geom√©trica dos Vetores QKV

<image: Um espa√ßo vetorial 3D com vetores query, key e value representados como setas coloridas, mostrando √¢ngulos e proje√ß√µes entre eles>

A interpreta√ß√£o geom√©trica dos vetores QKV oferece insights valiosos sobre o funcionamento do mecanismo de aten√ß√£o:

1. **Query como Dire√ß√£o de Busca**: O vetor query pode ser visto como uma dire√ß√£o no espa√ßo vetorial que representa a informa√ß√£o que estamos buscando. [3]

2. **Key como Descritor de Conte√∫do**: Os vetores key funcionam como descritores do conte√∫do de cada palavra no contexto. [3]

3. **Value como Conte√∫do Sem√¢ntico**: Os vetores value carregam o conte√∫do sem√¢ntico real que ser√° usado para computar a sa√≠da. [3]

4. **Produto Escalar como Similaridade**: A opera√ß√£o de produto escalar entre query e key mede a similaridade entre a informa√ß√£o buscada e o conte√∫do dispon√≠vel. [4]

Matematicamente, podemos expressar a aten√ß√£o como:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Onde:
- $Q \in \mathbb{R}^{n \times d_k}$ √© a matriz de queries
- $K \in \mathbb{R}^{n \times d_k}$ √© a matriz de keys
- $V \in \mathbb{R}^{n \times d_v}$ √© a matriz de values
- $d_k$ √© a dimens√£o dos vetores query e key
- $n$ √© o n√∫mero de tokens na sequ√™ncia

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a dimensionalidade dos vetores query e key ($d_k$) afeta a estabilidade num√©rica do c√°lculo de aten√ß√£o, e por que a divis√£o por $\sqrt{d_k}$ √© necess√°ria?

2. Descreva geometricamente como o produto escalar entre um vetor query e um vetor key captura a relev√¢ncia entre duas palavras no contexto.

### Pap√©is Distintos no Processo de Aten√ß√£o

Os vetores QKV desempenham pap√©is complementares no processo de aten√ß√£o:

1. **Query: Foco da Aten√ß√£o**
   - Determina o que √© relevante para a palavra atual
   - Projeta a "pergunta" que o modelo est√° fazendo ao contexto [5]

2. **Key: √çndice de Conte√∫do**
   - Fornece um "√≠ndice" para o conte√∫do de cada palavra no contexto
   - Permite compara√ß√£o eficiente com a query [5]

3. **Value: Informa√ß√£o Contextual**
   - Cont√©m a informa√ß√£o real que ser√° agregada
   - Representa o "conte√∫do" que ser√° ponderado pela aten√ß√£o [5]

> ‚ùó **Ponto de Aten√ß√£o**: A separa√ß√£o em QKV permite que o modelo aprenda diferentes transforma√ß√µes para cada aspecto da aten√ß√£o, aumentando a flexibilidade e expressividade do mecanismo.

### Contribui√ß√£o para Captura de Rela√ß√µes entre Palavras

Os vetores QKV permitem capturar diferentes tipos de rela√ß√µes entre palavras:

1. **Rela√ß√µes Sint√°ticas**: 
   - Query e Key podem aprender a capturar padr√µes sint√°ticos, como concord√¢ncia sujeito-verbo. [6]
   - Exemplo: Em "The keys to the cabinet are on the table", a aten√ß√£o pode focar em "keys" ao processar "are".

2. **Rela√ß√µes Sem√¢nticas**:
   - Value vectors podem codificar informa√ß√µes sem√¢nticas profundas. [6]
   - Exemplo: Em "The chicken crossed the road because it wanted to get to the other side", o modelo pode relacionar "it" com "chicken" baseado no conte√∫do sem√¢ntico.

3. **Rela√ß√µes de Longa Dist√¢ncia**:
   - A aten√ß√£o permite capturar depend√™ncias de longa dist√¢ncia eficientemente. [7]
   - Exemplo: Em textos longos, informa√ß√µes relevantes de par√°grafos anteriores podem ser acessadas diretamente.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a multi-head attention contribui para a captura de diferentes tipos de rela√ß√µes entre palavras? Explique em termos dos pap√©is de QKV.

2. Proponha uma modifica√ß√£o no mecanismo de aten√ß√£o que poderia melhorar a captura de rela√ß√µes hier√°rquicas em textos. Como isso afetaria os vetores QKV?

### Mecanismos Alternativos de Aten√ß√£o

Al√©m da aten√ß√£o de produto escalar escalado, existem alternativas que exploram diferentes aspectos das rela√ß√µes entre palavras:

1. **Aten√ß√£o Aditiva**:
   - Usa uma rede feed-forward para computar scores de aten√ß√£o
   - F√≥rmula: $\text{score}(q, k) = v^T \tanh(W_q q + W_k k)$ [8]
   - Vantagem: Pode capturar rela√ß√µes n√£o-lineares entre query e key

2. **Aten√ß√£o Baseada em Dist√¢ncia**:
   - Incorpora informa√ß√£o de posi√ß√£o relativa no c√°lculo da aten√ß√£o
   - F√≥rmula: $\text{score}(q, k, d) = q^T k + w^T f(d)$, onde $d$ √© a dist√¢ncia e $f$ √© uma fun√ß√£o de codifica√ß√£o de dist√¢ncia [9]
   - Vantagem: Melhora a modelagem de depend√™ncias locais

3. **Aten√ß√£o Esparsa**:
   - Limita a aten√ß√£o a um subconjunto de posi√ß√µes baseado em algum crit√©rio (e.g., top-k)
   - Vantagem: Reduz complexidade computacional e pode focar em rela√ß√µes mais importantes [10]

> üí° **Inova√ß√£o**: Experimentos recentes mostram que combinar diferentes tipos de aten√ß√£o em um √∫nico modelo pode levar a melhorias significativas na performance em v√°rias tarefas de NLP.

### Implementa√ß√£o Avan√ßada: Multi-Head Attention

A multi-head attention √© uma extens√£o crucial do mecanismo b√°sico de aten√ß√£o, permitindo que o modelo aprenda m√∫ltiplas representa√ß√µes de aten√ß√£o em paralelo. Vamos implementar uma vers√£o simplificada usando PyTorch:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def split_heads(self, x):
        batch_size, seq_len, _ = x.size()
        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
    
    def forward(self, query, key, value, mask=None):
        q = self.split_heads(self.W_q(query))
        k = self.split_heads(self.W_k(key))
        v = self.split_heads(self.W_v(value))
        
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))
        
        attn_probs = F.softmax(attn_scores, dim=-1)
        context = torch.matmul(attn_probs, v)
        
        context = context.transpose(1, 2).contiguous().view(query.size(0), -1, self.d_model)
        output = self.W_o(context)
        
        return output, attn_probs
```

Esta implementa√ß√£o demonstra como os pap√©is de Query, Key e Value s√£o manipulados em um contexto de multi-head attention, permitindo que o modelo aprenda m√∫ltiplas representa√ß√µes de aten√ß√£o simultaneamente.

### Conclus√£o

A decomposi√ß√£o do mecanismo de aten√ß√£o em Query, Key e Value representa uma inova√ß√£o fundamental nos transformers, permitindo uma modelagem rica e flex√≠vel das rela√ß√µes entre palavras. A interpreta√ß√£o geom√©trica desses vetores oferece insights valiosos sobre como o modelo captura diferentes aspectos do contexto lingu√≠stico. Mecanismos alternativos de aten√ß√£o expandem ainda mais as possibilidades, abrindo caminhos para futuras inova√ß√µes em arquiteturas de transformers e processamento de linguagem natural.

### Quest√µes Avan√ßadas

1. Como voc√™ projetaria um mecanismo de aten√ß√£o que pudesse capturar explicitamente rela√ß√µes hier√°rquicas em textos (por exemplo, estrutura de √°rvore sint√°tica)? Considere modifica√ß√µes nos pap√©is de QKV e na fun√ß√£o de scoring.

2. Discuta as implica√ß√µes computacionais e de modelagem de usar diferentes dimensionalidades para Query, Key e Value. Como isso poderia afetar a capacidade do modelo de capturar diferentes tipos de rela√ß√µes lingu√≠sticas?

3. Proponha e descreva matematicamente um novo mecanismo de aten√ß√£o que combine aspectos da aten√ß√£o baseada em produto escalar e da aten√ß√£o aditiva. Quais seriam as potenciais vantagens deste h√≠brido?

### Refer√™ncias

[1] "To capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value." (Trecho de Transformers and Large Language Models - Chapter 10)

[2] "The core intuition of attention is the idea of comparing an item of interest to a collection of other items in a way that reveals their relevance in the current context." (Trecho de Transformers and Large Language Models - Chapter 10)

[3] "Given these projections, the score between a current focus of attention, x, and an element in the preceding context, x, consists of a dot product between its query vector qi and the preceding element's key vectors k." (Trecho de Transformers and Large Language Models - Chapter 10)

[4] "score(x, x ) = qi ¬∑ k ji j ‚àödk" (Trecho de Transformers and Large Language Models - Chapter 10)

[5] "As the current focus of attention when being compared to all of the other preceding inputs. We'll refer to this role as a query. In its role as a preceding input being compared to the current focus of attention. We'll refer to this role as a key. And finally, as a value used to compute the output for the current focus of attention." (Trecho de Transformers and Large Language Models - Chapter 10)

[6] "In (10.1), the phrase The keys is the subject of the sentence, and in English and many languages, must agree in grammatical number with the verb are; in this case both are plural. In English we can't use a singular verb like is with a plural subject like keys; we'll discuss agreement more in Chapter 17. In (10.2), the pronoun it corefers to the chicken; it's the chicken that wants to get to the other side." (Trecho de Transformers and Large Language Models - Chapter 10)

[7] "These helpful contextual words can be quite far way in the sentence or paragraph." (Trecho de Transformers and Large Language Models - Chapter 10)

[8] "To capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value." (Trecho de Transformers and Large Language Models - Chapter 10)

[9] "Even more complex positional embedding methods exist, such as ones that represent relative position instead of absolute position, often implemented in the attention mechanism at each layer rather than being added once at the initial input." (Trecho de Transformers and Large Language Models - Chapter 10)

[10] "Transformers address this issue with multihead self-attention layers. These are sets of self-attention layers, called heads, that reside in parallel layers at the same depth in a model, each with its own set of parameters. By using these distinct sets of parameters, each head can learn different aspects of the relationships among inputs at the same level of abstraction." (Trecho de Transformers and Large Language Models - Chapter 10)