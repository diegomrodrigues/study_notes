## Sele√ß√£o de Demonstra√ß√µes: Heur√≠sticas para Escolha de Demonstra√ß√µes Eficazes

<image: Um diagrama mostrando diferentes m√©todos de sele√ß√£o de demonstra√ß√µes, incluindo similaridade com a entrada e sele√ß√£o automatizada baseada em melhoria de desempenho. O diagrama deve incluir setas conectando os m√©todos a um conjunto de demonstra√ß√µes, representando o processo de sele√ß√£o.>

### Introdu√ß√£o

A sele√ß√£o de demonstra√ß√µes √© um aspecto crucial no campo do prompt engineering e few-shot learning para Large Language Models (LLMs). Este t√≥pico explora as heur√≠sticas e m√©todos utilizados para escolher demonstra√ß√µes eficazes que melhoram o desempenho dos modelos em tarefas espec√≠ficas. A escolha adequada de demonstra√ß√µes pode significativamente impactar a capacidade do modelo de generalizar e realizar tarefas com maior precis√£o [1].

### Conceitos Fundamentais

| Conceito                   | Explica√ß√£o                                                   |
| -------------------------- | ------------------------------------------------------------ |
| **Few-shot Learning**      | T√©cnica onde o modelo aprende a realizar uma tarefa com apenas alguns exemplos de demonstra√ß√£o [2]. |
| **Demonstra√ß√µes**          | Exemplos rotulados fornecidos no prompt para guiar o modelo na realiza√ß√£o de uma tarefa espec√≠fica [3]. |
| **Heur√≠sticas de Sele√ß√£o** | M√©todos e crit√©rios utilizados para escolher as demonstra√ß√µes mais eficazes para uma determinada tarefa [4]. |

> ‚ö†Ô∏è **Important Note**: A efic√°cia das demonstra√ß√µes n√£o depende apenas da quantidade, mas principalmente da qualidade e relev√¢ncia para a tarefa em quest√£o.

### Heur√≠sticas para Sele√ß√£o de Demonstra√ß√µes

#### Similaridade com a Entrada

Uma das heur√≠sticas mais comuns para selecionar demonstra√ß√µes eficazes √© baseada na similaridade com a entrada atual [5]. Esta abordagem parte do princ√≠pio de que exemplos semelhantes √† tarefa em quest√£o podem fornecer um contexto mais relevante para o modelo.

<image: Um gr√°fico de dispers√£o mostrando a distribui√ß√£o de exemplos em um espa√ßo bidimensional, com clusters representando diferentes tipos de tarefas. Destaque alguns pontos como "entrada atual" e "demonstra√ß√µes selecionadas" baseadas em proximidade.>

Para implementar esta heur√≠stica, pode-se utilizar t√©cnicas de embedding e c√°lculo de similaridade:

```python
import torch
from transformers import AutoTokenizer, AutoModel

def get_embeddings(texts, model, tokenizer):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)

def select_similar_demonstrations(input_text, demonstrations, model, tokenizer, top_k=3):
    input_embedding = get_embeddings([input_text], model, tokenizer)
    demo_embeddings = get_embeddings(demonstrations, model, tokenizer)
    
    similarities = torch.cosine_similarity(input_embedding, demo_embeddings)
    top_indices = similarities.argsort(descending=True)[:top_k]
    
    return [demonstrations[i] for i in top_indices]
```

Esta fun√ß√£o seleciona as `top_k` demonstra√ß√µes mais similares √† entrada com base na similaridade de cosseno entre seus embeddings [6].

> ‚úîÔ∏è **Highlight**: A sele√ß√£o baseada em similaridade pode ser particularmente eficaz para tarefas onde o contexto espec√≠fico √© crucial, como em tradu√ß√£o ou resposta a perguntas espec√≠ficas de dom√≠nio.

#### Sele√ß√£o Automatizada Baseada em Desempenho

Outra abordagem avan√ßada √© a sele√ß√£o automatizada de demonstra√ß√µes com base na melhoria de desempenho que elas proporcionam [7]. Este m√©todo envolve a avalia√ß√£o iterativa de diferentes conjuntos de demonstra√ß√µes em um conjunto de valida√ß√£o.

<image: Um fluxograma mostrando o processo de sele√ß√£o automatizada: conjunto inicial de demonstra√ß√µes -> avalia√ß√£o em conjunto de valida√ß√£o -> sele√ß√£o das melhores -> itera√ß√£o.>

A implementa√ß√£o desta abordagem pode ser feita utilizando um algoritmo de busca, como a busca em feixe (beam search):

```python
def evaluate_demonstrations(model, task, demonstrations, validation_set):
    # Implementa√ß√£o da avalia√ß√£o do modelo com as demonstra√ß√µes no conjunto de valida√ß√£o
    pass

def beam_search_demonstrations(model, task, candidate_demos, validation_set, beam_width=3, iterations=5):
    current_beam = [set()]
    for _ in range(iterations):
        candidates = []
        for demo_set in current_beam:
            for demo in candidate_demos:
                if demo not in demo_set:
                    new_set = demo_set.union({demo})
                    score = evaluate_demonstrations(model, task, new_set, validation_set)
                    candidates.append((new_set, score))
        
        current_beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]
    
    return current_beam[0][0]  # Retorna o melhor conjunto de demonstra√ß√µes
```

Este algoritmo realiza uma busca em feixe para encontrar o conjunto de demonstra√ß√µes que maximiza o desempenho no conjunto de valida√ß√£o [8].

> ‚ùó **Attention Point**: A sele√ß√£o automatizada pode ser computacionalmente intensiva, especialmente para grandes conjuntos de candidatos a demonstra√ß√µes. √â importante balancear a qualidade da sele√ß√£o com a efici√™ncia computacional.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a dimensionalidade dos embeddings pode afetar a efic√°cia da sele√ß√£o baseada em similaridade? Discuta poss√≠veis trade-offs.
2. Em um cen√°rio de few-shot learning para classifica√ß√£o de texto, como voc√™ abordaria a sele√ß√£o de demonstra√ß√µes para garantir uma representa√ß√£o equilibrada das classes?

### Compara√ß√£o de M√©todos de Sele√ß√£o

| üëç Vantagens                                                  | üëé Desvantagens                                           |
| ------------------------------------------------------------ | -------------------------------------------------------- |
| Similaridade: R√°pido e intuitivo [9]                         | Similaridade: Pode n√£o capturar nuances da tarefa [10]   |
| Baseado em Desempenho: Otimiza diretamente para a tarefa [11] | Baseado em Desempenho: Computacionalmente intensivo [12] |

### Considera√ß√µes Te√≥ricas

A sele√ß√£o de demonstra√ß√µes pode ser formalizada como um problema de otimiza√ß√£o. Dado um conjunto de candidatos a demonstra√ß√µes $D = \{d_1, ..., d_n\}$, queremos encontrar um subconjunto $S \subset D$ que maximize o desempenho do modelo $M$ na tarefa $T$:

$$
S^* = \argmax_{S \subset D, |S| \leq k} \mathbb{E}_{x \sim X}[P(M(x|S) = y)]
$$

Onde $X$ √© o espa√ßo de entradas, $y$ √© a sa√≠da correta, e $k$ √© o n√∫mero m√°ximo de demonstra√ß√µes permitidas [13].

Esta formula√ß√£o captura a ess√™ncia do problema, mas na pr√°tica, a otimiza√ß√£o exata √© intrat√°vel para grandes conjuntos de demonstra√ß√µes. Por isso, utilizamos heur√≠sticas e m√©todos aproximados.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como voc√™ adaptaria a formula√ß√£o matem√°tica acima para lidar com m√∫ltiplas tarefas simultaneamente em um cen√°rio de aprendizado multi-tarefa?
2. Discuta as implica√ß√µes te√≥ricas de usar embeddings pr√©-treinados versus embeddings espec√≠ficos da tarefa para a sele√ß√£o baseada em similaridade.

### Conclus√£o

A sele√ß√£o eficaz de demonstra√ß√µes √© um aspecto cr√≠tico no desenvolvimento de sistemas de prompt engineering e few-shot learning. As heur√≠sticas baseadas em similaridade oferecem uma abordagem r√°pida e intuitiva, enquanto m√©todos automatizados baseados em desempenho podem proporcionar resultados mais otimizados, embora com maior custo computacional [14]. A escolha do m√©todo depende do equil√≠brio entre efici√™ncia computacional, desempenho desejado e caracter√≠sticas espec√≠ficas da tarefa em quest√£o.

### Quest√µes Avan√ßadas

1. Como voc√™ abordaria o problema de sele√ß√£o de demonstra√ß√µes em um cen√°rio de aprendizado cont√≠nuo, onde novas tarefas e dados est√£o constantemente sendo introduzidos?
2. Proponha e discuta um m√©todo h√≠brido que combine sele√ß√£o baseada em similaridade e otimiza√ß√£o de desempenho, visando maximizar a efic√°cia das demonstra√ß√µes enquanto minimiza o custo computacional.
3. Considerando as limita√ß√µes de contexto em LLMs, como voc√™ equilibraria a quantidade e qualidade das demonstra√ß√µes com a necessidade de espa√ßo para a tarefa real e a resposta do modelo?

### Refer√™ncias

[1] "Demonstrations are generally created by formatting examples drawn from a labeled training set" (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[2] "Few-shot prompting, as contrasted with zero-shot prompting which means instructions that don't include labeled examples." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[3] "We call such examples demonstrations." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[4] "There are some heuristics about what makes a good demonstration." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[5] "For example, using demonstrations that are similar to the current input seems to improve performance." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[6] "It can thus be useful to dynamically retrieve demonstrations for each input, based on their similarity to the current example (for example, comparing the embedding of the current example with embeddings of each of the training set example to find the best top-T)." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[7] "But more generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[8] "Systems like DSPy (Khattab et al., 2024), a framework for algorithmically optimizing LM prompts, can automatically find the optimum set of demonstrations to include by searching through the space of possible demonstrations to include." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[9] "For example, using demonstrations that are similar to the current input seems to improve performance." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[10] "The reason is that the primary benefit in examples is to demonstrate the task to be performed to the LLM and the format of the sequence, not to provide relevant information as to the right answer for any particular question." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[11] "But more generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[12] "Adding too many examples seems to cause the model to overfit to details of the exact examples chosen and generalize poorly." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[13] "Task performance for sentiment analysis or multiple-choice question answering can be measured in accuracy; for machine translation with chrF; and for summarization via Rouge." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)

[14] "The goal is to continue to seek improved prompts given the computational resources available." (Excerpt from CHAPTER 12: Model Alignment, Prompting, and In-Context Learning)