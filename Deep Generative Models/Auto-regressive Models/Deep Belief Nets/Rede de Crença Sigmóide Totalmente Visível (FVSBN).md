## Rede de Cren√ßa Sigm√≥ide Totalmente Vis√≠vel (FVSBN)

<img src="C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240819090552882.png" alt="image-20240819090552882" style="zoom: 80%;" />

### Introdu√ß√£o

A Rede de Cren√ßa Sigm√≥ide Totalmente Vis√≠vel (FVSBN - Fully Visible Sigmoid Belief Network) √© um modelo probabil√≠stico poderoso para representar distribui√ß√µes sobre vari√°veis bin√°rias. Este modelo pertence √† classe de modelos autoregressivos, que decomp√µem a distribui√ß√£o conjunta em um produto de distribui√ß√µes condicionais [1]. A FVSBN √© particularmente interessante devido √† sua capacidade de modelar depend√™ncias complexas entre vari√°veis, mantendo uma estrutura computacionalmente trat√°vel.

### Conceitos Fundamentais

| Conceito                  | Explica√ß√£o                                                   |
| ------------------------- | ------------------------------------------------------------ |
| **Modelo Autoregressivo** | Um modelo que expressa a probabilidade conjunta como um produto de probabilidades condicionais, seguindo uma ordem espec√≠fica das vari√°veis. [1] |
| **Regress√£o Log√≠stica**   | M√©todo usado para modelar cada probabilidade condicional na FVSBN, permitindo capturar rela√ß√µes n√£o-lineares entre as vari√°veis. [6] |
| **Amostragem Ancestral**  | T√©cnica para gerar amostras da distribui√ß√£o modelada, seguindo a ordem definida pelo modelo autoregressivo. [7] |

> ‚ö†Ô∏è **Nota Importante**: A FVSBN √© um modelo totalmente observ√°vel, diferindo de muitos outros modelos probabil√≠sticos que incluem vari√°veis latentes.

### Estrutura do Modelo FVSBN

A FVSBN modela uma distribui√ß√£o conjunta sobre vari√°veis bin√°rias $X = (X_1, ..., X_n)$ usando a regra da cadeia de probabilidade:

$$
p(x_1, ..., x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)...p(x_n|x_1,...,x_{n-1})
$$

Cada probabilidade condicional √© modelada usando regress√£o log√≠stica [6]:

$$
p(X_i = 1|x_1, ..., x_{i-1}) = \sigma(\alpha_i^0 + \sum_{j=1}^{i-1} \alpha_i^j x_j)
$$

Onde $\sigma(z) = \frac{1}{1 + e^{-z}}$ √© a fun√ß√£o sigm√≥ide e $\alpha_i^j$ s√£o os par√¢metros do modelo.

### Avalia√ß√£o da Probabilidade Conjunta

Para avaliar $p(x_1, ..., x_n)$ para um dado vetor $x$, seguimos estes passos [7]:

1. Calcular $p(x_1)$ usando os par√¢metros apropriados.
2. Para $i = 2$ at√© $n$:
   - Calcular $p(x_i|x_1, ..., x_{i-1})$ usando a equa√ß√£o da regress√£o log√≠stica.
3. Multiplicar todas as probabilidades calculadas.

> ‚úîÔ∏è **Ponto de Destaque**: A avalia√ß√£o da probabilidade conjunta pode ser realizada em tempo $O(n^2)$, onde $n$ √© o n√∫mero de vari√°veis.

### Amostragem da Distribui√ß√£o

![image-20240819233713726](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240819233713726.png)

Para gerar uma amostra da distribui√ß√£o modelada pela FVSBN [7]:

1. Amostrar $x_1 \sim p(x_1)$.
2. Para $i = 2$ at√© $n$:
   - Calcular $p(x_i = 1|x_1, ..., x_{i-1})$.
   - Amostrar $x_i \sim \text{Bernoulli}(p(x_i = 1|x_1, ..., x_{i-1}))$.

Este processo √© conhecido como amostragem ancestral e garante que a amostra gerada segue a distribui√ß√£o modelada pela FVSBN.

#### Implementa√ß√£o em PyTorch

Aqui est√° uma implementa√ß√£o simplificada da FVSBN em PyTorch:

```python
import torch
import torch.nn as nn

class FVSBN(nn.Module):
    def __init__(self, n_variables):
        super(FVSBN, self).__init__()
        self.n_variables = n_variables
        self.alphas = nn.ParameterList([
            nn.Parameter(torch.randn(i)) for i in range(1, n_variables + 1)
        ])
    
    def forward(self, x):
        log_probs = []
        for i in range(self.n_variables):
            if i == 0:
                prob = torch.sigmoid(self.alphas[i])
            else:
                prob = torch.sigmoid(torch.sum(self.alphas[i][:i] * x[:i]))
            log_prob = torch.where(x[i] == 1, torch.log(prob), torch.log(1 - prob))
            log_probs.append(log_prob)
        return torch.sum(torch.stack(log_probs))

    def sample(self):
        x = torch.zeros(self.n_variables)
        for i in range(self.n_variables):
            if i == 0:
                prob = torch.sigmoid(self.alphas[i])
            else:
                prob = torch.sigmoid(torch.sum(self.alphas[i][:i] * x[:i]))
            x[i] = torch.bernoulli(prob)
        return x
```

Esta implementa√ß√£o permite tanto a avalia√ß√£o da log-probabilidade de uma amostra quanto a gera√ß√£o de novas amostras da distribui√ß√£o modelada.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a complexidade computacional da avalia√ß√£o da probabilidade conjunta na FVSBN se compara com a de um modelo de Boltzmann restrito (RBM)?

2. Quais s√£o as implica√ß√µes da ordem escolhida para as vari√°veis na FVSBN em termos de modelagem e desempenho?

### Vantagens e Desvantagens da FVSBN

| üëç Vantagens                                       | üëé Desvantagens                                               |
| ------------------------------------------------- | ------------------------------------------------------------ |
| Modelagem direta da distribui√ß√£o conjunta [1]     | Necessidade de especificar uma ordem para as vari√°veis [6]   |
| Amostragem eficiente via amostragem ancestral [7] | Potencial perda de informa√ß√£o devido √† ordem fixa das vari√°veis |
| Avalia√ß√£o exata da probabilidade conjunta [7]     | Escalabilidade limitada para um grande n√∫mero de vari√°veis   |

### Extens√µes e Variantes

1. **NADE (Neural Autoregressive Distribution Estimation)**: Uma extens√£o da FVSBN que utiliza redes neurais mais complexas para modelar as distribui√ß√µes condicionais, permitindo capturar depend√™ncias mais sofisticadas entre as vari√°veis [9].

2. **MADE (Masked Autoencoder for Distribution Estimation)**: Uma variante que utiliza mascaramento para criar uma rede neural que respeita a estrutura autoregressiva, permitindo computa√ß√£o paralela eficiente [10].

A formula√ß√£o matem√°tica do NADE √© similar √† FVSBN, mas com uma camada oculta adicional:

$$
p(x_i = 1|x_{<i}) = \sigma(V_i h_i + b_i)
$$

$$
h_i = \sigma(W_{<i} x_{<i} + c)
$$

Onde $W_{<i}$ representa as primeiras $i-1$ colunas de $W$, e $V_i$ √© a $i$-√©sima linha de $V$.

### Aplica√ß√µes Pr√°ticas

A FVSBN e suas variantes t√™m sido aplicadas com sucesso em diversas √°reas:

1. **Modelagem de Imagens**: Utilizada para modelar distribui√ß√µes de pixels em imagens, permitindo a gera√ß√£o de novas imagens realistas [11].

2. **Processamento de Linguagem Natural**: Aplicada na modelagem de sequ√™ncias de palavras ou caracteres, √∫til para tarefas como completamento de texto [12].

3. **Detec√ß√£o de Anomalias**: A capacidade de avaliar probabilidades exatas torna a FVSBN √∫til para identificar amostras at√≠picas em conjuntos de dados [13].

> üí° **Dica**: A FVSBN pode ser usada como um bloco de constru√ß√£o em modelos mais complexos, como redes profundas de cren√ßa (DBNs).

### Conclus√£o

A Rede de Cren√ßa Sigm√≥ide Totalmente Vis√≠vel (FVSBN) representa um marco importante no desenvolvimento de modelos probabil√≠sticos para dados bin√°rios. Sua estrutura autoregressiva permite uma modelagem eficiente e interpret√°vel de distribui√ß√µes complexas, tornando-a uma ferramenta valiosa em diversos dom√≠nios de aprendizado de m√°quina e intelig√™ncia artificial.

A FVSBN serve como base para modelos mais avan√ßados, como NADE e MADE, que estendem seus princ√≠pios para capturar depend√™ncias ainda mais complexas. Embora tenha limita√ß√µes, como a necessidade de especificar uma ordem fixa para as vari√°veis, a FVSBN continua sendo um modelo relevante, especialmente em cen√°rios onde a interpretabilidade e a avalia√ß√£o exata de probabilidades s√£o cruciais.

### Quest√µes Avan√ßadas

1. Como voc√™ poderia modificar a estrutura da FVSBN para permitir a modelagem de vari√°veis cont√≠nuas? Quais seriam os desafios e as poss√≠veis abordagens para superar essas limita√ß√µes?

2. Considere um cen√°rio onde voc√™ precisa modelar uma distribui√ß√£o sobre 1000 vari√°veis bin√°rias. Compare e contraste o uso de uma FVSBN com outras abordagens, como Redes Neurais Variacionais ou Modelos de Fluxo Normalizado, em termos de expressividade, efici√™ncia computacional e facilidade de treinamento.

3. Proponha uma arquitetura h√≠brida que combine os princ√≠pios da FVSBN com t√©cnicas de aten√ß√£o usadas em modelos de linguagem modernos. Como essa arquitetura poderia superar algumas das limita√ß√µes da FVSBN original, especialmente em rela√ß√£o √† ordem fixa das vari√°veis?

### Refer√™ncias

[1] "We can pick an ordering of all the random variables, i.e., raster scan ordering of pixels from top-left (X1) to bottom-right (Xn=784)" (Trecho de cs236_lecture3.pdf)

[6] "p(xi|x1, ¬∑ ¬∑ ¬∑ , xi‚àí1) = œÉ(Œ±i0 + Pi‚àí1 j=1 Œ±i j xj)" (Trecho de cs236_lecture3.pdf)

[7] "How to sample from p(x1, ¬∑ ¬∑ ¬∑ , x784)? 1 Sample x1 ‚àº p(x1) (np.random.choice([1,0],p=[ÀÜx1, 1 ‚àí ÀÜx1])) 2 Sample x2 ‚àº p(x2 | x1 = x1) 3 Sample x3 ‚àº p(x3 | x1 = x1, x2 = x2) ¬∑ ¬∑ ¬∑" (Trecho de cs236_lecture3.pdf)

[9] "NADE: Neural Autoregressive Density Estimation To improve model: use one layer neural network instead of logistic regression" (Trecho de cs236_lecture3.pdf)

[10] "MADE: Masked Autoencoder for Distribution Estimation 1 Challenge: An autoencoder that is autoregressive (DAG structure) 2 Solution: use masks to disallow certain paths (Germain et al., 2015)." (Trecho de cs236_lecture3.pdf)

[11] "Results on downsampled ImageNet. Very slow: sequential likelihood evaluation." (Trecho de cs236_lecture3.pdf)

[12] "Train on Wikipedia. Then sample from the model:" (Trecho de cs236_lecture3.pdf)

[13] "Application in Adversarial Attacks and Anomaly detection Machine learning methods are vulnerable to adversarial examples Can we detect them?" (Trecho de cs236_lecture3.pdf)