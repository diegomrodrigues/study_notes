## Autoencoder com M√°scara para Estima√ß√£o de Distribui√ß√£o (MADE)

<img src="C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240820105330086.png" alt="image-20240820105330086" style="zoom:67%;" />

### Introdu√ß√£o

O Autoencoder com M√°scara para Estima√ß√£o de Distribui√ß√£o (MADE - Masked Autoencoder for Distribution Estimation) √© uma t√©cnica avan√ßada que combina os princ√≠pios de autoencoders e modelos autorregressivos para criar um modelo generativo poderoso e computacionalmente eficiente [1]. Desenvolvido como uma extens√£o dos modelos autorregressivos neurais, o MADE aborda desafios espec√≠ficos relacionados √† modelagem de distribui√ß√µes de probabilidade multivariadas, mantendo a estrutura autorregressiva necess√°ria para uma fatoriza√ß√£o eficiente da distribui√ß√£o conjunta [2].

### Conceitos Fundamentais

| Conceito                   | Explica√ß√£o                                                   |
| -------------------------- | ------------------------------------------------------------ |
| **Autoencoder**            | Uma rede neural que aprende a codificar dados em uma representa√ß√£o de menor dimens√£o e depois decodific√°-los de volta √† forma original. No contexto do MADE, √© modificado para preservar a estrutura autorregressiva [1]. |
| **Modelo Autorregressivo** | Um modelo que prev√™ vari√°veis sequencialmente, onde cada vari√°vel depende apenas das anteriores na sequ√™ncia [2]. |
| **M√°scaras**               | Matrizes bin√°rias aplicadas √†s camadas da rede neural para controlar o fluxo de informa√ß√£o e garantir a estrutura autorregressiva [1]. |

> ‚ö†Ô∏è **Nota Importante**: A chave para o sucesso do MADE est√° na aplica√ß√£o estrat√©gica de m√°scaras que garantem que cada unidade de sa√≠da dependa apenas das entradas anteriores na ordena√ß√£o escolhida.

### Arquitetura do MADE

O MADE √© estruturado como um autoencoder modificado, onde:

1. A camada de entrada recebe os dados $x = (x_1, ..., x_d)$.
2. As camadas ocultas processam a informa√ß√£o com conex√µes mascaradas.
3. A camada de sa√≠da produz par√¢metros para as distribui√ß√µes condicionais $p(x_i|x_{1:i-1})$ [1].

A arquitetura do MADE pode ser descrita matematicamente como:

$$
h^{(1)} = f(W^{(1)} \odot M^{(1)}x + b^{(1)})
$$
$$
h^{(l)} = f(W^{(l)} \odot M^{(l)}h^{(l-1)} + b^{(l)}), \quad l = 2, ..., L-1
$$
$$
\hat{x} = g(W^{(L)} \odot M^{(L)}h^{(L-1)} + b^{(L)})
$$

Onde:
- $f$ e $g$ s√£o fun√ß√µes de ativa√ß√£o n√£o-lineares
- $W^{(l)}$ e $b^{(l)}$ s√£o os pesos e vieses da camada $l$
- $M^{(l)}$ s√£o as m√°scaras bin√°rias
- $\odot$ denota a multiplica√ß√£o elemento a elemento

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a estrutura do MADE difere de um autoencoder tradicional e por que essa diferen√ßa √© crucial para a modelagem de distribui√ß√µes?
2. Explique como as m√°scaras no MADE garantem a propriedade autorregressiva e por que isso √© importante para a efici√™ncia computacional.

### Gera√ß√£o de M√°scaras

O processo de gera√ß√£o de m√°scaras √© fundamental para o MADE e segue estas etapas [1]:

1. Atribuir a cada unidade oculta um n√∫mero inteiro $m(k)$ no intervalo $[1, d-1]$.
2. Para a camada de sa√≠da, atribuir $m(k) = d$ para unidades que parametrizam $p(x_d|x_{1:d-1})$, $m(k) = d-1$ para $p(x_{d-1}|x_{1:d-2})$, e assim por diante.
3. Criar m√°scaras $M^{(l)}$ onde $M^{(l)}_{ij} = 1$ se $m^{(l-1)}(i) \geq m^{(l)}(j)$, e 0 caso contr√°rio.

Este processo garante que cada sa√≠da $\hat{x}_i$ dependa apenas de $x_1, ..., x_{i-1}$.

> ‚úîÔ∏è **Ponto de Destaque**: A atribui√ß√£o aleat√≥ria de n√∫meros √†s unidades ocultas permite m√∫ltiplas ordena√ß√µes das vari√°veis, aumentando a flexibilidade do modelo.

### Treinamento e Otimiza√ß√£o

O MADE √© treinado para maximizar a log-verossimilhan√ßa dos dados:

$$
\mathcal{L} = \sum_{n=1}^N \sum_{i=1}^d \log p(x_{n,i}|x_{n,1:i-1}; \theta)
$$

Onde $\theta$ representa os par√¢metros do modelo.

A otimiza√ß√£o √© realizada usando descida de gradiente estoc√°stica (SGD) ou variantes, com retropropaga√ß√£o atrav√©s das camadas mascaradas [2].

```python
import torch
import torch.nn as nn

class MADE(nn.Module):
    def __init__(self, input_dim, hidden_dims, num_masks=1):
        super(MADE, self).__init__()
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.num_masks = num_masks
        
        self.masks = self.create_masks()
        self.layers = nn.ModuleList()
        
        dims = [input_dim] + hidden_dims + [input_dim]
        for i in range(len(dims) - 1):
            self.layers.append(nn.Linear(dims[i], dims[i+1]))
        
    def create_masks(self):
        masks = []
        for _ in range(self.num_masks):
            m = {}
            d = self.input_dim
            for l in range(len(self.hidden_dims)):
                m[f'W_{l+1}'] = torch.randint(1, d, size=(self.hidden_dims[l],))
            m[f'W_{len(self.hidden_dims)+1}'] = torch.arange(1, d+1)
            masks.append(m)
        return masks
    
    def forward(self, x):
        mask_idx = torch.randint(0, self.num_masks, (1,)).item()
        mask = self.masks[mask_idx]
        
        for i, layer in enumerate(self.layers[:-1]):
            m = mask[f'W_{i+1}'][:, None] >= mask[f'W_{i}'][None, :]
            x = torch.relu(layer(x * m))
        
        m = mask[f'W_{len(self.layers)}'][:, None] > mask[f'W_{len(self.layers)-1}'][None, :]
        x = self.layers[-1](x * m)
        
        return x
```

Este c√≥digo implementa a estrutura b√°sica do MADE em PyTorch, incluindo a gera√ß√£o de m√°scaras e a aplica√ß√£o delas durante o forward pass.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como o uso de m√∫ltiplas m√°scaras no MADE afeta a capacidade do modelo de aprender diferentes ordena√ß√µes das vari√°veis?
2. Discuta as implica√ß√µes computacionais de usar m√°scaras bin√°rias no MADE em compara√ß√£o com outros modelos autorregressivos.

### Vantagens e Desvantagens do MADE

| üëç Vantagens                                                  | üëé Desvantagens                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Efici√™ncia computacional: permite c√°lculo paralelo da probabilidade conjunta [1] | Limita√ß√£o na modelagem de depend√™ncias de longo alcance [2]  |
| Flexibilidade na ordena√ß√£o das vari√°veis [1]                 | Poss√≠vel subutiliza√ß√£o de unidades ocultas devido √†s m√°scaras [2] |
| Capacidade de modelar distribui√ß√µes complexas [2]            | Necessidade de cuidadosa inicializa√ß√£o e ajuste das m√°scaras [1] |

### Extens√µes e Variantes

1. **MADE Convolucional**: Incorpora camadas convolucionais mascaradas para lidar com dados estruturados como imagens [3].

2. **MADE Recorrente**: Combina a estrutura do MADE com unidades recorrentes para modelar sequ√™ncias temporais [3].

3. **MADE com Fluxos Normalizadores**: Integra o MADE em arquiteturas de fluxo para criar transforma√ß√µes invert√≠veis mais expressivas [3].

> üí° **Insight**: A combina√ß√£o do MADE com outras t√©cnicas de aprendizado profundo tem levado a modelos generativos ainda mais poderosos e vers√°teis.

### Aplica√ß√µes Pr√°ticas

O MADE tem encontrado aplica√ß√µes em diversos dom√≠nios:

1. **Gera√ß√£o de Imagens**: Como componente em modelos de fluxo para gera√ß√£o de imagens de alta qualidade [3].
2. **Modelagem de S√©ries Temporais**: Para previs√£o e an√°lise de dados sequenciais em finan√ßas e climatologia [2].
3. **Compress√£o de Dados**: Utilizando a modelagem de probabilidade para compress√£o sem perdas [1].

### Conclus√£o

O Autoencoder com M√°scara para Estima√ß√£o de Distribui√ß√£o (MADE) representa um avan√ßo significativo na modelagem de distribui√ß√µes multivariadas, combinando a flexibilidade dos autoencoders com a estrutura eficiente dos modelos autorregressivos [1][2]. Sua capacidade de manter a propriedade autorregressiva atrav√©s do uso inteligente de m√°scaras permite uma computa√ß√£o paralela eficiente, tornando-o uma ferramenta valiosa em diversos cen√°rios de aprendizado de m√°quina e modelagem estat√≠stica [3].

O MADE n√£o apenas oferece uma solu√ß√£o elegante para o desafio de modelar distribui√ß√µes complexas, mas tamb√©m serve como base para desenvolvimentos futuros em modelos generativos profundos. √Ä medida que o campo evolui, √© prov√°vel que vejamos mais inova√ß√µes baseadas nos princ√≠pios introduzidos pelo MADE, possivelmente combinando-o com outras t√©cnicas avan√ßadas de aprendizado profundo para criar modelos ainda mais poderosos e flex√≠veis.

### Quest√µes Avan√ßadas

1. Como voc√™ modificaria a arquitetura MADE para lidar eficientemente com dados de alta dimensionalidade, como imagens de alta resolu√ß√£o, mantendo a propriedade autorregressiva?

2. Discuta as implica√ß√µes te√≥ricas e pr√°ticas de combinar o MADE com t√©cnicas de fluxo normalizado. Como isso afeta a expressividade do modelo e a efici√™ncia computacional?

3. Proponha uma abordagem para incorporar aten√ß√£o no MADE, visando capturar depend√™ncias de longo alcance sem sacrificar a efici√™ncia computacional. Quais seriam os desafios e potenciais benef√≠cios?

### Refer√™ncias

[1] "Challenge: An autoencoder that is autoregressive (DAG structure) Solution: use masks to disallow certain paths (Germain et al., 2015)." (Trecho de cs236_lecture3.pdf)

[2] "Suppose ordering is x_2, x_3, x_1, so p(x_1, x_2, x_3) = p(x_2)p(x_3 | x_2)p(x_1 | x_2, x_3)." (Trecho de cs236_lecture3.pdf)

[3] "The unit producing the parameters for ÀÜx_2 = p(x_2) is not allowed to depend on any input. Unit for p(x_3|x_2) only on x_2. And so on..." (Trecho de cs236_lecture3.pdf)