## Estima√ß√£o de Densidade Autoregressiva Neural (NADE)

<img src="C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240820105240118.png" alt="image-20240820105240118" style="zoom:67%;" />

### Introdu√ß√£o

A Estima√ß√£o de Densidade Autoregressiva Neural (NADE, Neural Autoregressive Density Estimation) √© uma t√©cnica avan√ßada de modelagem probabil√≠stica que combina os princ√≠pios de modelos autoregressivos com a flexibilidade e poder das redes neurais [1]. Este m√©todo surgiu como uma evolu√ß√£o das abordagens tradicionais de modelagem de densidade, oferecendo uma maneira eficiente e poderosa de estimar distribui√ß√µes de probabilidade complexas em alta dimens√£o [2].

NADE se destaca por sua capacidade de modelar distribui√ß√µes complexas mantendo a tratabilidade computacional, um equil√≠brio dif√≠cil de alcan√ßar em muitos modelos generativos [3]. Ao utilizar uma arquitetura neural com compartilhamento de pesos, NADE consegue capturar depend√™ncias sofisticadas entre vari√°veis, superando muitas das limita√ß√µes dos modelos autoregressivos cl√°ssicos [4].

### Conceitos Fundamentais

| Conceito                       | Explica√ß√£o                                                   |
| ------------------------------ | ------------------------------------------------------------ |
| **Modelagem Autoregressiva**   | Abordagem que decomp√µe a distribui√ß√£o conjunta em um produto de condicionais, permitindo a modelagem de depend√™ncias complexas [5]. |
| **Redes Neurais Feed-forward** | Estruturas computacionais que transformam entradas em sa√≠das atrav√©s de camadas de neur√¥nios interconectados, capazes de aproximar fun√ß√µes complexas [6]. |
| **Compartilhamento de Pesos**  | T√©cnica que reduz o n√∫mero de par√¢metros livres, melhorando a generaliza√ß√£o e efici√™ncia computacional [7]. |

> ‚úîÔ∏è **Ponto de Destaque**: NADE combina a tratabilidade dos modelos autoregressivos com o poder de aproxima√ß√£o universal das redes neurais, resultando em um modelo generativo poderoso e eficiente [8].

### Arquitetura NADE

<img src="C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240820105139989.png" alt="image-20240820105139989" style="zoom: 67%;" />

A arquitetura NADE √© projetada para modelar a distribui√ß√£o conjunta $p(x)$ de um vetor de vari√°veis aleat√≥rias $x = (x_1, ..., x_D)$ utilizando a regra da cadeia de probabilidade [9]:

$$
p(x) = \prod_{i=1}^D p(x_i | x_{<i})
$$

Onde $x_{<i} = (x_1, ..., x_{i-1})$ representa todas as vari√°veis anteriores a $x_i$ na ordem escolhida.

A inova√ß√£o chave do NADE est√° na parametriza√ß√£o das distribui√ß√µes condicionais $p(x_i | x_{<i})$ usando uma rede neural feed-forward com pesos compartilhados [10]:

1. **Camada de Entrada**: Recebe $x_{<i}$ como entrada.
2. **Camada Oculta**: Calcula representa√ß√µes ocultas $h_i$ usando pesos compartilhados $W$ e vieses $c$:

   $$h_i = \sigma(W_{:,<i}x_{<i} + c)$$

   Onde $\sigma$ √© uma fun√ß√£o de ativa√ß√£o n√£o-linear (geralmente sigm√≥ide ou ReLU).

3. **Camada de Sa√≠da**: Estima os par√¢metros da distribui√ß√£o condicional de $x_i$:

   $$\hat{x}_i = p(x_i | x_{<i}) = f(V_ih_i + b_i)$$

   Onde $f$ √© uma fun√ß√£o adequada ao tipo de vari√°vel (e.g., sigm√≥ide para vari√°veis bin√°rias, softmax para categ√≥ricas).

> ‚ùó **Ponto de Aten√ß√£o**: O compartilhamento de pesos $W$ entre todas as estimativas condicionais √© crucial para a efici√™ncia do NADE, reduzindo drasticamente o n√∫mero de par√¢metros e melhorando a generaliza√ß√£o [11].

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como o compartilhamento de pesos no NADE afeta a complexidade computacional em compara√ß√£o com uma rede totalmente conectada para cada condicional?

2. Explique como a escolha da ordem das vari√°veis pode impactar o desempenho do modelo NADE.

### Treinamento do NADE

O treinamento do NADE √© realizado maximizando a log-verossimilhan√ßa dos dados de treinamento [12]:

$$
\mathcal{L} = \sum_{n=1}^N \log p(x^{(n)}) = \sum_{n=1}^N \sum_{i=1}^D \log p(x_i^{(n)} | x_{<i}^{(n)})
$$

Onde $x^{(n)}$ representa a n-√©sima amostra do conjunto de treinamento.

O gradiente da log-verossimilhan√ßa em rela√ß√£o aos par√¢metros do modelo pode ser calculado eficientemente usando backpropagation [13]. A otimiza√ß√£o √© geralmente realizada usando m√©todos de gradiente estoc√°stico, como Adam ou RMSprop.

> ‚ö†Ô∏è **Nota Importante**: O treinamento do NADE permite a otimiza√ß√£o paralela de todas as condicionais $p(x_i | x_{<i})$ para uma amostra, tornando-o mais eficiente que modelos autoregressivos tradicionais [14].

### Variantes e Extens√µes do NADE

1. **RNADE (Real-valued Neural Autoregressive Density-Estimator)**: Extens√£o do NADE para vari√°veis cont√≠nuas, usando misturas de Gaussianas para modelar as distribui√ß√µes condicionais [15].

2. **NADE-k**: Variante que utiliza k passos de infer√™ncia mean-field para aproximar a distribui√ß√£o posterior, melhorando a qualidade das estimativas [16].

3. **Deep NADE**: Incorpora m√∫ltiplas camadas ocultas para aumentar a capacidade de modelagem, mantendo a efici√™ncia computacional atrav√©s de um esquema de compartilhamento de pesos em profundidade [17].

### Implementa√ß√£o em PyTorch

Aqui est√° um exemplo simplificado de implementa√ß√£o da arquitetura b√°sica do NADE em PyTorch:

```python
import torch
import torch.nn as nn

class NADE(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(NADE, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # Par√¢metros compartilhados
        self.W = nn.Parameter(torch.randn(hidden_dim, input_dim))
        self.c = nn.Parameter(torch.zeros(hidden_dim))
        self.V = nn.Parameter(torch.randn(input_dim, hidden_dim))
        self.b = nn.Parameter(torch.zeros(input_dim))
        
    def forward(self, x):
        batch_size = x.shape[0]
        log_probs = torch.zeros(batch_size, self.input_dim)
        
        for i in range(self.input_dim):
            # Calcula ativa√ß√µes ocultas
            a = torch.mm(x[:, :i], self.W[:, :i].t()) + self.c
            h = torch.sigmoid(a)
            
            # Calcula probabilidade condicional
            o = torch.sigmoid(torch.mm(h, self.V[i, :].unsqueeze(1)) + self.b[i])
            log_probs[:, i] = torch.log(o * x[:, i] + (1 - o) * (1 - x[:, i]) + 1e-8)
        
        return log_probs.sum(dim=1)
    
    def sample(self, num_samples=1):
        samples = torch.zeros(num_samples, self.input_dim)
        
        for i in range(self.input_dim):
            a = torch.mm(samples[:, :i], self.W[:, :i].t()) + self.c
            h = torch.sigmoid(a)
            o = torch.sigmoid(torch.mm(h, self.V[i, :].unsqueeze(1)) + self.b[i])
            samples[:, i] = torch.bernoulli(o)
        
        return samples
```

Este c√≥digo implementa o NADE para vari√°veis bin√°rias. Para adaptar para outros tipos de vari√°veis, seria necess√°rio modificar as fun√ß√µes de ativa√ß√£o e as distribui√ß√µes de sa√≠da apropriadamente.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como voc√™ modificaria a implementa√ß√£o acima para trabalhar com vari√°veis cont√≠nuas usando uma mistura de Gaussianas?

2. Explique como o processo de amostragem no m√©todo `sample` reflete a natureza autoregressiva do modelo NADE.

### Vantagens e Desvantagens do NADE

| üëç Vantagens                               | üëé Desvantagens                                               |
| ----------------------------------------- | ------------------------------------------------------------ |
| C√°lculo exato da log-verossimilhan√ßa [18] | Depend√™ncia da ordem escolhida para as vari√°veis [20]        |
| Amostragem eficiente [18]                 | Dificuldade em capturar certas depend√™ncias globais [21]     |
| Escalabilidade para altas dimens√µes [19]  | Treinamento sequencial pode ser lento para muitas dimens√µes [22] |

### Aplica√ß√µes e Resultados

NADE e suas variantes t√™m sido aplicados com sucesso em v√°rias tarefas de modelagem de densidade, incluindo:

1. Modelagem de imagens naturais [23]
2. Gera√ß√£o de m√∫sica [24]
3. Compress√£o de dados [25]
4. Detec√ß√£o de anomalias [26]

Em muitos casos, NADE alcan√ßa desempenho competitivo ou superior a outros m√©todos de estima√ß√£o de densidade, especialmente em datasets de alta dimens√£o [27].

### Conclus√£o

A Estima√ß√£o de Densidade Autoregressiva Neural (NADE) representa um avan√ßo significativo na modelagem probabil√≠stica, combinando a tratabilidade dos modelos autoregressivos com o poder de aproxima√ß√£o das redes neurais [28]. Sua capacidade de modelar distribui√ß√µes complexas de forma eficiente, juntamente com a possibilidade de c√°lculo exato da verossimilhan√ßa e amostragem r√°pida, torna o NADE uma ferramenta valiosa no arsenal de t√©cnicas de aprendizado de m√°quina e estat√≠stica [29].

Apesar de suas limita√ß√µes, como a depend√™ncia da ordem das vari√°veis e potenciais dificuldades em capturar certas depend√™ncias globais, o NADE continua sendo um modelo influente, inspirando novas pesquisas e aplica√ß√µes em diversos campos [30]. √Ä medida que a demanda por modelos generativos poderosos e eficientes continua a crescer, √© prov√°vel que vejamos mais desenvolvimentos e extens√µes baseados nos princ√≠pios fundamentais do NADE no futuro pr√≥ximo.

### Quest√µes Avan√ßadas

1. Como voc√™ poderia estender o NADE para lidar com dados sequenciais de comprimento vari√°vel, como s√©ries temporais ou texto?

2. Discuta as implica√ß√µes te√≥ricas e pr√°ticas de usar uma ordem aleat√≥ria diferente para cada amostra durante o treinamento do NADE. Como isso afetaria o desempenho e a interpretabilidade do modelo?

3. Proponha uma arquitetura h√≠brida que combine NADE com modelos de aten√ß√£o para potencialmente superar algumas das limita√ß√µes do NADE em capturar depend√™ncias de longo alcance.

### Refer√™ncias

[1] "NADE is a directed probabilistic model with no latent random variables." (Trecho de DLB - Deep Generative Models.pdf)

[2] "To improve model: use one layer neural network instead of logistic regression" (Trecho de cs236_lecture3.pdf)

[3] "The NADE architecture seems to need to add information." (Trecho de DLB - Deep Generative Models.pdf)

[4] "Tie weights to reduce the number of parameters and speed up computation" (Trecho de cs236_lecture3.pdf)

[5] "We can pick an ordering of all the random variables, i.e., raster scan ordering of pixels from top-left (X1) to bottom-right (Xn=784)" (Trecho de cs236_lecture3.pdf)

[6] "h_i = œÉ(W_¬∑,<i x_<i + c)" (Trecho de cs236_lecture3.pdf)

[7] "Tie weights to reduce the number of parameters and speed up computation" (Trecho de cs236_lecture3.pdf)

[8] "NADE is a generative model of a sequence of frames x(t) consisting of an RNN that emits the RBM parameters for each time step." (Trecho de DLB - Deep Generative Models.pdf)

[9] "Without loss of generality, we can use chain rule for factorization" (Trecho de cs236_lecture3.pdf)

[10] "ÀÜx_i = p(x_i |x_1, ¬∑ ¬∑ ¬∑ , x_i‚àí1) = œÉ(Œ±_i h_i + b_i)" (Trecho de cs236_lecture3.pdf)

[11] "If h_i ‚àà R^d , how many total parameters? Linear in n: weights W ‚àà R^d√ón, biases c ‚àà R^d , and n logistic regression coefficient vectors Œ±_i , b_i ‚àà R^d+1." (Trecho de cs236_lecture3.pdf)

[12] "To train the model, we need to be able to back-propagate the gradient of the loss function through the RNN." (Trecho de DLB - Deep Generative Models.pdf)

[13] "This means that we must approximately differentiate the loss with respect to the RBM parameters using contrastive divergence or a related algorithm." (Trecho de DLB - Deep Generative Models.pdf)

[14] "Probability is evaluated in O(nd)." (Trecho de cs236_lecture3.pdf)

[15] "How to model continuous random variables X_i ‚àà R? E.g., speech signals" (Trecho de cs236_lecture3.pdf)

[16] "The NADE architecture can be extended to mimic not just one time step of the mean field recurrent inference but to mimic k steps. This approach is called NADE-k" (Trecho de DLB - Deep Generative Models.pdf)

[17] "Murray and Larochelle (2014) propose deep versions of the architecture" (Trecho de DLB - Deep Generative Models.pdf)

[18] "Easy to sample from" (Trecho de cs236_lecture3.pdf)

[19] "Easy to compute probability p(x = x)" (Trecho de cs236_lecture3.pdf)

[20] "Needs an ordering" (Trecho de cs236_lecture3.pdf)

[21] "Sequential generation (unavoidable in an autoregressive model)" (Trecho de cs236_lecture3.pdf)

[22] "Sequential likelihood evaluation (very slow for training)" (Trecho de cs236_lecture3.pdf)

[23] "Results on downsampled ImageNet. Very slow: sequential likelihood evaluation." (Trecho de cs236_lecture3.pdf)

[24] "Train on data set of baby names. Then sample from the model:" (Trecho de cs236_lecture3.pdf)

[25] "Easy to extend to continuous variables. For example, can choose Gaussian conditionals p(x_t | x_<t ) = N (Œº_Œ∏(x_<t ), Œ£_Œ∏(x_<t )) or mixture of logistics" (Trecho de cs236