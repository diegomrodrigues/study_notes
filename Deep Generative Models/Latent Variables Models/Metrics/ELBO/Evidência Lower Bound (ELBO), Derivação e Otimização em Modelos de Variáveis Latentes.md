## Evid√™ncia Lower Bound (ELBO), Deriva√ß√£o e Otimiza√ß√£o em Modelos de Vari√°veis Latentes

<img src="C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240826143714331.png" alt="image-20240826143714331" style="zoom: 60%;" />

<img src="C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240826143759086.png" alt="image-20240826143759086" style="zoom:80%;" />

### Introdu√ß√£o

A Evid√™ncia Lower Bound (ELBO) √© um conceito fundamental na infer√™ncia variacional e no treinamento de modelos de vari√°veis latentes, como os Variational Autoencoders (VAEs). ==O ELBO fornece um limite inferior trat√°vel para a log-verossimilhan√ßa dos dados, que √© muitas vezes intrat√°vel de se calcular diretamente [1].== Esta t√©cnica √© crucial para otimizar modelos probabil√≠sticos complexos, ==permitindo a aprendizagem eficiente de distribui√ß√µes posteriores aproximadas sobre vari√°veis latentes.==

Neste resumo, exploraremos a deriva√ß√£o detalhada do ELBO, sua interpreta√ß√£o matem√°tica e estat√≠stica, e como ele √© utilizado na pr√°tica para treinar modelos de vari√°veis latentes. Abordaremos tamb√©m as nuances da otimiza√ß√£o do ELBO e suas implica√ß√µes para a qualidade dos modelos resultantes.

### Conceitos Fundamentais

| Conceito                     | Explica√ß√£o                                                   |
| ---------------------------- | ------------------------------------------------------------ |
| **Log-verossimilhan√ßa**      | A log-verossimilhan√ßa $\log p(x)$ √© uma medida da qualidade do modelo, representando a probabilidade logar√≠tmica dos dados observados sob o modelo [2]. |
| **Vari√°veis Latentes**       | Vari√°veis n√£o observadas $z$ que o modelo usa para capturar a estrutura subjacente dos dados [3]. |
| **Infer√™ncia Variacional**   | T√©cnica para aproximar distribui√ß√µes posteriores intrat√°veis usando otimiza√ß√£o [4]. |
| **Distribui√ß√£o Variacional** | Uma distribui√ß√£o $q_\phi(z|x)$ que aproxima a verdadeira posterior $p(z|x)$ [5]. |

> ‚úîÔ∏è **Ponto de Destaque**: ==O ELBO √© fundamentalmente uma reformula√ß√£o da log-verossimilhan√ßa que introduz uma distribui√ß√£o variacional==, tornando o problema de infer√™ncia trat√°vel computacionalmente.

### Deriva√ß√£o do ELBO

A deriva√ß√£o do ELBO come√ßa com a log-verossimilhan√ßa dos dados observados $x$:

$$
\log p(x) = \log \int p(x,z) dz
$$

Onde $p(x,z)$ √© a distribui√ß√£o conjunta dos dados observados $x$ e das vari√°veis latentes $z$ [6].

Introduzimos a distribui√ß√£o variacional $q_\phi(z|x)$:

$$
\log p(x) = \log \int p(x,z) \frac{q_\phi(z|x)}{q_\phi(z|x)} dz
$$

Aplicando a desigualdade de Jensen [7]:

$$
\log p(x) \geq \int q_\phi(z|x) \log \frac{p(x,z)}{q_\phi(z|x)} dz
$$

Esta desigualdade define o ELBO:

$$
\text{ELBO}(x; \phi) = \mathbb{E}_{q_\phi(z|x)} \left[\log \frac{p(x,z)}{q_\phi(z|x)}\right]
$$

Expandindo a fra√ß√£o dentro do logaritmo:

$$
\text{ELBO}(x; \phi) = \mathbb{E}_{q_\phi(z|x)} [\log p(x|z)] - \text{KL}(q_\phi(z|x) || p(z))
$$

Onde $\text{KL}(q_\phi(z|x) || p(z))$ √© a diverg√™ncia de Kullback-Leibler entre a distribui√ß√£o variacional e a prior sobre $z$ [8].

> ‚ùó **Ponto de Aten√ß√£o**: A decomposi√ß√£o do ELBO em um termo de reconstru√ß√£o e um termo de regulariza√ß√£o KL √© crucial para entender seu comportamento durante a otimiza√ß√£o.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a desigualdade de Jensen √© aplicada na deriva√ß√£o do ELBO e qual √© sua interpreta√ß√£o geom√©trica neste contexto?
2. Explique como o termo de diverg√™ncia KL no ELBO atua como um regularizador durante o treinamento de um VAE.

### Interpreta√ß√£o e Significado do ELBO

O ELBO pode ser interpretado de v√°rias maneiras:

1. **Limite Inferior**: ==O ELBO fornece um limite inferior para a log-verossimilhan√ßa $\log p(x)$ [9].==

2. **Balan√ßo Reconstru√ß√£o-Regulariza√ß√£o**: O primeiro termo $\mathbb{E}_{q_\phi(z|x)} [\log p(x|z)]$ incentiva uma boa reconstru√ß√£o dos dados, enquanto o termo KL age como regularizador [10].

3. **Minimiza√ß√£o da Diverg√™ncia**: Maximizar o ELBO √© equivalente a minimizar a diverg√™ncia KL entre a distribui√ß√£o variacional e a verdadeira posterior $p(z|x)$ [11].

<image: Um diagrama de Venn mostrando a rela√ß√£o entre a verdadeira posterior p(z|x), a distribui√ß√£o variacional q_œÜ(z|x), e como a maximiza√ß√£o do ELBO minimiza a diverg√™ncia entre elas.>

> ‚ö†Ô∏è **Nota Importante**: A maximiza√ß√£o do ELBO n√£o garante que a distribui√ß√£o variacional convergir√° exatamente para a verdadeira posterior, mas fornece a melhor aproxima√ß√£o poss√≠vel dentro da fam√≠lia de distribui√ß√µes escolhida.

### Otimiza√ß√£o do ELBO

==A otimiza√ß√£o do ELBO √© geralmente realizada usando t√©cnicas de gradiente estoc√°stico.== O desafio principal est√° em ==estimar o gradiente do ELBO com respeito aos par√¢metros do modelo $\theta$ e os par√¢metros variacionais $\phi$ [12].==

Um m√©todo popular √© o estimador reparametrizado, introduzido no contexto dos VAEs [13]:

$$
\nabla_{\phi,\theta} \text{ELBO} \approx \frac{1}{L} \sum_{l=1}^L \nabla_{\phi,\theta} [\log p(x|z^{(l)}) + \log p(z^{(l)}) - \log q_\phi(z^{(l)}|x)]
$$

Onde $z^{(l)} = g_\phi(\epsilon^{(l)}, x)$ e $\epsilon^{(l)} \sim p(\epsilon)$ [14].

```python
import torch
import torch.nn as nn

class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 2 * latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim)
        )
        
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def forward(self, x):
        h = self.encoder(x)
        mu, logvar = torch.chunk(h, 2, dim=-1)
        z = self.reparameterize(mu, logvar)
        x_recon = self.decoder(z)
        return x_recon, mu, logvar
    
    def elbo(self, x, x_recon, mu, logvar):
        recon_loss = nn.functional.mse_loss(x_recon, x, reduction='sum')
        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return recon_loss + kl_div
```

Este c√≥digo implementa um VAE b√°sico e calcula o ELBO, demonstrando como a reparametriza√ß√£o √© usada na pr√°tica [15].

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como o truque de reparametriza√ß√£o ajuda na estimativa do gradiente do ELBO, e por que isso √© importante para o treinamento de VAEs?
2. Discuta as vantagens e desvantagens de usar uma distribui√ß√£o variacional com diagonal de covari√¢ncia versus uma covari√¢ncia completa no contexto da otimiza√ß√£o do ELBO.

### Desafios e Considera√ß√µes Pr√°ticas

1. **Gap de Amortiza√ß√£o**: Em VAEs, o uso de um codificador amortizado pode levar a um gap entre o ELBO e a verdadeira log-verossimilhan√ßa [16].

2. **Colapso Posterior**: Em alguns casos, o termo KL pode dominar, levando a um fen√¥meno conhecido como colapso posterior [17].

3. **Escolha da Fam√≠lia Variacional**: A escolha da fam√≠lia de distribui√ß√µes para $q_\phi(z|x)$ afeta significativamente a qualidade da aproxima√ß√£o [18].

| üëç Vantagens                                | üëé Desvantagens                            |
| ------------------------------------------ | ----------------------------------------- |
| Permite infer√™ncia em modelos complexos    | Pode levar a aproxima√ß√µes sub√≥timas       |
| Fornece um objetivo de otimiza√ß√£o trat√°vel | Sens√≠vel √† escolha da fam√≠lia variacional |
| Balanceia reconstru√ß√£o e regulariza√ß√£o     | Pode sofrer de colapso posterior          |

### Extens√µes e Variantes do ELBO

1. **Œ≤-VAE**: Introduz um hiperpar√¢metro $\beta$ para controlar o termo KL [19]:

   $$
   \text{ELBO}_\beta = \mathbb{E}_{q_\phi(z|x)} [\log p(x|z)] - \beta \cdot \text{KL}(q_\phi(z|x) || p(z))
   $$

2. **Importance Weighted Autoencoder (IWAE)**: Usa amostragem por import√¢ncia para obter um limite inferior mais apertado [20]:

   $$
   \log p(x) \geq \mathbb{E}_{z_1,...,z_K \sim q_\phi(z|x)} \left[\log \frac{1}{K} \sum_{k=1}^K \frac{p(x,z_k)}{q_\phi(z_k|x)}\right]
   $$

3. **Variational Inference with Monte Carlo Objectives (VIMCO)**: Estende o IWAE para modelos com vari√°veis latentes discretas [21].

> ‚úîÔ∏è **Ponto de Destaque**: Estas extens√µes visam abordar limita√ß√µes espec√≠ficas do ELBO padr√£o, como o trade-off entre reconstru√ß√£o e regulariza√ß√£o ou a qualidade da estimativa da log-verossimilhan√ßa.

### Conclus√£o

O ELBO √© uma ferramenta poderosa e vers√°til na infer√™ncia variacional e no treinamento de modelos de vari√°veis latentes. Sua deriva√ß√£o e otimiza√ß√£o fornecem insights profundos sobre o comportamento desses modelos e as compensa√ß√µes envolvidas na aproxima√ß√£o de distribui√ß√µes posteriores complexas.

Compreender o ELBO √© fundamental para avan√ßar no campo de modelos generativos profundos e infer√™ncia probabil√≠stica. As extens√µes e variantes do ELBO continuam a ser uma √°rea ativa de pesquisa, prometendo melhorias na qualidade dos modelos e na efici√™ncia computacional.

### Quest√µes Avan√ßadas

1. Como voc√™ modificaria o ELBO para incorporar conhecimento pr√©vio sobre a estrutura das vari√°veis latentes em um dom√≠nio espec√≠fico?

2. Discuta as implica√ß√µes te√≥ricas e pr√°ticas de usar o ELBO versus outros crit√©rios de otimiza√ß√£o (como a log-verossimilhan√ßa exata) em modelos hier√°rquicos profundos.

3. Proponha e justifique uma nova variante do ELBO que poderia potencialmente superar as limita√ß√µes das abordagens existentes em cen√°rios de dados esparsos ou de alta dimensionalidade.

### Refer√™ncias

[1] "A natural question is whether it would be better to use a distribution defined on the hypersphere." (Trecho de Latent Variable Models.pdf)

[2] "The logarithm of the marginal distribution could be approximated as follows:" (Trecho de Latent Variable Models.pdf)

[3] "Let us consider a family of variational distributions parameterized by œÜ, {q_œÜ(z)}œÜ." (Trecho de Latent Variable Models.pdf)

[4] "We know the form of these distributions, and we assume that they assign non-zero probability mass to all z ‚àà Z^M." (Trecho de Latent Variable Models.pdf)

[5] "Then, the logarithm of the marginal distribution could be approximated as follows:" (Trecho de Latent Variable Models.pdf)

[6] "ln p(x) = ln ‚à´ p(x|z)p(z) dz" (Trecho de Latent Variable Models.pdf)

[7] "In the fourth line we used Jensen's inequality." (Trecho de Latent Variable Models.pdf)

[8] "The second part of the ELBO, E_z~q_œÜ(z|x)[ln q_œÜ(z|x) ‚àí ln p(z)], could be seen as a regularizer and it coincides with the Kullback‚ÄìLeibler (KL) divergence." (Trecho de Latent Variable Models.pdf)

[9] "As a result, we obtain an auto-encoder-like model, with a stochastic encoder, q_œÜ(z|x), and a stochastic decoder, p(x|z)." (Trecho de Latent Variable Models.pdf)

[10] "The first part of the ELBO, E_z~q_œÜ(z|x) [ln p(x|z)], is referred to as the (negative) reconstruction error, because x is encoded to z and then decoded back." (Trecho de Latent Variable Models.pdf)

[11] "The second part of the ELBO, E_z~q_œÜ(z|x)[ln q_œÜ(z|x) ‚àí ln p(z)], could be seen as a regularizer and it coincides with the Kullback‚ÄìLeibler (KL) divergence." (Trecho de Latent Variable Models.pdf)

[12] "There are two questions left to get the full picture of the VAEs:" (Trecho de Latent Variable Models.pdf)

[13] "As observed by Kingma and Welling [6] and Rezende et al. [7], we can drastically reduce the variance of the gradient by using this reparameterization of the Gaussian distribution." (Trecho de Latent Variable Models.pdf)

[14] "Even better, since we learn the VAE using stochastic gradient descent, it is enough to sample z only once during training!" (Trecho de Latent Variable Models.pdf)

[15] "We went through a lot of theory and discussions, and you might think it is impossible to implement a VAE. However, it is actually simpler than it might look." (Trecho de Latent Variable Models.pdf)

[16] "As pointed out by Husz√°r [62], the reason for that is the inductive bias of the chosen class of models." (Trecho de Latent Variable Models.pdf)

[17] "It turns out that learning the two-level VAE is even more problematic than a VAE with a single latent because even for a relatively simple decoder the second latent variable z_2 is mostly unused [15, 70]. This effect is called the posterior collapse." (Trecho de Latent Variable Models.pdf)
