* TÃ³pico: Real NVP (real-valued non-volume-preserving)
  * Contexto: Um tipo de fluxo de normalizaÃ§Ã£o de acoplamento que particiona as variÃ¡veis latentes e usa transformaÃ§Ãµes invertÃ­veis para modelar distribuiÃ§Ãµes flexÃ­veis.
    * Lista de trechos relevantes:
      - "One solution to this problem is given by a form of normalizing flow model called real NVP... which is short for 'real-valued non-volume-preserving'."
      - "The idea is to partition the latent-variable vector ğ‘§ into two parts ğ‘§=(ğ‘§ğ´,ğ‘§ğµ)... "
      - "For the first part of the output vector, we simply copy the input: ğ‘¥ğ´=ğ‘§ğ´."
      - "The second part of the vector undergoes a linear transformation, but now the coefficients in the linear transformation are given by nonlinear functions of ğ‘§ğ´: ğ‘¥ğµ=exp(ğ‘ (ğ‘§ğ´,ğ‘¤))âŠ™ğ‘§ğµ+ğ‘(ğ‘§ğ´,ğ‘¤)"
      - "The overall transformation is easily invertible..."
      - "The real NVP model belongs to a broad class of normalizing flows called coupling flows..."

* TÃ³pico: Invertible mapping
  * Contexto: A necessidade de mapeamentos bijetivos (invertÃ­veis) entre o espaÃ§o latente e o espaÃ§o de dados para calcular a funÃ§Ã£o de verossimilhanÃ§a em fluxos de normalizaÃ§Ã£o.
    * Lista de trechos relevantes:
      -  "To calculate the likelihood function for this model, we need the data-space distribution, which depends on the inverse of the neural network function."
      - "This requires that, for every value of ğ‘¤, the functions ğ‘“(ğ‘§,ğ‘¤) and ğ‘”(ğ‘¥,ğ‘¤) are invertible, also called bijective, so that each value of ğ‘¥ corresponds to a unique value of ğ‘§ and vice versa."
      - "One consequence of requiring an invertible mapping is that the dimensionality of the latent space must be the same as that of the data space..."

* TÃ³pico: Jacobian determinant calculation
  * Contexto: A importÃ¢ncia e os mÃ©todos para calcular o determinante Jacobiano de forma eficiente, especialmente para matrizes triangulares inferiores, em fluxos de normalizaÃ§Ã£o.
    * Lista de trechos relevantes:
      - "We can then use the change of variables formula to calculate the data density: ğ‘ğ‘¥(ğ‘¥|ğ‘¤)=ğ‘ğ‘§(ğ‘”(ğ‘¥,ğ‘¤))|detğ½(ğ‘¥)|"
      - "Also, in general, the cost of evaluating the determinant of a ğ·Ã—ğ· matrix is ğ‘‚(ğ·3), so we will seek to impose some further restrictions on the model in order that evaluation of the Jacobian matrix determinant is more efficient."
      - "We therefore see that the Jacobian matrix (18.14) is a lower triangular matrix... Consequently, the determinant of the Jacobian is simply given by the product of the elements of exp(âˆ’ğ‘ (ğ‘§ğ´,ğ‘¤))."

* TÃ³pico: Coupling function (h(zB, g))
  * Contexto: Uma funÃ§Ã£o em fluxos de acoplamento que opera em zB e Ã© eficientemente invertÃ­vel dado o condicionador g(zA, w).
    * Lista de trechos relevantes:
      - "The real NVP model belongs to a broad class of normalizing flows called coupling flows, in which the linear transformation (18.11) is replaced by a more general form: xB = h(zB, g(zA, w))"
      - "where â„(ğ‘§ğµ,ğ‘”) is a function of ğ‘§ğµ that is efficiently invertible for any given value of ğ‘” and is called the coupling function."


* TÃ³pico: Conditioner (g(zA, w))
  * Contexto: Uma funÃ§Ã£o, tipicamente uma rede neural, que fornece flexibilidade para a transformaÃ§Ã£o em fluxos de acoplamento.
    * Lista de trechos relevantes:
      - "The function ğ‘”(ğ‘§ğ´,ğ‘¤) is called a conditioner and is typically represented by a neural network."

* TÃ³pico: Layer composition
  * Contexto: Combinar mÃºltiplas camadas de transformaÃ§Ãµes invertÃ­veis para criar fluxos de normalizaÃ§Ã£o mais flexÃ­veis.
    * Lista de trechos relevantes:
      - "A clear limitation of this approach is that the value of  ğ‘§ğ´  is unchanged by the transformation. This is easily resolved by adding another layer in which the roles of  ğ‘§ğ´  and  ğ‘§ğµ  are reversed, as illustrated in Figure 18.2. This double-layer structure can then be repeated multiple times to facilitate a very flexible class of generative models."
      - "By composing two layers of the form shown in Figure 18.1, we obtain a more flexible, but still invertible, nonlinear layer."


* TÃ³pico: Masked Autoregressive Flow (MAF)
  * Contexto: Um tipo de fluxo de normalizaÃ§Ã£o autoregressivo que usa distribuiÃ§Ãµes condicionais e redes neurais mascaradas para modelar distribuiÃ§Ãµes complexas.
    * Lista de trechos relevantes:
      - "This factorization can be used to construct a class of normalizing flow called a masked autoregressive flow, or MAF... given by ğ‘¥ğ‘–=â„(ğ‘§ğ‘–,ğ‘”ğ‘–(ğ‘¥1:ğ‘–âˆ’1,ğ‘¤ğ‘–))"
      - "Here â„(ğ‘§ğ‘–,â‹…) is the coupling function, which is chosen to be easily invertible with respect to ğ‘§ğ‘–, and ğ‘”ğ‘– is the conditioner, which is typically represented by a deep neural network."
      - "The term masked refers to the use of a single neural network to implement a set of equations of the form (18.17) along with a binary mask... that force a subset of the network weights to be zero to implement the autoregressive constraint (18.16)."


* TÃ³pico: Inverse Autoregressive Flow (IAF)
  * Contexto: Um tipo de fluxo de normalizaÃ§Ã£o autoregressivo que permite amostragem eficiente, mas requer cÃ¡lculos sequenciais para a funÃ§Ã£o de verossimilhanÃ§a.
    * Lista de trechos relevantes:
      - "To avoid this inefficient sampling, we can instead define an inverse autoregressive flow, or IAF... given by ğ‘¥ğ‘–=â„(ğ‘§ğ‘–,ğ‘”Ìƒğ‘–(ğ‘§1:ğ‘–âˆ’1,ğ‘¤ğ‘–))"
      - "Sampling is now efficient since, for a given choice of z, the evaluation of the elements ğ‘¥1,â€¦,ğ‘¥ğ· using (18.19) can be performed in parallel. However, the inverse function, which is needed to evaluate the likelihood, requires a series of calculations... which are intrinsically sequential and therefore slow."

* TÃ³pico: Autoregressive constraint
  * Contexto: A restriÃ§Ã£o de que cada variÃ¡vel em um fluxo autoregressivo depende apenas das variÃ¡veis anteriores na sequÃªncia.
    * Lista de trechos relevantes:
      - "We first choose an ordering of the variables... from which we can write, without loss of generality, ğ‘(ğ‘¥1,â€¦,ğ‘¥ğ·)=âˆğ‘–=1ğ·ğ‘(ğ‘¥ğ‘–|ğ‘¥1:ğ‘–âˆ’1)"
      - "...that force a subset of the network weights to be zero to implement the autoregressive constraint (18.16)."


* TÃ³pico: Neural ODEs
  * Contexto: Redes neurais definidas por equaÃ§Ãµes diferenciais, permitindo uma quantidade infinita de camadas.
    * Lista de trechos relevantes:
      - "This can be thought of as a deep network with an infinite number of layers."
      - "The formulation in (18.22) is known as a neural ordinary differential equation or neural ODE..."


* TÃ³pico: Adjoint sensitivity method
  * Contexto: Um mÃ©todo para calcular gradientes em ODEs neurais, anÃ¡logo Ã  retropropagaÃ§Ã£o em redes neurais padrÃ£o.
    * Lista de trechos relevantes:
      - "Instead, Chen et al. (2018) treat the ODE solver as a black box and use a technique called the adjoint sensitivity method, which can be viewed as the continuous analogue of explicit backpropagation."
      - "To apply backpropagation to neural ODEs, we define a quantity called the adjoint given by ğ‘(ğ‘¡)=ğ‘‘ğ¿ğ‘‘ğ‘§(ğ‘¡)."


* TÃ³pico: Adaptive function evaluation
  * Contexto: MÃ©todos de integraÃ§Ã£o numÃ©rica que adaptam os pontos de avaliaÃ§Ã£o da funÃ§Ã£o para eficiÃªncia e precisÃ£o.
    * Lista de trechos relevantes:
      - "This integral can be evaluated using standard numerical integration packages."
      - "In practice, more powerful numerical integration algorithms can adapt their function evaluation to achieve. In particular, they can adaptively choose values of  ğ‘¡  that typically are not uniformly spaced."


* TÃ³pico: Adjoint method
  * Contexto: O anÃ¡logo contÃ­nuo da retropropagaÃ§Ã£o para treinar ODEs neurais.
    * Lista de trechos relevantes:
      -  "Chen et al. (2018) treat the ODE solver as a black box and use a technique called the adjoint sensitivity method, which can be viewed as the continuous analogue of explicit backpropagation."


* TÃ³pico: Adjoint differential equation
  * Contexto: A equaÃ§Ã£o diferencial que descreve a evoluÃ§Ã£o do adjunto durante a retropropagaÃ§Ã£o em ODEs neurais.
    * Lista de trechos relevantes:
      - "The adjoint satisfies its own differential equation given by ğ‘‘ğ‘(ğ‘¡)ğ‘‘ğ‘¡=âˆ’ğ‘(ğ‘¡)ğ‘‡âˆ‡ğ‘§ğ‘“(ğ‘§(ğ‘¡),ğ‘¤), which is a continuous version of the chain rule of calculus."


* TÃ³pico: Gradient evaluation (for Neural ODEs)
  * Contexto: Calculando gradientes em relaÃ§Ã£o aos parÃ¢metros da rede em ODEs neurais por meio de integraÃ§Ã£o.
    * Lista de trechos relevantes:
      - "The third step in the backpropagation method is to evaluate derivatives of the loss with respect to network parameters by forming appropriate products of activations and gradients... this summation becomes an integration over  ğ‘¡ , which takes the form âˆ‡ğ‘¤ğ¿=âˆ’âˆ«0ğ‘‡ğ‘(ğ‘¡)ğ‘‡âˆ‡ğ‘¤ğ‘“(ğ‘§(ğ‘¡),ğ‘¤)ğ‘‘ğ‘¡."


* TÃ³pico: Continuous normalizing flow
  * Contexto: Usando uma ODE neural para definir um fluxo de normalizaÃ§Ã£o contÃ­nuo, propagando a distribuiÃ§Ã£o base atravÃ©s do tempo.
    * Lista de trechos relevantes:
      - "We can make use of a neural ordinary differential equation to define an alternative approach to the construction of tractable normalizing flow models... The resulting framework is known as a continuous normalizing flow..."


* TÃ³pico: Density transformation (for Continuous Flows)
  * Contexto: Calculando a mudanÃ§a na densidade de probabilidade Ã  medida que a distribuiÃ§Ã£o base Ã© propagada atravÃ©s da ODE neural.
    * Lista de trechos relevantes:
      - "Chen et al. (2018) showed that for neural ODEs, the transformation of the density can be evaluated by integrating a differential equation given by ğ‘‘lnğ‘(ğ‘§(ğ‘¡))ğ‘‘ğ‘¡=âˆ’Tr(âˆ‚ğ‘“âˆ‚ğ‘§(ğ‘¡))"


* TÃ³pico: Hutchinson's trace estimator
  * Contexto: Um mÃ©todo eficiente para aproximar o traÃ§o de uma matriz, usado em fluxos de normalizaÃ§Ã£o contÃ­nuos.
    * Lista de trechos relevantes:
      - "However, the cost of evaluating the trace can be reduced to ğ‘‚(ğ·) by using Hutchinsonâ€™s trace estimator..."
      - "Tr(ğ´)=ğ¸ğœ–[ğœ–ğ‘‡ğ´ğœ–]"


* TÃ³pico: Flow matching
  * Contexto: Uma tÃ©cnica para melhorar a eficiÃªncia do treinamento de fluxos de normalizaÃ§Ã£o contÃ­nuos.
    * Lista de trechos relevantes:
      - "Significant improvements in training efficiency for continuous normalizing flows can be achieved using a technique called flow matching (Lipman et al., 2022)."

* TÃ³pico: Jacobian and determinant relationships
  * Contexto: RelaÃ§Ãµes matemÃ¡ticas entre Jacobianos e determinantes em transformaÃ§Ãµes de variÃ¡veis.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.1

* TÃ³pico: Invertible transformations
  * Contexto: Compor e inverter mÃºltiplas transformaÃ§Ãµes invertÃ­veis.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.2

* TÃ³pico: Linear transformations
  * Contexto: TransformaÃ§Ãµes lineares e seus Jacobianos.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.3

* TÃ³pico: Autoregressive flow Jacobians
  * Contexto: A estrutura triangular inferior dos Jacobianos em fluxos autoregressivos.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.4


* TÃ³pico: Residual network to ODE
  * Contexto: Derivando a equaÃ§Ã£o de propagaÃ§Ã£o direta para uma ODE neural a partir de uma rede residual.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.5

* TÃ³pico: Backpropagation for ODEs
  * Contexto: Derivando a equaÃ§Ã£o de retropropagaÃ§Ã£o para ODEs neurais.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.6

* TÃ³pico: Gradient evaluation (for ODEs - Exercises)
  * Contexto: Derivando a equaÃ§Ã£o de avaliaÃ§Ã£o de gradiente para ODEs neurais.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.7


* TÃ³pico: One-dimensional density transformation
  * Contexto: Derivando a equaÃ§Ã£o para fluxos de normalizaÃ§Ã£o contÃ­nuos em uma dimensÃ£o.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.8

* TÃ³pico: Flow line plotting
  * Contexto: Plotando linhas de fluxo usando a inversa da CDF e a ODE neural.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.9

* TÃ³pico: Base density and flow inversion
  * Contexto: A relaÃ§Ã£o entre a densidade base e a densidade de saÃ­da em fluxos de normalizaÃ§Ã£o contÃ­nuos.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.10

* TÃ³pico: Hutchinson trace estimator - Proving unbiasedness
  * Contexto: Provando que o estimador de traÃ§o de Hutchinson Ã© nÃ£o viesado.
    * Lista de trechos relevantes:
      - ExercÃ­cio 18.11