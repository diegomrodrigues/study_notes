## M√©todos de Otimiza√ß√£o para Maximum Likelihood Estimation (MLE)

<img src="C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240821151639133.png" alt="image-20240821151639133" style="zoom: 50%;" />

### Introdu√ß√£o

A estima√ß√£o por m√°xima verossimilhan√ßa (Maximum Likelihood Estimation - MLE) √© um m√©todo fundamental na estat√≠stica e aprendizado de m√°quina para estimar os par√¢metros de um modelo probabil√≠stico [1]. No contexto de modelos generativos profundos, ==o MLE √© frequentemente utilizado para treinar redes neurais que representam distribui√ß√µes de probabilidade complexas [2].== Este resumo abordar√° dois m√©todos de otimiza√ß√£o essenciais para resolver problemas de MLE: o gradiente descendente e o gradiente descendente estoc√°stico (SGD).

### Conceitos Fundamentais

| Conceito                                | Explica√ß√£o                                                   |
| --------------------------------------- | ------------------------------------------------------------ |
| **Maximum Likelihood Estimation (MLE)** | ==M√©todo para estimar os par√¢metros de um modelo estat√≠stico maximizando a fun√ß√£o de verossimilhan√ßa [1]== |
| **Fun√ß√£o de Verossimilhan√ßa**           | ==Medida da probabilidade dos dados observados dado um conjunto de par√¢metros do modelo [2]== |
| **Gradiente**                           | ==Vetor de derivadas parciais que indica a dire√ß√£o de maior crescimento de uma fun√ß√£o [3]== |
| **Taxa de Aprendizado**                 | Hiperpar√¢metro que controla o tamanho dos passos durante a otimiza√ß√£o [4] |

> ‚úîÔ∏è **Ponto de Destaque**: A otimiza√ß√£o por MLE em modelos generativos profundos visa encontrar os ==par√¢metros que maximizam a probabilidade dos dados de treinamento sob o modelo [2].==

### Gradiente Descendente

==O gradiente descendente √© um algoritmo de otimiza√ß√£o de primeira ordem que busca o m√≠nimo local de uma fun√ß√£o==, ==movendo-se iterativamente na dire√ß√£o oposta ao gradiente [3].==

==No contexto do MLE, o objetivo √© maximizar a fun√ß√£o de log-verossimilhan√ßa $\ell(\theta)$, que para um conjunto de dados $D = \{x^{(1)}, ..., x^{(m)}\}$ e um modelo com par√¢metros $\theta$, √© definida como [5]:==
$$
\ell(\theta) = \log L(\theta, D) = \sum_{j=1}^m \sum_{i=1}^n \log p_{neural}(x_i^{(j)}|x_{<i}^{(j)}; \theta_i)
$$

O algoritmo do gradiente descendente para MLE pode ser descrito da seguinte forma [6]:

1. Inicialize $\theta_0$ aleatoriamente
2. Para cada itera√ß√£o $t$:
   a. Calcule o gradiente $\nabla_\theta \ell(\theta)$
   b. Atualize os par√¢metros: $\theta_{t+1} = \theta_t + \alpha_t \nabla_\theta \ell(\theta)$

Onde $\alpha_t$ √© a taxa de aprendizado na itera√ß√£o $t$.

> ‚ö†Ô∏è **Nota Importante**: ==O sinal positivo na atualiza√ß√£o dos par√¢metros se deve ao fato de estarmos maximizando a fun√ß√£o de log-verossimilhan√ßa, ao contr√°rio da minimiza√ß√£o t√≠pica em problemas de otimiza√ß√£o [6].==

#### Vantagens e Desvantagens do Gradiente Descendente

| üëç Vantagens                                          | üëé Desvantagens                                               |
| ---------------------------------------------------- | ------------------------------------------------------------ |
| ==Converg√™ncia garantida para fun√ß√µes convexas [7]== | Computacionalmente custoso para grandes conjuntos de dados [8] |
| Simplicidade de implementa√ß√£o [7]                    | ==Pode ficar preso em m√≠nimos locais em fun√ß√µes n√£o-convexas [8]== |
| Eficaz para problemas com poucos par√¢metros [7]      | ==Sens√≠vel √† escolha da taxa de aprendizado [8]==            |

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como o gradiente descendente lida com plat√¥s na superf√≠cie da fun√ß√£o objetivo? Explique as implica√ß√µes para o treinamento de modelos generativos profundos.

2. Descreva uma situa√ß√£o em que o gradiente descendente pode falhar na converg√™ncia para o √≥timo global em um problema de MLE. Como isso afeta a qualidade do modelo generativo resultante?

### Gradiente Descendente Estoc√°stico (SGD)

==O Gradiente Descendente Estoc√°stico (SGD) √© uma varia√ß√£o do gradiente descendente que estima o gradiente usando um subconjunto aleat√≥rio (mini-batch) dos dados em cada itera√ß√£o [9].== Esta abordagem √© particularmente √∫til para grandes conjuntos de dados e aprendizado online.

Para um conjunto de dados muito grande, o c√°lculo do gradiente completo pode ser aproximado por [10]:

$$
\nabla_\theta \ell(\theta) \approx m \mathbb{E}_{x^{(j)} \sim D}\left[\sum_{i=1}^n \nabla_\theta \log p_{neural}(x_i^{(j)}|x_{<i}^{(j)}; \theta_i)\right]
$$

O algoritmo SGD para MLE pode ser descrito como [11]:

1. Inicialize $\theta_0$ aleatoriamente
2. Para cada itera√ß√£o $t$:
   a. Amostre um mini-batch $B_t$ do conjunto de dados $D$
   b. Estime o gradiente: $\hat{g}_t = \frac{|D|}{|B_t|} \sum_{x^{(j)} \in B_t} \sum_{i=1}^n \nabla_\theta \log p_{neural}(x_i^{(j)}|x_{<i}^{(j)}; \theta_i)$
   c. Atualize os par√¢metros: $\theta_{t+1} = \theta_t + \alpha_t \hat{g}_t$

> ‚ùó **Ponto de Aten√ß√£o**: ==A estimativa do gradiente no SGD √© n√£o-enviesada, mas possui maior vari√¢ncia comparada ao gradiente completo [12].==

#### Vantagens e Desvantagens do SGD

| üëç Vantagens                                         | üëé Desvantagens                                           |
| --------------------------------------------------- | -------------------------------------------------------- |
| Eficiente para grandes conjuntos de dados [13]      | ==Maior vari√¢ncia nas atualiza√ß√µes dos par√¢metros [14]== |
| Permite aprendizado online [13]                     | ==Pode requerer mais itera√ß√µes para converg√™ncia [14]==  |
| Pode escapar de m√≠nimos locais devido ao ru√≠do [13] | ==Sens√≠vel √† escolha do tamanho do mini-batch [14]==     |

#### Implementa√ß√£o em PyTorch

Aqui est√° um exemplo simplificado de como implementar SGD para treinar um modelo generativo usando PyTorch:

```python
import torch
import torch.nn as nn
import torch.optim as optim

class GenerativeModel(nn.Module):
    def __init__(self):
        super().__init__()
        # Defina a arquitetura do modelo aqui

    def forward(self, x):
        # Implemente a passagem forward

    def log_prob(self, x):
        # Retorna o log da probabilidade de x

def train_step(model, optimizer, batch):
    optimizer.zero_grad()
    loss = -model.log_prob(batch).mean()  # Negative log-likelihood
    loss.backward()
    optimizer.step()
    return loss.item()

# Configura√ß√£o
model = GenerativeModel()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Loop de treinamento
for epoch in range(num_epochs):
    for batch in dataloader:
        loss = train_step(model, optimizer, batch)
    print(f"Epoch {epoch}, Loss: {loss}")
```

> üí° **Dica**: ==Para melhorar a converg√™ncia do SGD, considere usar t√©cnicas como momentum, AdaGrad, ou Adam, que adaptam a taxa de aprendizado durante o treinamento [15].==

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como o tamanho do mini-batch afeta o trade-off entre velocidade de converg√™ncia e qualidade da solu√ß√£o no SGD? Discuta as implica√ß√µes para o treinamento de modelos generativos complexos.

2. Explique como o SGD pode ajudar a evitar overfitting em modelos generativos profundos. Quais s√£o as considera√ß√µes ao escolher entre SGD e gradiente descendente completo neste contexto?

### Conclus√£o

Os m√©todos de otimiza√ß√£o, como o gradiente descendente e o SGD, s√£o fundamentais para o treinamento eficaz de modelos generativos profundos usando MLE [16]. ==Enquanto o gradiente descendente oferece uma abordagem direta e teoricamente s√≥lida, o SGD proporciona efici√™ncia computacional e adaptabilidade a grandes conjuntos de dados, essenciais para aplica√ß√µes pr√°ticas em aprendizado profundo [17].==

A escolha entre estes m√©todos depende das caracter√≠sticas espec√≠ficas do problema, como o tamanho do conjunto de dados, a complexidade do modelo e os recursos computacionais dispon√≠veis [18]. Em muitos casos pr√°ticos, ==variantes do SGD, como Adam ou RMSprop, s√£o preferidas devido √† sua capacidade de adaptar as taxas de aprendizado e lidar com gradientes esparsos [19].==

√Ä medida que os modelos generativos se tornam mais complexos, a otimiza√ß√£o eficiente continua sendo um desafio crucial, impulsionando pesquisas em novas t√©cnicas de otimiza√ß√£o e estrat√©gias de regulariza√ß√£o [20].

### Quest√µes Avan√ßadas

1. Compare o desempenho te√≥rico e pr√°tico do SGD com m√©todos de segunda ordem, como o algoritmo de Newton, no contexto de treinamento de modelos generativos profundos. Quais s√£o os trade-offs entre precis√£o, velocidade e requerimentos de mem√≥ria?

2. Discuta como as t√©cnicas de otimiza√ß√£o estoc√°stica podem ser estendidas para lidar com problemas de otimiza√ß√£o com restri√ß√µes em modelos generativos, como garantir a normaliza√ß√£o adequada das distribui√ß√µes de probabilidade.

3. Analise o impacto da geometria da fun√ß√£o objetivo na efic√°cia do SGD para modelos generativos. Como t√©cnicas como normaliza√ß√£o de batch ou inicializa√ß√£o cuidadosa dos pesos podem melhorar a converg√™ncia?

### Refer√™ncias

[1] "The goal of learning is to return a model P_Œ∏ that precisely captures the distribution P_data from which our data was sampled" (Trecho de cs236_lecture4.pdf)

[2] "We want to construct P_Œ∏ as "close" as possible to P_data (recall we assume we are given a dataset D of samples from P_data)" (Trecho de cs236_lecture4.pdf)

[3] "Compute ‚àá_Œ∏ ‚Ñì(Œ∏) (by back propagation)" (Trecho de cs236_lecture4.pdf)

[4] "Œ∏_t+1 = Œ∏_t + Œ±_t ‚àá_Œ∏ ‚Ñì(Œ∏)" (Trecho de cs236_lecture4.pdf)

[5] "‚Ñì(Œ∏) = log L(Œ∏, D) = Œ£_j=1^m Œ£_i=1^n log p_neural(x_i^(j)|x_<i^(j); Œ∏_i)" (Trecho de cs236_lecture4.pdf)

[6] "1. Initialize Œ∏_0 at random
    2. Compute ‚àá_Œ∏ ‚Ñì(Œ∏) (by back propagation)
    3. Œ∏_t+1 = Œ∏_t + Œ±_t ‚àá_Œ∏ ‚Ñì(Œ∏)" (Trecho de cs236_lecture4.pdf)

[7] "Non-convex optimization problem, but often works well in practice" (Trecho de cs236_lecture4.pdf)

[8] "What if m = |D| is huge?" (Trecho de cs236_lecture4.pdf)

[9] "Monte Carlo: Sample x^(j) ~ D; ‚àá_Œ∏ ‚Ñì(Œ∏) ‚âà m Œ£_i=1^n ‚àá_Œ∏ log p_neural(x_i^(j)|x_<i^(j); Œ∏_i)" (Trecho de cs236_lecture4.pdf)

[10] "‚àá_Œ∏ ‚Ñì(Œ∏) = m Œï_x^(j)~D [Œ£_i=1^n ‚àá_Œ∏ log p_neural(x_i^(j)|x_<i^(j); Œ∏_i)]" (Trecho de cs236_lecture4.pdf)

[11] "Monte Carlo: Sample x^(j) ~ D; ‚àá_Œ∏ ‚Ñì(Œ∏) ‚âà m Œ£_i=1^n ‚àá_Œ∏ log p_neural(x_i^(j)|x_<i^(j); Œ∏_i)" (Trecho de cs236_lecture4.pdf)

[12] "Monte Carlo: Sample x^(j) ~ D; ‚àá_Œ∏ ‚Ñì(Œ∏) ‚âà m Œ£_i=1^n ‚àá_Œ∏ log p_neural(x_i^(j)|x_<i^(j); Œ∏_i)" (Trecho de cs236_lecture4.pdf)

[13] "Monte Carlo: Sample x^(j) ~ D; ‚àá_Œ∏ ‚Ñì(Œ∏) ‚âà m Œ£_i=1^n ‚àá_Œ∏ log p_neural(x_i^(j)|x_<i^(j); Œ∏_i)" (Trecho de cs236_lecture4.pdf)

[14] "Monte Carlo: Sample x^(j) ~ D; ‚àá_Œ∏ ‚Ñì(Œ∏) ‚âà m Œ£_i=1^n ‚àá_Œ∏ log p_neural(x_i^(j)|x_<i^(j); Œ∏_i)" (Trecho de cs236_lecture4.pdf)

[15] "For autoregressive models, it is easy to compute p_Œ∏(x)" (Trecho de cs236_lecture4.pdf)

[16] "Ideally, evaluate in parallel each conditional log p_neural(x_i^(j)|x_<i^(j); Œ∏_i). Not like RNNs." (Trecho de cs236_lecture4.pdf)

[17] "Natural to train them via maximum likelihood" (Trecho de cs236_lecture4.pdf)

[18] "Higher log-likelihood doesn't necessarily mean better looking samples" (Trecho de cs236_lecture4.pdf)

[19] "Other ways of measuring similarity are possible (Generative Adversarial Networks, GANs)" (Trecho de cs236_lecture4.pdf)

[20] "Natural to train them via maximum likelihood" (Trecho de cs236_lecture4.pdf)