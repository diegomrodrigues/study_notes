## Regra de Classifica√ß√£o em An√°lise Discriminante Linear

![image-20240802112459738](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240802112459738.png)

A regra de classifica√ß√£o √© um componente fundamental na An√°lise Discriminante Linear (LDA), fornecendo um mecanismo para atribuir novas observa√ß√µes a classes predefinidas com base nos valores ajustados pelo modelo [1]. Este resumo aprofunda-se nos aspectos te√≥ricos e pr√°ticos desta regra, explorando sua formula√ß√£o matem√°tica, implementa√ß√£o e implica√ß√µes para a classifica√ß√£o em problemas de m√∫ltiplas classes.

### Conceitos Fundamentais

| Conceito                        | Explica√ß√£o                                                   |
| ------------------------------- | ------------------------------------------------------------ |
| **Valores Ajustados**           | S√£o as estimativas das probabilidades posteriores de pertencimento a cada classe, calculadas pelo modelo LDA para uma dada observa√ß√£o. [1] |
| **Regra de Classifica√ß√£o**      | Crit√©rio que atribui uma nova observa√ß√£o √† classe com o maior valor ajustado, maximizando a probabilidade posterior estimada. [1] |
| **Fun√ß√£o Discriminante Linear** | Fun√ß√£o linear dos preditores que √© utilizada para calcular os valores ajustados e, consequentemente, determinar a classifica√ß√£o. [2] |

> ‚ö†Ô∏è **Nota Importante**: A regra de classifica√ß√£o em LDA assume que as classes s√£o mutuamente exclusivas e coletivamente exaustivas, ou seja, cada observa√ß√£o pertence a exatamente uma classe.

### Formula√ß√£o Matem√°tica da Regra de Classifica√ß√£o

A regra de classifica√ß√£o em LDA pode ser expressa matematicamente da seguinte forma [3]:

$$
\hat{G}(x) = \arg\max_{k} \hat{f}_k(x)
$$

Onde:
- $\hat{G}(x)$ √© a classe prevista para a observa√ß√£o $x$
- $\hat{f}_k(x)$ √© o valor ajustado (estimativa da probabilidade posterior) para a classe $k$
- $\arg\max_{k}$ denota o argumento $k$ que maximiza a express√£o subsequente

Esta formula√ß√£o encapsula o princ√≠pio fundamental da regra de classifica√ß√£o: atribuir a observa√ß√£o √† classe com a maior probabilidade posterior estimada [4].

#### C√°lculo dos Valores Ajustados

Os valores ajustados $\hat{f}_k(x)$ s√£o calculados usando as fun√ß√µes discriminantes lineares, que t√™m a forma geral [5]:

$$
\delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log \pi_k
$$

Onde:
- $\Sigma$ √© a matriz de covari√¢ncia comum estimada
- $\mu_k$ √© o vetor de m√©dias estimado para a classe $k$
- $\pi_k$ √© a probabilidade a priori estimada para a classe $k$

Os valores ajustados s√£o ent√£o obtidos atrav√©s da transforma√ß√£o softmax [6]:

$$
\hat{f}_k(x) = \frac{e^{\delta_k(x)}}{\sum_{j=1}^K e^{\delta_j(x)}}
$$

Esta transforma√ß√£o garante que os valores ajustados somem 1 e possam ser interpretados como probabilidades.

> ‚úîÔ∏è **Ponto de Destaque**: A transforma√ß√£o softmax preserva a ordem relativa dos valores discriminantes, garantindo que a classe com o maior $\delta_k(x)$ tamb√©m tenha o maior $\hat{f}_k(x)$.

Vamos criar um exemplo num√©rico passo a passo para ilustrar o c√°lculo dos valores ajustados em um problema de classifica√ß√£o com An√°lise Discriminante Linear (LDA) com tr√™s classes. Seguiremos cada etapa detalhadamente.

Exemplo: Classifica√ß√£o de tr√™s tipos de flores baseada em duas caracter√≠sticas: comprimento e largura da p√©tala.

Passo 1: Dados iniciais

Suponha que temos os seguintes dados estimados:

1) Matriz de covari√¢ncia comum estimada (Œ£):
   $$ \Sigma = \begin{bmatrix} 0.3 & 0.1 \\ 0.1 & 0.2 \end{bmatrix} $$

2) Vetores de m√©dias estimados para cada classe (Œº‚Çñ):
   $$ \mu_1 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}, \quad \mu_2 = \begin{bmatrix} 3 \\ 2 \end{bmatrix}, \quad \mu_3 = \begin{bmatrix} 4 \\ 3 \end{bmatrix} $$

3) Probabilidades a priori estimadas (œÄ‚Çñ):
   $$ \pi_1 = 0.3, \quad \pi_2 = 0.5, \quad \pi_3 = 0.2 $$

4) Nova observa√ß√£o a ser classificada:
   $$ x = \begin{bmatrix} 3.5 \\ 2.5 \end{bmatrix} $$

Passo 2: Calcular Œ£‚Åª¬π

$$ \Sigma^{-1} = \begin{bmatrix} 3.8462 & -1.9231 \\ -1.9231 & 5.7692 \end{bmatrix} $$

Passo 3: Calcular Œ¥‚Çñ(x) para cada classe

Para a classe 1:
$$ \begin{aligned}
\delta_1(x) &= x^T\Sigma^{-1}\mu_1 - \frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1 + \log \pi_1 \\
&= [3.5 \quad 2.5] \begin{bmatrix} 3.8462 & -1.9231 \\ -1.9231 & 5.7692 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} - \frac{1}{2}[2 \quad 1] \begin{bmatrix} 3.8462 & -1.9231 \\ -1.9231 & 5.7692 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} + \log 0.3 \\
&= 15.3846 - 5.7692 - 1.2040 \\
&= 8.4114
\end{aligned} $$

Para a classe 2:
$$ \begin{aligned}
\delta_2(x) &= x^T\Sigma^{-1}\mu_2 - \frac{1}{2}\mu_2^T\Sigma^{-1}\mu_2 + \log \pi_2 \\
&= [3.5 \quad 2.5] \begin{bmatrix} 3.8462 & -1.9231 \\ -1.9231 & 5.7692 \end{bmatrix} \begin{bmatrix} 3 \\ 2 \end{bmatrix} - \frac{1}{2}[3 \quad 2] \begin{bmatrix} 3.8462 & -1.9231 \\ -1.9231 & 5.7692 \end{bmatrix} \begin{bmatrix} 3 \\ 2 \end{bmatrix} + \log 0.5 \\
&= 26.9231 - 13.4615 - 0.6931 \\
&= 12.7685
\end{aligned} $$

Para a classe 3:
$$ \begin{aligned}
\delta_3(x) &= x^T\Sigma^{-1}\mu_3 - \frac{1}{2}\mu_3^T\Sigma^{-1}\mu_3 + \log \pi_3 \\
&= [3.5 \quad 2.5] \begin{bmatrix} 3.8462 & -1.9231 \\ -1.9231 & 5.7692 \end{bmatrix} \begin{bmatrix} 4 \\ 3 \end{bmatrix} - \frac{1}{2}[4 \quad 3] \begin{bmatrix} 3.8462 & -1.9231 \\ -1.9231 & 5.7692 \end{bmatrix} \begin{bmatrix} 4 \\ 3 \end{bmatrix} + \log 0.2 \\
&= 38.4615 - 24.2308 - 1.6094 \\
&= 12.6213
\end{aligned} $$

Passo 4: Aplicar a transforma√ß√£o softmax para obter $\hat{f}_k(x)$

$$ \begin{aligned}
\hat{f}_1(x) &= \frac{e^{8.4114}}{e^{8.4114} + e^{12.7685} + e^{12.6213}} = 0.0041 \\
\hat{f}_2(x) &= \frac{e^{12.7685}}{e^{8.4114} + e^{12.7685} + e^{12.6213}} = 0.5306 \\
\hat{f}_3(x) &= \frac{e^{12.6213}}{e^{8.4114} + e^{12.7685} + e^{12.6213}} = 0.4653
\end{aligned} $$

Passo 5: Interpretar os resultados

Os valores ajustados $\hat{f}_k(x)$ representam as probabilidades posteriores estimadas de que a nova observa√ß√£o x perten√ßa a cada uma das tr√™s classes:

- Classe 1: 0.41% de probabilidade
- Classe 2: 53.06% de probabilidade
- Classe 3: 46.53% de probabilidade

Conclus√£o: Seguindo a regra de classifica√ß√£o LDA, classificar√≠amos esta nova observa√ß√£o na Classe 2, pois ela tem o maior valor ajustado (maior probabilidade posterior estimada).

Este exemplo num√©rico demonstra como os c√°lculos s√£o realizados na pr√°tica, desde as fun√ß√µes discriminantes lineares at√© a obten√ß√£o das probabilidades posteriores atrav√©s da transforma√ß√£o softmax. Ele ilustra como o LDA utiliza as informa√ß√µes das m√©dias das classes, da matriz de covari√¢ncia comum e das probabilidades a priori para fazer previs√µes sobre novas observa√ß√µes.

### Implementa√ß√£o da Regra de Classifica√ß√£o

A implementa√ß√£o pr√°tica da regra de classifica√ß√£o em um ambiente de programa√ß√£o como Python pode ser realizada de forma eficiente usando opera√ß√µes vetorizadas. Aqui est√° um exemplo conciso de como isso poderia ser feito:

```python
import numpy as np

def lda_classify(X, means, cov_inv, priors):
    K = means.shape[0]  # N√∫mero de classes
    N = X.shape[0]  # N√∫mero de observa√ß√µes
    
    # Calcula os valores discriminantes para todas as classes
    discriminants = np.dot(X, np.dot(cov_inv, means.T)) - \
                    0.5 * np.sum(np.dot(means, cov_inv) * means, axis=1) + \
                    np.log(priors)
    
    # Aplica softmax para obter probabilidades
    exp_disc = np.exp(discriminants - np.max(discriminants, axis=1, keepdims=True))
    probs = exp_disc / np.sum(exp_disc, axis=1, keepdims=True)
    
    # Classifica cada observa√ß√£o na classe com maior probabilidade
    predictions = np.argmax(probs, axis=1)
    
    return predictions, probs
```

Esta implementa√ß√£o assume que `X` √© uma matriz de observa√ß√µes, `means` √© uma matriz de m√©dias das classes, `cov_inv` √© a inversa da matriz de covari√¢ncia comum, e `priors` √© um vetor de probabilidades a priori das classes.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a regra de classifica√ß√£o LDA se comportaria se duas classes tivessem exatamente o mesmo valor ajustado m√°ximo para uma determinada observa√ß√£o? Como voc√™ modificaria a implementa√ß√£o para lidar com esse caso?

2. Explique como a escolha das probabilidades a priori $\pi_k$ afeta a regra de classifica√ß√£o e em quais cen√°rios pr√°ticos voc√™ consideraria modific√°-las a partir de suas estimativas emp√≠ricas.

### Propriedades e Implica√ß√µes da Regra de Classifica√ß√£o

A regra de classifica√ß√£o LDA possui v√°rias propriedades importantes que influenciam seu desempenho e aplicabilidade [7]:

1. **Linearidade das Fronteiras de Decis√£o**: As fronteiras entre as regi√µes de classifica√ß√£o s√£o hiperplanos no espa√ßo de caracter√≠sticas, o que pode ser uma limita√ß√£o em dados com rela√ß√µes n√£o-lineares complexas entre as classes.

2. **Sensibilidade √† Escala**: A LDA √© invariante √† escala das vari√°veis preditoras, desde que a transforma√ß√£o de escala seja aplicada consistentemente a todas as classes.

3. **Robustez a Outliers**: Comparada a m√©todos como QDA (An√°lise Discriminante Quadr√°tica), a LDA tende a ser mais robusta a outliers devido √† suposi√ß√£o de covari√¢ncia comum entre as classes.

4. **Efici√™ncia Computacional**: A regra de classifica√ß√£o LDA √© computacionalmente eficiente, especialmente para problemas com muitas classes, pois requer apenas o c√°lculo de fun√ß√µes lineares.

> ‚ùó **Ponto de Aten√ß√£o**: Embora eficiente, a regra de classifica√ß√£o LDA assume normalidade multivariada e homoscedasticidade (igualdade de matrizes de covari√¢ncia entre classes). Viola√ß√µes dessas suposi√ß√µes podem impactar o desempenho do classificador.

### Compara√ß√£o com Outras T√©cnicas de Classifica√ß√£o

| üëç Vantagens da Regra LDA                               | üëé Desvantagens da Regra LDA                                  |
| ------------------------------------------------------ | ------------------------------------------------------------ |
| Simplicidade e interpretabilidade [8]                  | Suposi√ß√µes restritivas sobre a distribui√ß√£o dos dados [9]    |
| Efici√™ncia computacional em problemas multiclasse [10] | Incapacidade de capturar rela√ß√µes n√£o-lineares complexas [11] |
| Bom desempenho quando as suposi√ß√µes s√£o atendidas [12] | Sensibilidade a classes altamente desequilibradas [13]       |

### Extens√µes e Varia√ß√µes

1. **Regulariza√ß√£o**: Incorpora√ß√£o de termos de regulariza√ß√£o na estima√ß√£o da matriz de covari√¢ncia para melhorar a estabilidade e o desempenho em dimens√µes elevadas [14].

2. **LDA Esparsa**: Modifica√ß√µes da regra de classifica√ß√£o para promover esparsidade nos coeficientes discriminantes, facilitando a interpreta√ß√£o e potencialmente melhorando a generaliza√ß√£o [15].

3. **LDA Kernel**: Extens√£o n√£o-linear da LDA usando t√©cnicas de kernel para lidar com fronteiras de decis√£o n√£o-lineares [16].

A formula√ß√£o matem√°tica para LDA Kernel pode ser expressa como:

$$
\delta_k(x) = \sum_{i=1}^N \alpha_{ki} K(x, x_i) + \beta_k
$$

Onde $K(x, x_i)$ √© a fun√ß√£o kernel, $\alpha_{ki}$ s√£o os coeficientes aprendidos, e $\beta_k$ √© o termo de vi√©s para a classe $k$.

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como voc√™ modificaria a regra de classifica√ß√£o LDA para incorporar custos diferentes para diferentes tipos de erros de classifica√ß√£o? Forne√ßa uma formula√ß√£o matem√°tica para esta modifica√ß√£o.

2. Descreva um cen√°rio em que a LDA Kernel seria prefer√≠vel √† LDA padr√£o e explique como voc√™ escolheria e otimizaria a fun√ß√£o kernel apropriada.

### Conclus√£o

A regra de classifica√ß√£o em An√°lise Discriminante Linear √© um componente crucial que traduz os resultados do modelo em decis√µes de classifica√ß√£o pr√°ticas. Sua simplicidade, efici√™ncia computacional e base te√≥rica s√≥lida a tornam uma escolha popular em muitas aplica√ß√µes de aprendizado de m√°quina e estat√≠stica [17]. No entanto, √© fundamental que os praticantes estejam cientes das suposi√ß√µes subjacentes e das limita√ß√µes potenciais ao aplicar esta t√©cnica em problemas do mundo real. A compreens√£o profunda da regra de classifica√ß√£o LDA, incluindo suas propriedades matem√°ticas e implica√ß√µes pr√°ticas, √© essencial para sua aplica√ß√£o eficaz e para o desenvolvimento de extens√µes e melhorias futuras.

### Quest√µes Avan√ßadas

1. Considere um problema de classifica√ß√£o com tr√™s classes em um espa√ßo bidimensional. Dado que a LDA produziu as seguintes fun√ß√µes discriminantes:

   $\delta_1(x) = 2x_1 + 3x_2 - 1$
   $\delta_2(x) = -x_1 + 2x_2 + 2$
   $\delta_3(x) = x_1 - x_2 + 1$

   Descreva geometricamente as regi√µes de decis√£o resultantes e derive as equa√ß√µes das fronteiras de decis√£o entre as classes.

2. Em um cen√°rio de alta dimensionalidade (p >> n), como voc√™ modificaria a regra de classifica√ß√£o LDA para lidar com o problema de singularidade da matriz de covari√¢ncia? Discuta as implica√ß√µes te√≥ricas e pr√°ticas de sua abordagem.

3. Proponha e justifique uma m√©trica de avalia√ß√£o apropriada para um classificador LDA em um problema de detec√ß√£o de fraude banc√°ria, onde as classes s√£o altamente desequilibradas (99.9% transa√ß√µes leg√≠timas, 0.1% fraudulentas) e o custo de falsos negativos √© significativamente maior que o de falsos positivos.
