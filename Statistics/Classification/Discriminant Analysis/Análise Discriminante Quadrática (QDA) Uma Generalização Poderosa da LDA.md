## An√°lise Discriminante Quadr√°tica (QDA): Uma Generaliza√ß√£o Poderosa da LDA

![image-20240802163204034](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240802163204034.png)

A An√°lise Discriminante Quadr√°tica (QDA) emerge como uma extens√£o sofisticada da An√°lise Discriminante Linear (LDA), oferecendo uma abordagem mais flex√≠vel e potente para problemas de classifica√ß√£o em estat√≠stica e aprendizado de m√°quina [1]. Enquanto a LDA assume matrizes de covari√¢ncia id√™nticas para todas as classes, a QDA relaxa essa restri√ß√£o, permitindo que cada classe tenha sua pr√≥pria matriz de covari√¢ncia [2]. Esta generaliza√ß√£o resulta em fronteiras de decis√£o quadr√°ticas, em contraste com as fronteiras lineares da LDA, proporcionando uma capacidade significativamente maior de modelar rela√ß√µes complexas nos dados.

### Conceitos Fundamentais

| Conceito                                        | Explica√ß√£o                                                   |
| ----------------------------------------------- | ------------------------------------------------------------ |
| **Matriz de Covari√¢ncia Espec√≠fica por Classe** | Na QDA, cada classe $k$ tem sua pr√≥pria matriz de covari√¢ncia $\Sigma_k$, permitindo uma representa√ß√£o mais precisa da distribui√ß√£o dos dados dentro de cada classe. [2] |
| **Fun√ß√µes Discriminantes Quadr√°ticas**          | As fun√ß√µes discriminantes na QDA s√£o quadr√°ticas em $x$, resultando em fronteiras de decis√£o n√£o-lineares no espa√ßo de caracter√≠sticas. [3] |
| **Estima√ß√£o de Par√¢metros**                     | A QDA requer a estima√ß√£o de par√¢metros adicionais em compara√ß√£o com a LDA, incluindo matrizes de covari√¢ncia separadas para cada classe. [4] |

> ‚ö†Ô∏è **Nota Importante**: A flexibilidade adicional da QDA vem com o custo de um aumento significativo no n√∫mero de par√¢metros a serem estimados, especialmente em espa√ßos de alta dimens√£o.

### Formula√ß√£o Matem√°tica da QDA

A QDA baseia-se no modelo gaussiano para as densidades de classe condicional, mas sem a restri√ß√£o de covari√¢ncias iguais:

$$
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}} e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}
$$

Onde:
- $f_k(x)$ √© a densidade de probabilidade da classe $k$
- $\mu_k$ √© o vetor m√©dio da classe $k$
- $\Sigma_k$ √© a matriz de covari√¢ncia da classe $k$
- $p$ √© a dimens√£o do espa√ßo de caracter√≠sticas

A fun√ß√£o discriminante quadr√°tica para a classe $k$ √© dada por [5]:

$$
\delta_k(x) = -\frac{1}{2}\log|\Sigma_k| - \frac{1}{2}(x - \mu_k)^T\Sigma_k^{-1}(x - \mu_k) + \log\pi_k
$$

Onde $\pi_k$ √© a probabilidade a priori da classe $k$.

#### Fronteiras de Decis√£o Quadr√°ticas

A fronteira de decis√£o entre duas classes $k$ e $l$ √© definida pelo conjunto de pontos que satisfazem $\delta_k(x) = \delta_l(x)$, resultando em uma equa√ß√£o quadr√°tica em $x$ [6]:

$$
\{x : (\mu_k - \mu_l)^T(\Sigma_k^{-1} - \Sigma_l^{-1})x + \frac{1}{2}x^T(\Sigma_l^{-1} - \Sigma_k^{-1})x + c = 0\}
$$

Onde $c$ √© uma constante que depende de $\mu_k$, $\mu_l$, $\Sigma_k$, $\Sigma_l$, $\pi_k$, e $\pi_l$.

> ‚úîÔ∏è **Ponto de Destaque**: A natureza quadr√°tica das fronteiras de decis√£o confere √† QDA uma capacidade superior de modelar rela√ß√µes n√£o-lineares nos dados, tornando-a mais adequada para distribui√ß√µes de classe com formas complexas.

### Estima√ß√£o de Par√¢metros na QDA

A estima√ß√£o dos par√¢metros na QDA segue o princ√≠pio da m√°xima verossimilhan√ßa [7]:

1. **Vetores M√©dios**: $\hat{\mu}_k = \frac{1}{N_k}\sum_{g_i=k} x_i$
2. **Matrizes de Covari√¢ncia**: $\hat{\Sigma}_k = \frac{1}{N_k}\sum_{g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$
3. **Probabilidades a Priori**: $\hat{\pi}_k = \frac{N_k}{N}$

Onde $N_k$ √© o n√∫mero de observa√ß√µes na classe $k$ e $N$ √© o n√∫mero total de observa√ß√µes.

#### [Quest√µes T√©cnicas/Te√≥ricas]

1. Como a complexidade computacional da QDA se compara √† da LDA em termos do n√∫mero de par√¢metros a serem estimados?
2. Em que cen√°rios a QDA seria prefer√≠vel √† LDA, e quais s√£o os trade-offs envolvidos nessa escolha?

### Implementa√ß√£o e Considera√ß√µes Pr√°ticas

A implementa√ß√£o da QDA em Python pode ser realizada utilizando bibliotecas como scikit-learn. Aqui est√° um exemplo conciso de como aplicar QDA:

```python
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuma que X e y j√° est√£o definidos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train, y_train)

y_pred = qda.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Acur√°cia da QDA: {accuracy:.4f}")
```

> ‚ùó **Ponto de Aten√ß√£o**: A QDA requer mais dados de treinamento do que a LDA para estimar os par√¢metros adicionais de forma confi√°vel. Em conjuntos de dados menores ou de alta dimensionalidade, pode haver um risco de overfitting.

### Compara√ß√£o entre QDA e LDA

| üëç Vantagens da QDA                                           | üëé Desvantagens da QDA                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Maior flexibilidade na modelagem de distribui√ß√µes de classe [8] | Requer mais par√¢metros, aumentando o risco de overfitting [9] |
| Fronteiras de decis√£o n√£o-lineares, adequadas para rela√ß√µes complexas [10] | Maior complexidade computacional [11]                        |
| Melhor desempenho quando as covari√¢ncias das classes diferem significativamente [12] | Menos robusto em dados de alta dimensionalidade com amostras limitadas [13] |

### Regulariza√ß√£o na QDA

Para mitigar o risco de overfitting, especialmente em cen√°rios de alta dimensionalidade ou com amostras limitadas, t√©cnicas de regulariza√ß√£o podem ser aplicadas √† QDA [14]:

1. **Shrinkage**: Reduz a vari√¢ncia das estimativas das matrizes de covari√¢ncia atrav√©s de uma combina√ß√£o convexa com uma matriz de identidade:

   $$\hat{\Sigma}_k(\alpha) = \alpha\hat{\Sigma}_k + (1-\alpha)I$$

   Onde $\alpha \in [0,1]$ √© o par√¢metro de regulariza√ß√£o.

2. **Pooling Parcial**: Combina as matrizes de covari√¢ncia estimadas com uma matriz de covari√¢ncia comum:

   $$\hat{\Sigma}_k(\gamma) = \gamma\hat{\Sigma}_k + (1-\gamma)\hat{\Sigma}$$

   Onde $\gamma \in [0,1]$ e $\hat{\Sigma}$ √© a matriz de covari√¢ncia combinada de todas as classes.

> üí° **Dica**: A escolha dos par√¢metros de regulariza√ß√£o ($\alpha$ ou $\gamma$) pode ser otimizada atrav√©s de valida√ß√£o cruzada.

#### [Quest√µes T√©cnicas/Te√≥ricas]

1. Como a regulariza√ß√£o afeta o vi√©s-vari√¢ncia trade-off na QDA?
2. Em um cen√°rio com classes altamente desequilibradas, como a QDA se compara √† LDA em termos de desempenho e robustez?

### Conclus√£o

A An√°lise Discriminante Quadr√°tica representa uma evolu√ß√£o significativa em rela√ß√£o √† LDA, oferecendo um framework mais flex√≠vel para classifica√ß√£o [15]. Sua capacidade de modelar fronteiras de decis√£o n√£o-lineares a torna particularmente valiosa em cen√°rios onde as distribui√ß√µes de classe exibem caracter√≠sticas distintas de forma e orienta√ß√£o [16]. No entanto, essa flexibilidade adicional vem com o custo de uma maior complexidade computacional e um risco aumentado de overfitting, especialmente em espa√ßos de alta dimens√£o ou com amostras limitadas [17]. A aplica√ß√£o judiciosa de t√©cnicas de regulariza√ß√£o e uma avalia√ß√£o cuidadosa do trade-off entre complexidade do modelo e tamanho do conjunto de dados s√£o cruciais para explorar todo o potencial da QDA em problemas de classifica√ß√£o do mundo real.

### Quest√µes Avan√ßadas

1. Como voc√™ abordaria o problema de sele√ß√£o de caracter√≠sticas no contexto da QDA, considerando o trade-off entre a capacidade discriminativa e o risco de overfitting?

2. Desenvolva uma estrat√©gia para combinar QDA com t√©cnicas de redu√ß√£o de dimensionalidade (e.g., PCA, LDA) para lidar com dados de alta dimensionalidade. Quais seriam os pr√≥s e contras dessa abordagem?

3. Considerando um cen√°rio de aprendizado semi-supervisionado, como voc√™ adaptaria o algoritmo QDA para incorporar informa√ß√µes de dados n√£o rotulados na estima√ß√£o dos par√¢metros do modelo?

### Refer√™ncias

[1] "Quadratic discriminant analysis (QDA) emerge como uma extens√£o sofisticada da An√°lise Discriminante Linear (LDA)" (Trecho de ESL II)

[2] "Enquanto a LDA assume matrizes de covari√¢ncia id√™nticas para todas as classes, a QDA relaxa essa restri√ß√£o, permitindo que cada classe tenha sua pr√≥pria matriz de covari√¢ncia" (Trecho de ESL II)

[3] "As fun√ß√µes discriminantes na QDA s√£o quadr√°ticas em x" (Trecho de ESL II)

[4] "A QDA requer a estima√ß√£o de par√¢metros adicionais em compara√ß√£o com a LDA, incluindo matrizes de covari√¢ncia separadas para cada classe" (Trecho de ESL II)

[5] "A fun√ß√£o discriminante quadr√°tica para a classe k √© dada por Œ¥_k(x) = -1/2 log |Œ£_k| - 1/2 (x - Œº_k)^T Œ£_k^(-1) (x - Œº_k) + log œÄ_k" (Trecho de ESL II)

[6] "A fronteira de decis√£o entre duas classes k e l √© definida pelo conjunto de pontos que satisfazem Œ¥_k(x) = Œ¥_l(x), resultando em uma equa√ß√£o quadr√°tica em x" (Trecho de ESL II)

[7] "A estima√ß√£o dos par√¢metros na QDA segue o princ√≠pio da m√°xima verossimilhan√ßa" (Trecho de ESL II)

[8] "Maior flexibilidade na modelagem de distribui√ß√µes de classe" (Trecho de ESL II)

[9] "Requer mais par√¢metros, aumentando o risco de overfitting" (Trecho de ESL II)

[10] "Fronteiras de decis√£o n√£o-lineares, adequadas para rela√ß√µes complexas" (Trecho de ESL II)

[11] "Maior complexidade computacional" (Trecho de ESL II)

[12] "Melhor desempenho quando as covari√¢ncias das classes diferem significativamente" (Trecho de ESL II)

[13] "Menos robusto em dados de alta dimensionalidade com amostras limitadas" (Trecho de ESL II)

[14] "Para mitigar o risco de overfitting, especialmente em cen√°rios de alta dimensionalidade ou com amostras limitadas, t√©cnicas de regulariza√ß√£o podem ser aplicadas √† QDA" (Trecho de ESL II)

[15] "A An√°lise Discriminante Quadr√°tica representa uma evolu√ß√£o significativa em rela√ß√£o √† LDA" (Trecho de ESL II)

[16] "Sua capacidade de modelar fronteiras de decis√£o n√£o-lineares a torna particularmente valiosa em cen√°rios onde as distribui√ß√µes de classe exibem caracter√≠sticas distintas de forma e orienta√ß√£o" (Trecho de ESL II)

[17] "No entanto, essa flexibilidade adicional vem com o custo de uma maior complexidade computacional e um risco aumentado de overfitting, especialmente em espa√ßos de alta dimens√£o ou com amostras limitadas" (Trecho de ESL II)