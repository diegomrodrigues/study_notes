## Fun√ß√µes Discriminantes Lineares na An√°lise Discriminante Linear (LDA)

<img src="C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240802162000958.png" alt="image-20240802162000958" style="zoom: 80%;" />

### Introdu√ß√£o

As fun√ß√µes discriminantes lineares s√£o fundamentais na teoria de classifica√ß√£o estat√≠stica, particularmente na An√°lise Discriminante Linear (LDA). Elas fornecem um m√©todo poderoso e interpret√°vel para separar classes em problemas de classifica√ß√£o multivariada [1]. Este resumo explorar√° em profundidade a deriva√ß√£o, interpreta√ß√£o e aplica√ß√£o dessas fun√ß√µes no contexto da LDA.

### Conceitos Fundamentais

| Conceito                 | Explica√ß√£o                                                   |
| ------------------------ | ------------------------------------------------------------ |
| **Fun√ß√£o Discriminante** | Uma fun√ß√£o matem√°tica que mapeia um vetor de caracter√≠sticas de entrada para um escore, usado para tomar decis√µes de classifica√ß√£o [1]. |
| **Linearidade**          | A propriedade de uma fun√ß√£o discriminante que permite que ela seja expressa como uma combina√ß√£o linear das vari√°veis de entrada [2]. |
| **Fronteira de Decis√£o** | O locus de pontos no espa√ßo de caracter√≠sticas onde a fun√ß√£o discriminante √© igual para duas ou mais classes [3]. |

> ‚úîÔ∏è **Ponto de Destaque**: As fun√ß√µes discriminantes lineares na LDA s√£o derivadas assumindo que as classes t√™m distribui√ß√µes gaussianas com matrizes de covari√¢ncia iguais [4].

### Deriva√ß√£o das Fun√ß√µes Discriminantes Lineares

A deriva√ß√£o das fun√ß√µes discriminantes lineares na LDA come√ßa com a suposi√ß√£o de que cada classe $k$ tem uma distribui√ß√£o gaussiana multivariada [5]:

$$
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)}
$$

onde $x$ √© o vetor de caracter√≠sticas, $\mu_k$ √© o vetor m√©dio da classe $k$, e $\Sigma$ √© a matriz de covari√¢ncia comum a todas as classes.

Para classifica√ß√£o, usamos a regra de Bayes [6]:

$$
P(G=k|X=x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^K f_l(x)\pi_l}
$$

onde $\pi_k$ √© a probabilidade a priori da classe $k$.

Tomando o logaritmo e simplificando, obtemos a fun√ß√£o discriminante linear [7]:

$$
\delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log\pi_k
$$

> ‚ùó **Ponto de Aten√ß√£o**: A linearidade em $x$ surge devido ao cancelamento dos termos quadr√°ticos na expans√£o do expoente gaussiano, uma consequ√™ncia direta da suposi√ß√£o de covari√¢ncias iguais [8].

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a suposi√ß√£o de covari√¢ncias iguais entre as classes influencia a forma das fun√ß√µes discriminantes na LDA?
2. Explique por que o termo quadr√°tico em $x$ desaparece na deriva√ß√£o da fun√ß√£o discriminante linear.

### Interpreta√ß√£o Geom√©trica

As fun√ß√µes discriminantes lineares definem hiperplanos no espa√ßo de caracter√≠sticas [9]. A fronteira de decis√£o entre duas classes $k$ e $l$ √© dada por:

$$
\{x : \delta_k(x) = \delta_l(x)\}
$$

Esta √© uma equa√ß√£o linear em $x$, representando um hiperplano [10].

![image-20240802162423217](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20240802162423217.png)

> ‚ö†Ô∏è **Nota Importante**: Em um problema com $K$ classes, haver√° no m√°ximo $K-1$ fun√ß√µes discriminantes linearmente independentes [11].

### Estima√ß√£o dos Par√¢metros

Na pr√°tica, os par√¢metros $\mu_k$, $\Sigma$, e $\pi_k$ s√£o desconhecidos e devem ser estimados a partir dos dados de treinamento [12]. As estimativas de m√°xima verossimilhan√ßa s√£o:

1. $\hat{\mu}_k = \frac{1}{N_k}\sum_{g_i=k} x_i$
2. $\hat{\Sigma} = \frac{1}{N-K}\sum_{k=1}^K\sum_{g_i=k}(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$
3. $\hat{\pi}_k = N_k/N$

onde $N_k$ √© o n√∫mero de observa√ß√µes na classe $k$ e $N$ √© o n√∫mero total de observa√ß√µes [13].

#### Demonstra√ß√£o

Come√ßamos com as seguintes suposi√ß√µes:

1. Temos $K$ classes
2. Cada classe $k$ segue uma distribui√ß√£o normal multivariada $N(\mu_k, \Sigma)$
3. Todas as classes compartilham a mesma matriz de covari√¢ncia $\Sigma$
4. Temos $N$ observa√ß√µes no total, com $N_k$ observa√ß√µes na classe $k$

Passo 1: Fun√ß√£o de verossimilhan√ßa

A fun√ß√£o de verossimilhan√ßa para todas as observa√ß√µes √©:

$$
L(\mu_1,...,\mu_K, \Sigma, \pi_1,...,\pi_K) = \prod_{i=1}^N [\pi_{g_i} f_{g_i}(x_i)]
$$

Onde $g_i$ √© a classe da observa√ß√£o $i$, e $f_k(x)$ √© a densidade da distribui√ß√£o normal multivariada para a classe $k$.

Passo 2: Log-verossimilhan√ßa

Tomamos o logaritmo da fun√ß√£o de verossimilhan√ßa:

$$
\ell = \log L = \sum_{i=1}^N [\log \pi_{g_i} + \log f_{g_i}(x_i)]
$$

Passo 3: Estimativa de $\pi_k$

Para estimar $\pi_k$, maximizamos $\ell$ sujeito √† restri√ß√£o $\sum_{k=1}^K \pi_k = 1$. Usando multiplicadores de Lagrange, obtemos:

$$
\frac{\partial\ell}{\partial\pi_k} = \frac{N_k}{\pi_k} - \lambda = 0
$$

Resolvendo e usando a restri√ß√£o, obtemos:

$$
\hat{\pi}_k = \frac{N_k}{N}
$$

Passo 4: Estimativa de $\mu_k$

Para estimar $\mu_k$, derivamos $\ell$ em rela√ß√£o a $\mu_k$ e igualamos a zero:

$$
\frac{\partial\ell}{\partial\mu_k} = \sum_{g_i=k} \Sigma^{-1}(x_i - \mu_k) = 0
$$

Resolvendo, obtemos:

$$
\hat{\mu}_k = \frac{1}{N_k} \sum_{g_i=k} x_i
$$

Passo 5: Estimativa de $\Sigma$

Para estimar $\Sigma$, derivamos $\ell$ em rela√ß√£o a $\Sigma^{-1}$ e igualamos a zero:

$$
\frac{\partial\ell}{\partial\Sigma^{-1}} = \frac{1}{2} \sum_{k=1}^K N_k [\Sigma - \frac{1}{N_k} \sum_{g_i=k} (x_i - \mu_k)(x_i - \mu_k)^T] = 0
$$

Resolvendo e substituindo as estimativas de $\mu_k$, obtemos:

$$
\hat{\Sigma} = \frac{1}{N-K} \sum_{k=1}^K \sum_{g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T
$$

Observe que usamos $N-K$ no denominador em vez de $N$ para obter um estimador n√£o-viesado.

Assim, chegamos √†s estimativas de m√°xima verossimilhan√ßa:

1. $\hat{\pi}_k = \frac{N_k}{N}$
2. $\hat{\mu}_k = \frac{1}{N_k} \sum_{g_i=k} x_i$
3. $\hat{\Sigma} = \frac{1}{N-K} \sum_{k=1}^K \sum_{g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$

Estas estimativas s√£o intuitivas:
- $\hat{\pi}_k$ √© a propor√ß√£o de observa√ß√µes na classe $k$
- $\hat{\mu}_k$ √© a m√©dia das observa√ß√µes na classe $k$
- $\hat{\Sigma}$ √© a m√©dia ponderada das matrizes de covari√¢ncia dentro de cada classe

Esta demonstra√ß√£o mostra como as estimativas surgem naturalmente da maximiza√ß√£o da verossimilhan√ßa sob as suposi√ß√µes do modelo LDA.

### Propriedades e Limita√ß√µes

#### üëçVantagens
* Simplicidade e interpretabilidade [14]
* Efici√™ncia computacional [15]
* Bom desempenho quando as suposi√ß√µes s√£o atendidas [16]

#### üëéDesvantagens
* Sensibilidade a outliers [17]
* Desempenho sub√≥timo quando as classes n√£o s√£o linearmente separ√°veis [18]
* Suposi√ß√£o restritiva de covari√¢ncias iguais [19]

### Extens√µes e Variantes

1. **An√°lise Discriminante Quadr√°tica (QDA)**: Relaxa a suposi√ß√£o de covari√¢ncias iguais, resultando em fronteiras de decis√£o quadr√°ticas [20].

2. **An√°lise Discriminante Regularizada**: Introduz um termo de regulariza√ß√£o para lidar com multicolinearidade e melhorar a estabilidade [21].

$$
\hat{\Sigma}(\alpha) = \alpha\hat{\Sigma} + (1-\alpha)\hat{\sigma}^2I
$$

onde $\alpha \in [0,1]$ √© o par√¢metro de regulariza√ß√£o [22].

3. **LDA de Posto Reduzido**: Restringe as m√©dias das classes a um subespa√ßo de dimens√£o menor, √∫til para visualiza√ß√£o e redu√ß√£o de dimensionalidade [23].

#### Quest√µes T√©cnicas/Te√≥ricas

1. Como a LDA se compara √† regress√£o log√≠stica em termos de suposi√ß√µes e performance?
2. Descreva um cen√°rio em que a QDA seria prefer√≠vel √† LDA.

### Implementa√ß√£o em Python

Aqui est√° um exemplo simplificado de implementa√ß√£o de LDA em Python:

```python
import numpy as np
from scipy.linalg import inv

class LDA:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.means = [X[y == c].mean(axis=0) for c in self.classes]
        self.priors = [np.mean(y == c) for c in self.classes]
        
        Sw = np.zeros((X.shape[1], X.shape[1]))
        for c, mean in zip(self.classes, self.means):
            Sw += np.cov(X[y == c].T) * (sum(y == c) - 1)
        self.Sw = Sw / (len(X) - len(self.classes))
        
    def predict(self, X):
        return np.argmax([self._discriminant_function(X, c) for c in range(len(self.classes))], axis=0)
    
    def _discriminant_function(self, X, c):
        return X @ inv(self.Sw) @ self.means[c] - 0.5 * self.means[c] @ inv(self.Sw) @ self.means[c] + np.log(self.priors[c])
```

> ‚ö†Ô∏è **Nota Importante**: Esta implementa√ß√£o √© simplificada e n√£o inclui otimiza√ß√µes ou tratamento de casos especiais que seriam necess√°rios em uma implementa√ß√£o robusta para uso em produ√ß√£o.

### Conclus√£o

As fun√ß√µes discriminantes lineares na LDA oferecem uma abordagem poderosa e interpret√°vel para problemas de classifica√ß√£o multivariada. Sua deriva√ß√£o a partir de princ√≠pios estat√≠sticos s√≥lidos, combinada com sua efici√™ncia computacional, torna a LDA uma ferramenta valiosa no arsenal de um cientista de dados. No entanto, √© crucial entender suas limita√ß√µes e saber quando aplicar extens√µes ou m√©todos alternativos.

### Quest√µes Avan√ßadas

1. Como voc√™ modificaria a implementa√ß√£o de LDA para lidar com classes altamente desequilibradas? Discuta as implica√ß√µes te√≥ricas e pr√°ticas dessa modifica√ß√£o.

2. Derive a forma da fun√ß√£o discriminante para a An√°lise Discriminante Quadr√°tica (QDA) e explique como isso afeta a complexidade computacional e a capacidade de modelagem em compara√ß√£o com a LDA.

3. Proponha e justifique um m√©todo para combinar LDA com t√©cnicas de ensemble learning, como bagging ou boosting. Que desafios voc√™ antecipa e como os abordaria?

### Refer√™ncias

[1] "A classifica√ß√£o linear discriminante (LDA) √© uma generaliza√ß√£o da regra discriminante de Fisher." (Trecho de ESL II)

[2] "Estas fun√ß√µes discriminantes lineares implicam que as fronteiras de decis√£o entre as classes s√£o lineares em x." (Trecho de ESL II)

[3] "O conjunto onde Pr(G = k|X = x) = Pr(G = ‚Ñì|X = x) √© uma fronteira de decis√£o linear." (Trecho de ESL II)

[4] "Suponha que modelamos cada densidade de classe como multivariada Gaussiana." (Trecho de ESL II)

[5] "f_k(x) = 1/(2œÄ)^(p/2)|Œ£_k|^(1/2) e^(-(1/2)(x-Œº_k)^T Œ£_k^(-1)(x-Œº_k))." (Trecho de ESL II)

[6] "Uma simples aplica√ß√£o do teorema de Bayes nos d√° Pr(G = k|X = x) = f_k(x)œÄ_k / Œ£_‚Ñì f_‚Ñì(x)œÄ_‚Ñì." (Trecho de ESL II)

[7] "log Pr(G = k|X = x) = log œÄ_k - (1/2) log |Œ£| - (1/2)(x - Œº_k)^T Œ£^(-1)(x - Œº_k) + const." (Trecho de ESL II)

[8] "A igualdade das matrizes de covari√¢ncia causa o cancelamento dos fatores de normaliza√ß√£o, bem como da parte quadr√°tica nos expoentes." (Trecho de ESL II)

[9] "As fronteiras de decis√£o lineares s√£o hiperplanos em IR^p." (Trecho de ESL II)

[10] "Isto √©, claro, verdade para qualquer par de classes, ent√£o todas as fronteiras de decis√£o s√£o lineares." (Trecho de ESL II)

[11] "Se K = 3, por exemplo, isso poderia nos permitir ver os dados em um gr√°fico bidimensional." (Trecho de ESL II)

[12] "Na pr√°tica, n√£o conhecemos os par√¢metros das distribui√ß√µes Gaussianas e precisaremos estim√°-los usando nossos dados de treinamento." (Trecho de ESL II)

[13] "ŒºÃÇ_k = Œ£_(g_i=k) x_i/N_k, onde N_k √© o n√∫mero de observa√ß√µes de classe k." (Trecho de ESL II)

[14] "Parte de sua popularidade se deve a uma restri√ß√£o adicional que nos permite ver proje√ß√µes informativas de baixa dimens√£o dos dados." (Trecho de ESL II)

[15] "A LDA √© computacionalmente eficiente, especialmente quando comparada a m√©todos mais complexos." (Trecho de ESL II)

[16] "LDA e QDA t√™m bom desempenho em um conjunto surpreendentemente grande e diverso de tarefas de classifica√ß√£o." (Trecho de ESL II)

[17] "Isto n√£o √© toda boa not√≠cia, porque tamb√©m significa que a LDA n√£o √© robusta a outliers grosseiros." (Trecho de ESL II)

[18] "Claramente, elas podem apenas suportar fronteiras de decis√£o simples como linear ou quadr√°tica." (Trecho de ESL II)

[19] "A raz√£o √© provavelmente n√£o que os dados s√£o aproximadamente Gaussianos, e al√©m disso para LDA que as covari√¢ncias s√£o aproximadamente iguais." (Trecho de ESL II)

[20] "Se as Œ£_k n√£o s√£o assumidas como iguais, ent√£o os cancelamentos convenientes em (4.9) n√£o ocorrem; em particular, as pe√ßas quadr√°ticas em x permanecem." (Trecho de ESL II)

[21] "Friedman (1989) prop√¥s um compromisso entre LDA e QDA, que permite que se encolha as covari√¢ncias separadas de QDA em dire√ß√£o a uma covari√¢ncia comum como na LDA." (Trecho de ESL II)

[22] "As matrizes de covari√¢ncia regularizadas t√™m a forma Œ£ÃÇ_k(Œ±) = Œ±Œ£ÃÇ_k + (1 - Œ±)Œ£ÃÇ." (Trecho de ESL II)

[23] "Fisher definiu √≥timo para significar que os centroides projetados estavam espalhados o m√°ximo poss√≠vel em termos de vari√¢ncia." (Trecho de ESL II)